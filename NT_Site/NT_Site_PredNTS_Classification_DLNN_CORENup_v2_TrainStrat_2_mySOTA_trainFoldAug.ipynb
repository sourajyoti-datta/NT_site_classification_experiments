{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 5\n",
    "expName = \"NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\"\n",
    "outPath = \"Results\"\n",
    "foldName = \"folds.pickle\"\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "seed = None\n",
    "\n",
    "input_data_folder = \"Data\"\n",
    "training_data_file = \"Training-datasets-PredNTS.txt\"\n",
    "independent_data_file = \"independent dataset-PredNTS.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.is_gpu_available(cuda_only=True))\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define all CUSTOM functions\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_encode_nt(sequence, char_dict):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),len(char_dict)))\n",
    "    \n",
    "    i = 0\n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in char_dict.keys()):\n",
    "            seq_encoded[i][char_dict[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            raise ValueError('Incorrect character in NT sequence: '+sequence)\n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Build k-fold functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def labels_1d_to_2d(labels_1d):\n",
    "    labels_2d = np.eye(2)[labels_1d]\n",
    "    return labels_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Function to customize the DLNN architecture with parameters\n",
    "# ##################################################################################\n",
    "\n",
    "# def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "#                  conv_filters_per_layer_1 = 50, kernel_length_1 = 5, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "#                  max_pool_width_1 = 2, max_pool_stride_1 = 2, ## 1st Maxpool layer parameters\n",
    "#                  lstm_decode_units = 50, ## LSTM layer parameters\n",
    "#                  conv_filters_per_layer_2 = 50,  kernel_length_2 = 10, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "#                  max_pool_width_2 = 2, max_pool_stride_2 = 2, ## 2nd Maxpool layer parameters\n",
    "#                  dense_decode_units = 370, ## Dense layer parameters\n",
    "#                  prob = 0.75, learn_rate = 0.0003, loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "#     beta = 0.001\n",
    "    \n",
    "#     ######################################################################################################\n",
    "#     ########  SEQUENCE  ##################################################################################\n",
    "#     ######################################################################################################\n",
    "    \n",
    "#     input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "#     x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "#                                 strides = conv_strides_1, \n",
    "#                                 kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                                 padding = \"same\")(input1)\n",
    "#     x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "#     x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "#     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "#     ## LSTM Path\n",
    "\n",
    "#     x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "#     x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "#     x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "#     ## Conv Path\n",
    "\n",
    "#     x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, \n",
    "#                                 strides = conv_strides_2, \n",
    "#                                 kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                                 padding = 'same')(x1)\n",
    "#     x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "#     x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "#     x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "#     x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "#     x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "#     ######################################################################################################\n",
    "#     ########  Classifier  ################################################################################\n",
    "#     ######################################################################################################\n",
    "    \n",
    "#     y = tf.keras.layers.Dense(dense_decode_units, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                               activation = 'relu')(x4)\n",
    "    \n",
    "#     y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "#     y = tf.keras.layers.Dense(1, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                               activation = 'sigmoid')(y)\n",
    "\n",
    "#     ## Generate Model from input and output\n",
    "#     model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "#     ## Compile model\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), \n",
    "#                       loss = loss, metrics = metrics)\n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), \n",
    "#                       loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to customize the DLNN architecture with parameters\n",
    "##################################################################################\n",
    "\n",
    "def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "                 conv_filters_per_layer_1 = 10, kernel_length_1 = 10, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "                 max_pool_width_1 = 3, max_pool_stride_1 = 3, ## 1st Maxpool layer parameters\n",
    "                 lstm_decode_units = 25, ## LSTM layer parameters\n",
    "                 conv_filters_per_layer_2 = 10,  kernel_length_2 = 5, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "                 max_pool_width_2 = 3, max_pool_stride_2 = 3, ## 2nd Maxpool layer parameters\n",
    "                 dense_decode_units = 128, ## Dense layer parameters\n",
    "                 prob = 0.5, learn_rate = 0.0005, \n",
    "                 loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  SEQUENCE  ##################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "                                strides = conv_strides_1, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = \"same\")(input1)\n",
    "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "    x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "    ## Conv Path\n",
    "\n",
    "    x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, strides = conv_strides_2, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), padding = 'same')(x1)\n",
    "    x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "    x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "    x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  Classifier  ################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    y = tf.keras.layers.Dense(dense_decode_units, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'relu'\n",
    "                             )(x4)\n",
    "    \n",
    "    y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(1, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'sigmoid')(y)\n",
    "\n",
    "    ## Generate Model from input and output\n",
    "    model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Function to customize the DLNN architecture with parameters\n",
    "# ##################################################################################\n",
    "\n",
    "# def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "#                  conv_filters_per_layer_1 = 25, kernel_length_1 = 10, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "#                  max_pool_width_1 = 3, max_pool_stride_1 = 3, ## 1st Maxpool layer parameters\n",
    "#                  lstm_decode_units = 25, ## LSTM layer parameters\n",
    "#                  conv_filters_per_layer_2 = 25,  kernel_length_2 = 5, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "#                  max_pool_width_2 = 3, max_pool_stride_2 = 3, ## 2nd Maxpool layer parameters\n",
    "#                  dense_decode_units = 150, ## Dense layer parameters\n",
    "#                  prob = 0.5, learn_rate = 0.0005, \n",
    "#                  loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "#     beta = 0.001\n",
    "    \n",
    "#     ######################################################################################################\n",
    "#     ########  SEQUENCE  ##################################################################################\n",
    "#     ######################################################################################################\n",
    "    \n",
    "#     input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "#     x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "#                                 strides = conv_strides_1, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                                 padding = \"same\")(input1)\n",
    "#     x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "#     x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "#     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "#     ## LSTM Path\n",
    "\n",
    "#     x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    \n",
    "#     x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "#     x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "#     ## Conv Path\n",
    "\n",
    "#     x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, strides = conv_strides_2, \n",
    "#                                 kernel_regularizer = tf.keras.regularizers.l2(beta), padding = 'same')(x1)\n",
    "#     x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "#     x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "#     x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "#     x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "#     x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "#     ######################################################################################################\n",
    "#     ########  Classifier  ################################################################################\n",
    "#     ######################################################################################################\n",
    "    \n",
    "#     y = tf.keras.layers.Dense(dense_decode_units, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                               activation = 'relu')(x4)\n",
    "    \n",
    "#     y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "#     y = tf.keras.layers.Dense(1, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta),\n",
    "#                               activation = 'sigmoid')(y)\n",
    "\n",
    "#     ## Generate Model from input and output\n",
    "#     model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "#     ## Compile model\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss, metrics = metrics)\n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 41, 21)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 41, 50)       5300        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 41, 50)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 20, 50)       0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20, 50)       0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 20, 50)       25050       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 20, 50)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 20, 50)       20200       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 10, 50)      0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20, 50)       0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10, 50)       0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1000)         0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 500)          0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1500)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 370)          555370      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 370)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            371         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 606,291\n",
      "Trainable params: 606,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DLNN_CORENup().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### read training file\n",
    "##################################################################################\n",
    "train_file_path = os.path.join(input_data_folder, training_data_file)\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t', header=None)\n",
    "train_data.columns = ['Sequence', 'name', 'id', 'flag', 'label_original', 'type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### read independent data file\n",
    "##################################################################################\n",
    "indpe_file_path = os.path.join(input_data_folder, independent_data_file)\n",
    "indpe_data = pd.read_csv(indpe_file_path, sep='\\t', header=None)\n",
    "indpe_data.columns = ['Sequence', 'name', 'id', 'flag', 'label_original', 'type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty char count per sequence\n",
    "train_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in train_data['Sequence']]\n",
    "# incomplete sequence flag\n",
    "train_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in train_data['Sequence']]\n",
    "\n",
    "indpe_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in indpe_data['Sequence']]\n",
    "indpe_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in indpe_data['Sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_get_stats(source_data, target_data):\n",
    "    \n",
    "    # empty char count per sequence\n",
    "    source_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in source_data['Sequence']]\n",
    "    # incomplete sequence flag\n",
    "    source_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in source_data['Sequence']]\n",
    "    \n",
    "    target_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in target_data['Sequence']]\n",
    "    target_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in target_data['Sequence']]\n",
    "\n",
    "    # 0:1\n",
    "    train_label_nonempty_ratio = source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    train_label_ratio = (source_data.shape[0]-sum(source_data[\"label_original\"] == 1)) / sum(source_data[\"label_original\"] == 1)\n",
    "\n",
    "    # 0:1\n",
    "    indpe_label_nonempty_ratio = target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    indpe_label_ratio = (target_data.shape[0]-sum(target_data[\"label_original\"] == 1)) / sum(target_data[\"label_original\"] == 1)\n",
    "\n",
    "    print('Current train_label_nonempty_ratio:', train_label_nonempty_ratio, 'train_label_ratio:', train_label_ratio)\n",
    "    print('Target indpe_label_nonempty_ratio:', indpe_label_nonempty_ratio, 'indpe_label_ratio:', indpe_label_ratio)\n",
    "\n",
    "    increase_0_data_factor = int(round(indpe_label_ratio/train_label_ratio)) - 1\n",
    "    increase_empty_data_factor = int(round(indpe_label_nonempty_ratio/train_label_nonempty_ratio)) - 1\n",
    "    \n",
    "    return increase_0_data_factor, increase_empty_data_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 183\n",
      "Ratio empty/total: 0.07682619647355164\n"
     ]
    }
   ],
   "source": [
    "plt.hist(train_data['Empty_count'][train_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(train_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(train_data['has_empty'])/train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(train_data['Empty_count'][(train_data['Empty_count'] != 0) & (train_data['label_original'] == -1)])\n",
    "axs[1].hist(train_data['Empty_count'][(train_data['Empty_count'] != 0) & (train_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_original</th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_original  has_empty\n",
       "0              -1         39\n",
       "1               1        144"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 68\n",
      "Ratio empty/total: 0.05551020408163265\n"
     ]
    }
   ],
   "source": [
    "plt.hist(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(indpe_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(indpe_data['has_empty'])/indpe_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2cAAAFTCAYAAAC9P3T3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAewUlEQVR4nO3de7RkZ1kn4N9rOsglgQTTC0KSplEzzggjkOnhIooZUScEJOigExwlXFytKCM4qBNwrYCsNUq84A0XGEkkIAMoF40QhCiwkBmJdGIIucAQMZjEQJoEEiI3A+/8cSp4ONTpPt2n+tRX5zzPWrXOvnxV+927du+vf7V37aruDgAAAPP1dfMuAAAAAOEMAABgCMIZAADAAIQzAACAAQhnAAAAAxDOAAAABiCcATCEqnpbVZ0x7zoAYF7K75wBcLCq6vZlo3dP8oUkX5qM/0R3v2aD6rg2yX2S3DFZ/lVJXpXknO7+8hqevzPJPyQ5vLvvOHSVAsDqts27AAAWV3cfcefwJCD9eHf/5cp2VbVtA0LP93f3X1bVvZJ8V5LfTvLwJE87xMsFgJlwWSMAM1dVJ1fV9VX1P6vq40n+sKqOrqq3VNXeqvrUZPj4Zc95d1X9+GT4qVX13qr69Unbf6iqx65l2d19a3dfkOS/Jjmjqh40ec3HVdXfVdVtVXVdVb1w2dPeM/n76aq6vaoeWVXfVFXvrKqbq+qTVfWaqjpqBpsHAKYSzgA4VO6b5N5J7p9kd5b6nD+cjO9I8rkkL93H8x+e5MNJjknyq0nOrapa68K7+2+TXJ/kOyeT/jnJU5IcleRxSZ5ZVU+czHv05O9R3X1Ed/9NkkryK0nul+TfJTkhyQvXunwAOFDCGQCHypeTvKC7v9Ddn+vum7v7jd392e7+TJL/laXLD1fzse7+g+7+UpLzkxybpe+VHYh/ylJATHe/u7s/2N1f7u7Lk7x2X8vv7mu6+6JJ/XuTvGQ/9QLAuvjOGQCHyt7u/vydI1V19yS/meSUJEdPJh9ZVYdNAthKH79zoLs/OzlpdsSUdvtyXJJbJst/eJIXJ3lQkrsk+fokf7LaE6vqPln63tp3JjkySx9ofuoAlw8Aa+bMGQCHysrbAT83ybckeXh33zP/einhmi9VPBBV9R+zFM7eO5n0v5NckOSE7r5XkpcvW/a0Wxf/8mT6v5/U+6OHqlYASIQzADbOkVn6ntmnq+reSV5wKBZSVfesqscneV2SP+ruDy5b/i3d/fmqeliSH1n2tL1ZugzzG1fUe3uSW6vquCQ/fyjqBYA7CWcAbJTfSnK3JJ9M8r4kfzHj1//zqvpMkuuS/GKWviO2/Db6P5XkRZM2ZyX54ztndPdns/QduP9TVZ+uqkck+aUkJyW5Nclbk7xpxvUCwFfxI9QAAAADcOYMAABgAMIZAADAAIQzAACAAQhnAAAAAxDOYAuoqmur6nvW2Lar6psPcjkH/VyAWXLcAxaRcAbMVS05u6punjzOrio/9AtsWlX1n6rqXVV1a1VdO+96gHEIZ8C87U7yxCQPTvJtSb4/yU/MsyCAQ+yfk5wXP2wOrCCcwRZTVQ+rqr+Z/NDujVX10qq6y4pmp1bVR6vqk1X1a1X1dcue//SqurqqPlVVb6+q+6+zpDOS/EZ3X9/dNyT5jSRPXedrAnzFaMe97v7b7n51ko+u53WAzUc4g63nS0l+NskxSR6Z5DFJfmpFmx9IsivJSUlOS/L0JKmq05I8P8kPJtme5K+TvHbaQqrqzMl/hKY+ljV9YJIPLBv/wGQawKyMdtwDmEo4gy2muy/p7vd19x3dfW2S30/yXSuand3dt3T3Pyb5rSRPnkz/ySS/0t1Xd/cdSX45yUOmfYrc3S/u7qNWeyxrekSSW5eN35rkCN87A2ZlwOMewFTCGWwxVfVvquotVfXxqrotS//ROGZFs+uWDX8syf0mw/dP8tvLPgW+JUklOW4dJd2e5J7Lxu+Z5Pbu7nW8JsBXDHjcA5hKOIOt52VJPpTkxO6+Z5Yu11l5luqEZcM7kvzTZPi6JD+x4tPgu3X3/125kKp6flXdvtpjWdMrs3QzkDs9eDINYFZGO+4BTCWcwdZzZJLbktxeVf82yTOntPn5qjq6qk5I8uwkr59Mf3mS51XVA5Okqu5VVT80bSHd/cvdfcRqj2VNX5Xkf1TVcVV1vyTPTfLKmawpwJKhjntV9XVVddckhy+N1l2n3KAE2IKEM9h6fi7JjyT5TJI/yL/+B2S5P0tySZLLkrw1yblJ0t1vTnJ2ktdNLg26Islj11nP7yf58yQfnLzeWyfTAGZltOPeo5N8LsmFWTpL97kk71jnawKbQPlaBwAAwPw5cwYAADAA4QwAAGAAwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgANs2cmHHHHNM79y5cyMXCcAcXHLJJZ/s7u3zrmNR6B8Bto599ZEbGs527tyZPXv2bOQiAZiDqvrYvGtYJPpHgK1jX32kyxoBAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgAMIZAADAAIQzAACAAew3nFXVeVV1U1VdsWzavavqoqr6yOTv0Ye2TACYv6q6a1X9bVV9oKqurKpfmtLm66vq9VV1TVVdXFU751AqAAtoLWfOXpnklBXTzkzyV919YpK/mowDwGb3hSTf3d0PTvKQJKdU1SNWtHlGkk919zcn+c0kZ29siQAsqv2Gs+5+T5JbVkw+Lcn5k+HzkzxxtmUBwHh6ye2T0cMnj17RbHkf+YYkj6mq2qASAVhgB/uds/t0942T4Y8nuc+M6gGAoVXVYVV1WZKbklzU3RevaHJckuuSpLvvSHJrkm/Y0CIBWEjb1vsC3d1VtfJTw6+oqt1JdifJjh071rs4pth55lvnXcJwrn3x4+ZdArBJdfeXkjykqo5K8uaqelB3X7Gfp30N/ePGGKWP1C+xaEb5tzOaQ/1v+WDPnH2iqo5Nksnfm1Zr2N3ndPeu7t61ffv2g1wcAIyluz+d5F352u9l35DkhCSpqm1J7pXk5inP1z8C8FUONpxdkOSMyfAZSf5sNuUAwLiqavvkjFmq6m5JvjfJh1Y0W95HPinJO7t71StMAOBO+72ssapem+TkJMdU1fVJXpDkxUn+uKqekeRjSX74UBYJAIM4Nsn5VXVYlj7g/OPufktVvSjJnu6+IMm5SV5dVddk6YZap8+vXAAWyX7DWXc/eZVZj5lxLQAwtO6+PMlDp0w/a9nw55P80EbWBcDmcLCXNQIAADBDwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgAMIZAADAAIQzAACAAQhnAAAAAxDOAAAABiCcAQAADEA4AwAAGIBwBgAAMADhDAAAYADCGQAAwACEMwAAgAEIZwAAAAMQzgAAAAYgnAEAAAxAOAMAABiAcAYAADAA4QwAAGAAwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgAMIZAADAAIQzAFijqjqhqt5VVVdV1ZVV9ewpbU6uqlur6rLJ46x51ArA4tk27wIAYIHckeS53X1pVR2Z5JKquqi7r1rR7q+7+/FzqA+ABebMGQCsUXff2N2XToY/k+TqJMfNtyoANgvhDAAOQlXtTPLQJBdPmf3IqvpAVb2tqh64sZUBsKhc1ggAB6iqjkjyxiTP6e7bVsy+NMn9u/v2qjo1yZ8mOXHKa+xOsjtJduzYcWgLBmAhOHMGAAegqg7PUjB7TXe/aeX87r6tu2+fDF+Y5PCqOmZKu3O6e1d379q+ffshrxuA8QlnALBGVVVJzk1ydXe/ZJU29520S1U9LEt97c0bVyUAi8pljQCwdo9K8mNJPlhVl02mPT/JjiTp7pcneVKSZ1bVHUk+l+T07u451ArAghHOAGCNuvu9SWo/bV6a5KUbUxEAm8m6Lmusqp+d/AjnFVX12qq666wKAwAA2EoOOpxV1XFJfibJru5+UJLDkpw+q8IAAAC2kvXeEGRbkrtV1bYkd0/yT+svCQAAYOs56HDW3Tck+fUk/5jkxiS3dvc7ZlUYAADAVrKeyxqPTnJakgckuV+Se1TVj05pt7uq9lTVnr179x58pQAAAJvYei5r/J4k/9Dde7v7X5K8Kcm3r2zkRzYBAAD2bz3h7B+TPKKq7j75sc3HJLl6NmUBAABsLev5ztnFSd6Q5NIkH5y81jkzqgsAAGBLWdePUHf3C5K8YEa1AAAAbFnrvZU+AAAAMyCcAQAADEA4AwAAGIBwBgAAMADhDAAAYADCGQAAwACEMwAAgAEIZwAAAAMQzgAAAAYgnAEAAAxAOAMAABiAcAYAADAA4QwAAGAAwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgANvmXQCw9ew8863zLuErrn3x4+ZdAgBAEmfOAAAAhiCcAQAADEA4AwAAGIBwBgBrVFUnVNW7quqqqrqyqp49pU1V1e9U1TVVdXlVnTSPWgFYPG4IAgBrd0eS53b3pVV1ZJJLquqi7r5qWZvHJjlx8nh4kpdN/gLAPjlzBgBr1N03dvelk+HPJLk6yXErmp2W5FW95H1JjqqqYze4VAAWkHAGAAehqnYmeWiSi1fMOi7JdcvGr8/XBjgA+BouawSAA1RVRyR5Y5LndPdtB/kau5PsTpIdO3bMsLr5G+m3DGEtRtpn/f7m1ubMGQAcgKo6PEvB7DXd/aYpTW5IcsKy8eMn075Kd5/T3bu6e9f27dsPTbEALBThDADWqKoqyblJru7ul6zS7IIkT5nctfERSW7t7hs3rEgAFpbLGgFg7R6V5MeSfLCqLptMe36SHUnS3S9PcmGSU5Nck+SzSZ628WUCsIiEMwBYo+5+b5LaT5tO8tMbUxEAm4nLGgEAAAYgnAEAAAxAOAMAABiAcAYAADAA4QwAAGAAwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGsK5wVlVHVdUbqupDVXV1VT1yVoUBAABsJdvW+fzfTvIX3f2kqrpLkrvPoCYAAIAt56DDWVXdK8mjkzw1Sbr7i0m+OJuyAAAAtpb1XNb4gCR7k/xhVf1dVb2iqu6xslFV7a6qPVW1Z+/evetYHAAAwOa1nnC2LclJSV7W3Q9N8s9JzlzZqLvP6e5d3b1r+/bt61gcAADA5rWecHZ9kuu7++LJ+BuyFNYAAAA4QAcdzrr740muq6pvmUx6TJKrZlIVAADAFrPeuzX+9ySvmdyp8aNJnrb+kgAAALaedYWz7r4sya7ZlAIAALB1retHqAEAAJgN4QwAAGAAwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgAMIZAADAAIQzAACAAQhnAAAAAxDOAAAABiCcAQAADEA4AwAAGIBwBgAAMADhDADWqKrOq6qbquqKVeafXFW3VtVlk8dZG10jAItr27wLAIAF8sokL03yqn20+evufvzGlAPAZuLMGQCsUXe/J8kt864DgM1JOAOA2XpkVX2gqt5WVQ9crVFV7a6qPVW1Z+/evRtZHwCDEs4AYHYuTXL/7n5wkt9N8qerNezuc7p7V3fv2r59+0bVB8DAhDMAmJHuvq27b58MX5jk8Ko6Zs5lAbAghDMAmJGqum9V1WT4YVnqZ2+eb1UALAp3awSANaqq1yY5OckxVXV9khckOTxJuvvlSZ6U5JlVdUeSzyU5vbt7TuUCsGCEMwBYo+5+8n7mvzRLt9oHgAPmskYAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgAMIZAADAAIQzAACAAQhnAAAAAxDOAAAABiCcAQAADEA4AwAAGIBwBgAAMADhDAAAYADCGQAAwADWHc6q6rCq+ruqesssCgIAANiKZnHm7NlJrp7B6wAAAGxZ6wpnVXV8ksclecVsygEAANia1nvm7LeS/EKSL6+/FAAAgK1r28E+saoen+Sm7r6kqk7eR7vdSXYnyY4dOw52cUPaeeZb510Cqxjpvbn2xY+bdwnsw0j7yijsswAwH+s5c/aoJE+oqmuTvC7Jd1fVH61s1N3ndPeu7t61ffv2dSwOAABg8zrocNbdz+vu47t7Z5LTk7yzu390ZpUBAABsIX7nDAAAYAAH/Z2z5br73UnePYvXAgAA2IqcOQMAABiAcAYAADAA4QwAAGAAwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAegqs6rqpuq6opV5ldV/U5VXVNVl1fVSRtdIwCLSTgDgAPzyiSn7GP+Y5OcOHnsTvKyDagJgE1AOAOAA9Dd70lyyz6anJbkVb3kfUmOqqpjN6Y6ABaZcAYAs3VckuuWjV8/mQYA+7Rt3gUAwFZUVbuzdNljduzYMZPX3HnmW2fyOmxuI+0n1774cfMuYTgjvT9sPGfOAGC2bkhywrLx4yfTvkp3n9Pdu7p71/bt2zesOADGJZwBwGxdkOQpk7s2PiLJrd1947yLAmB8LmsEgANQVa9NcnKSY6rq+iQvSHJ4knT3y5NcmOTUJNck+WySp82nUgAWjXAGAAegu5+8n/md5Kc3qBwANhGXNQIAAAxAOAMAABiAcAYAADCAhfvOmd9+YNHYZwEAWAtnzgAAAAYgnAEAAAxAOAMAABiAcAYAADAA4QwAAGAAwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgAMIZAADAAIQzAACAAQhnAAAAAxDOAAAABiCcAQAADEA4AwAAGIBwBgAAMADhDAAAYAAHHc6q6oSqeldVXVVVV1bVs2dZGAAAwFaybR3PvSPJc7v70qo6MsklVXVRd181o9oAAAC2jIM+c9bdN3b3pZPhzyS5OslxsyoMAABgK5nJd86qameShya5eBavBwAAsNWsO5xV1RFJ3pjkOd1925T5u6tqT1Xt2bt373oXBwAAsCmtK5xV1eFZCmav6e43TWvT3ed0967u3rV9+/b1LA4AAGDTWs/dGivJuUmu7u6XzK4kAACArWc9Z84eleTHknx3VV02eZw6o7oAAAC2lIO+lX53vzdJzbAWAACALWsmd2sEgK2iqk6pqg9X1TVVdeaU+U+tqr3Lrir58XnUCcDiWc+PUAPAllJVhyX5vSTfm+T6JO+vqgu6+6oVTV/f3c/a8AIBWGjOnAHA2j0syTXd/dHu/mKS1yU5bc41AbBJCGcAsHbHJblu2fj1k2kr/Zequryq3lBVJ2xMaQAsOuEMAGbrz5Ps7O5vS3JRkvOnNaqq3VW1p6r27N27d0MLBGBMwhkArN0NSZafCTt+Mu0ruvvm7v7CZPQVSf7DtBfq7nO6e1d379q+ffshKRaAxSKcAcDavT/JiVX1gKq6S5LTk1ywvEFVHbts9AlJrt7A+gBYYO7WCABr1N13VNWzkrw9yWFJzuvuK6vqRUn2dPcFSX6mqp6Q5I4ktyR56twKBmChCGcAcAC6+8IkF66Ydtay4ecled5G1wXA4nNZIwAAwACEMwAAgAEIZwAAAAMQzgAAAAYgnAEAAAxAOAMAABiAcAYAADAA4QwAAGAAwhkAAMAAhDMAAIABCGcAAAADEM4AAAAGIJwBAAAMQDgDAAAYgHAGAAAwAOEMAABgAMIZAADAAIQzAACAAQhnAAAAAxDOAAAABiCcAQAADEA4AwAAGIBwBgAAMADhDAAAYADCGQAAwACEMwAAgAEIZwAAAAMQzgAAAAYgnAEAAAxAOAMAABiAcAYAADAA4QwAAGAAwhkAAMAAhDMAAIABrCucVdUpVfXhqrqmqs6cVVEAMKr99X1V9fVV9frJ/IurauccygRgAR10OKuqw5L8XpLHJvnWJE+uqm+dVWEAMJo19n3PSPKp7v7mJL+Z5OyNrRKARbWeM2cPS3JNd3+0u7+Y5HVJTptNWQAwpLX0faclOX8y/IYkj6mq2sAaAVhQ6wlnxyW5btn49ZNpALBZraXv+0qb7r4jya1JvmFDqgNgoW071Auoqt1Jdk9Gb6+qDx/qZc7QMUk+Oe8iDsIi1r2INSeLWfci1pwsZt2LWHPq7JnUff9Z1LKZzbl/XMh9cxULsS61totbF2Jd1mDN67HG7TJPm+U9SazLTMxon121j1xPOLshyQnLxo+fTPsq3X1OknPWsZy5qao93b1r3nUcqEWsexFrThaz7kWsOVnMuhex5mRx694ga+n77mxzfVVtS3KvJDevfKF59o+b6T22LuPZLOuRWJdRbaZ1WWk9lzW+P8mJVfWAqrpLktOTXDCbsgBgSGvp+y5IcsZk+ElJ3tndvYE1ArCgDvrMWXffUVXPSvL2JIclOa+7r5xZZQAwmNX6vqp6UZI93X1BknOTvLqqrklyS5YCHADs17q+c9bdFya5cEa1jGghL8fMYta9iDUni1n3ItacLGbdi1hzsrh1b4hpfV93n7Vs+PNJfmij6zpAm+k9ti7j2SzrkViXUW2mdfkq5UoLAACA+VvPd84AAACYkS0fzqrqhKp6V1VdVVVXVtWzp7Q5uapurarLJo+zpr3WRquqa6vqg5Oa9kyZX1X1O1V1TVVdXlUnzaPOZfV8y7JteFlV3VZVz1nRZohtXVXnVdVNVXXFsmn3rqqLquojk79Hr/LcMyZtPlJVZ0xrs4E1/1pVfWjy/r+5qo5a5bn73JcOpVXqfmFV3bBsPzh1leeeUlUfnuzjZ8655tcvq/faqrpslefOc1tPPd6Nvm9zcBa5f5tm0fq8aRapH5xmEfvG1Sxqn7lKPQvXj65mUfvXmeruLf1IcmySkybDRyb5f0m+dUWbk5O8Zd61Tqn92iTH7GP+qUnelqSSPCLJxfOueVlthyX5eJL7j7itkzw6yUlJrlg27VeTnDkZPjPJ2VOed+8kH538PXoyfPQca/6+JNsmw2dPq3kt+9Ic6n5hkp9bwz7090m+Mcldknxg5b/djax5xfzfSHLWgNt66vFu9H3bY7bv94o2Qxxz17g+C9vnrVLv0P3gKjUvXN94gOsyfJ95AOsydD96IOuyYv6Q/essH1v+zFl339jdl06GP5Pk6iTHzbeqmTktyat6yfuSHFVVx867qInHJPn77v7YvAuZprvfk6W7rC13WpLzJ8PnJ3nilKf+5yQXdfct3f2pJBclOeVQ1bnctJq7+x3dfcdk9H1Z+k2moayyrdfiYUmu6e6PdvcXk7wuS+/RIbevmquqkvxwktduRC0HYh/Hu6H3bQ7OJu/fphm5z5tm6H5wmkXsG1ezqH3mNIvYj65mUfvXWdry4Wy5qtqZ5KFJLp4y+5FV9YGqeltVPXBjK1tVJ3lHVV1SVbunzD8uyXXLxq/POB3z6Vn9H9eI2zpJ7tPdN06GP57kPlPajLzNn56lT5Wn2d++NA/Pmlxact4ql8mMuq2/M8knuvsjq8wfYluvON4t+r7Nfixg/zbNIvd50yxiPzjNZj1+LFqfOc2i9qOrWYj+db2Es4mqOiLJG5M8p7tvWzH70ixddvDgJL+b5E83uLzVfEd3n5TksUl+uqoePe+C1qKWfrj1CUn+ZMrsUbf1V+ml8+cLc6vTqvrFJHckec0qTUbbl16W5JuSPCTJjVm6jGFRPDn7/lRv7tt6X8e7Rdu32b8F7d+mmfu/nVnZDP3gNJvl+LGAfeY0i9yPrmb4/nUWhLMkVXV4ljqu13T3m1bO7+7buvv2yfCFSQ6vqmM2uMyv0d03TP7elOTNWTo9vdwNSU5YNn78ZNq8PTbJpd39iZUzRt3WE5+48xKZyd+bprQZbptX1VOTPD7Jf5t0nF9jDfvShuruT3T3l7r7y0n+YJV6RtzW25L8YJLXr9Zm3tt6lePdQu7b7N+i9m/TLHCfN82i9oPTbKrjxyL2mdMsaj+6mkXoX2dly4ezyfWr5ya5urtfskqb+07apaoelqXtdvPGVTm1pntU1ZF3DmfpS6xXrGh2QZKn1JJHJLl12aUH87TqJx8jbutlLkhy5x2mzkjyZ1PavD3J91XV0ZNLCL5vMm0uquqUJL+Q5And/dlV2qxlX9pQK74n8gOZXs/7k5xYVQ+YfAp9epbeo3n6niQf6u7rp82c97bex/Fu4fZt9m9R+7dpFrzPm2ZR+8FpNs3xY1H7zGkWuB9dzdD960yt5a4hm/mR5DuydAr+8iSXTR6nJvnJJD85afOsJFdm6S4270vy7QPU/Y2Tej4wqe0XJ9OX111Jfi9Ld+L5YJJdA9R9jyx1MvdaNm24bZ2lTvPGJP+SpWuwn5HkG5L8VZKPJPnLJPeetN2V5BXLnvv0JNdMHk+bc83XZOl68jv37ZdP2t4vyYX72pfmXPerJ/vs5VnqKI5dWfdk/NQs3YHu7zey7mk1T6a/8s59eVnbkbb1ase7ofdtj5m/38Mdc9ewLgvZ562yLgvRD65S+8L1jQe4LsP3mQewLkP3oweyLpPpr8zA/essHzVZIQAAAOZoy1/WCAAAMALhDAAAYADCGQAAwACEMwAAgAEIZwAAAAMQzgAAAAYgnAEAAAxAOAMAABjA/weMj38wgaYh3AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(indpe_data['Empty_count'][(indpe_data['Empty_count'] != 0) & (indpe_data['label_original'] == -1)])\n",
    "axs[1].hist(indpe_data['Empty_count'][(indpe_data['Empty_count'] != 0) & (indpe_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_original</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                has_empty\n",
       "label_original           \n",
       "-1                     52\n",
       " 1                     16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indpe_data.groupby([\"label_original\"]).sum().filter(['has_empty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train_label_nonempty_ratio: 0.2708333333333333 train_label_ratio: 1.0\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n"
     ]
    }
   ],
   "source": [
    "_,_ = print_and_get_stats(train_data, indpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA - get drop lengths from distribution\n",
    "\n",
    "# from __future__ import division\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=10)\n",
    "\n",
    "# bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "# cdf = np.cumsum(hist)\n",
    "# cdf = cdf / cdf[-1]\n",
    "# values = np.random.rand(100)\n",
    "# value_bins = np.searchsorted(cdf, values)\n",
    "# random_from_cdf = bin_midpoints[value_bins]\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.hist(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], 10)\n",
    "# plt.subplot(122)\n",
    "# plt.hist(np.round(random_from_cdf).astype(int), 10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation on Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sequence_truncate(seq, random_length):\n",
    "    rep_str = \"\".join(['-']*random_length)\n",
    "    if np.random.choice((True, False)):\n",
    "        return_seq = rep_str + seq[random_length:]\n",
    "    else:\n",
    "        return_seq = seq[:-random_length] + rep_str\n",
    "    return return_seq\n",
    "\n",
    "def random_sequence_truncate(seq):\n",
    "    random_length = np.random.randint(1, 20)\n",
    "    return_seq = sequence_truncate(seq, random_length)\n",
    "    return return_seq\n",
    "\n",
    "def repeat_truncate_sequence_steps(seq, factor):\n",
    "    random_length = random.sample(range(1, int(len(seq)/2)), \n",
    "                                  factor)\n",
    "    return_seqs = []\n",
    "    for i in range(factor):\n",
    "        ret_seq = sequence_truncate(seq, random_length[i])\n",
    "        return_seqs.append(ret_seq)\n",
    "    return return_seqs\n",
    "\n",
    "def truncate_sequence_by_len(seq, ran_len):\n",
    "    return_seqs = []\n",
    "    ret_seq = sequence_truncate(seq, ran_len)\n",
    "    return ret_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_get_stats(source_data, target_data):\n",
    "    \n",
    "    # empty char count per sequence\n",
    "    source_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in source_data['Sequence']]\n",
    "    # incomplete sequence flag\n",
    "    source_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in source_data['Sequence']]\n",
    "    \n",
    "    target_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in target_data['Sequence']]\n",
    "    target_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in target_data['Sequence']]\n",
    "\n",
    "    # 0:1\n",
    "    train_label_nonempty_ratio = source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    train_label_ratio = (source_data.shape[0]-sum(source_data[\"label_original\"] == 1)) / sum(source_data[\"label_original\"] == 1)\n",
    "\n",
    "    # 0:1\n",
    "    indpe_label_nonempty_ratio = target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    indpe_label_ratio = (target_data.shape[0]-sum(target_data[\"label_original\"] == 1)) / sum(target_data[\"label_original\"] == 1)\n",
    "\n",
    "    print('Current train_label_nonempty_ratio:', train_label_nonempty_ratio, 'train_label_ratio:', train_label_ratio)\n",
    "    print('Target indpe_label_nonempty_ratio:', indpe_label_nonempty_ratio, 'indpe_label_ratio:', indpe_label_ratio)\n",
    "\n",
    "    increase_0_data_factor = int(round(indpe_label_ratio/train_label_ratio)) - 1\n",
    "    increase_empty_data_factor = int(round(indpe_label_nonempty_ratio/train_label_nonempty_ratio)) - 1\n",
    "    \n",
    "    return increase_0_data_factor, increase_empty_data_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(data, indpe_data, aug_factor=0.25, neg_pos_factor=1):\n",
    "    # empty char count per sequence\n",
    "    data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in data['Sequence']]\n",
    "    # incomplete sequence flag\n",
    "    data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in data['Sequence']]\n",
    "    \n",
    "    neg_data = data[data['label_original'] == -1].reset_index(drop=True)\n",
    "    pos_data = data[data['label_original'] == 1].reset_index(drop=True)\n",
    "    # neg_data = data.reset_index(drop=True)\n",
    "\n",
    "    not_empty_neg_idxs = np.where(neg_data['has_empty'] != True)[0]\n",
    "    not_empty_neg_idxs = np.random.permutation(not_empty_neg_idxs)[0:int(not_empty_neg_idxs.shape[0]*aug_factor)]\n",
    "    not_empty_pos_idxs = np.where(pos_data['has_empty'] != True)[0]\n",
    "    not_empty_pos_idxs = np.random.permutation(not_empty_pos_idxs)[0:int(not_empty_neg_idxs.shape[0]/neg_pos_factor)]\n",
    "\n",
    "    ##### Getting missing distribution\n",
    "    hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=20)\n",
    "    bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "    cdf = np.cumsum(hist)\n",
    "    cdf = cdf / cdf[-1]\n",
    "\n",
    "    ##### Negative records augmentation\n",
    "    neg_values = np.random.rand(not_empty_neg_idxs.shape[0])\n",
    "    neg_value_bins = np.searchsorted(cdf, neg_values)\n",
    "    neg_random_from_cdf = bin_midpoints[neg_value_bins]\n",
    "    neg_random_from_cdf = np.round(neg_random_from_cdf).astype(int)\n",
    "\n",
    "    for idx, ran_len in zip(list(not_empty_neg_idxs), list(list(neg_random_from_cdf))):\n",
    "        record = neg_data.iloc[idx].to_dict()\n",
    "        seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "        record['Sequence'] = seq\n",
    "        neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "    ##### Positive records augmentation\n",
    "    pos_values = np.random.rand(not_empty_pos_idxs.shape[0])\n",
    "    pos_value_bins = np.searchsorted(cdf, pos_values)\n",
    "    pos_random_from_cdf = bin_midpoints[pos_value_bins]\n",
    "    pos_random_from_cdf = np.round(pos_random_from_cdf).astype(int)\n",
    "\n",
    "    for idx, ran_len in zip(list(not_empty_pos_idxs), list(list(pos_random_from_cdf))):\n",
    "        record = pos_data.iloc[idx].to_dict()\n",
    "        seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "        record['Sequence'] = seq\n",
    "        pos_data = pos_data.append(record, ignore_index=True)\n",
    "\n",
    "    final_data = pd.concat((neg_data, pos_data, data), ignore_index=True)\n",
    "\n",
    "    final_data = final_data.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the K-fold from dataset\n",
    "def build_kfold_df(data_df, k=10, shuffle=False, seed=None):\n",
    "    data_df = data_df.reset_index(drop=True)\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(data_df['label_original'], data_df['label_original']):\n",
    "        X_train = data_df.iloc[train_index].reset_index(drop=True)\n",
    "        X_test = data_df.iloc[test_index].reset_index(drop=True)\n",
    "        kfoldList.append({\n",
    "            \"train\": X_train,\n",
    "            \"test\": X_test\n",
    "        })\n",
    "    return kfoldList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_folds_from_kfold_df(input_folds_in_df, indpe_data, all_char_dict):\n",
    "    folds = []\n",
    "    for fold in input_folds_in_df:\n",
    "        \n",
    "        fold_train_data = fold['train'].reset_index(drop=True)\n",
    "        fold_test_data = fold['test'].reset_index(drop=True)\n",
    "        \n",
    "        fold_train_data_aug = augmentation(fold_train_data, indpe_data).reset_index(drop=True)\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Train data\n",
    "        ##################################################################################\n",
    "        \n",
    "        # Create OHE of sequence\n",
    "        fold_train_data_aug['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                                         for val in fold_train_data_aug[\"Sequence\"]])\n",
    "\n",
    "        # Fix the labels\n",
    "        fold_train_data_aug['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                                  for val in fold_train_data_aug[\"label_original\"]])\n",
    "\n",
    "        # Extract features and labels\n",
    "        train_features = np.array(list(fold_train_data_aug['OHE_Sequence']))\n",
    "        train_labels = np.array(list(fold_train_data_aug['label']))\n",
    "        train_labels = train_labels.reshape((train_labels.shape[0], 1))\n",
    "        \n",
    "        ##################################################################################\n",
    "        ##### Test data\n",
    "        ##################################################################################\n",
    "        \n",
    "        # Create OHE of sequence\n",
    "        fold_test_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                                    for val in fold_test_data[\"Sequence\"]])\n",
    "        \n",
    "        # Fix the labels\n",
    "        fold_test_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                             for val in fold_test_data[\"label_original\"]])\n",
    "\n",
    "        # Extract features and labels\n",
    "        test_features = np.array(list(fold_test_data['OHE_Sequence']))\n",
    "        test_labels = np.array(list(fold_test_data['label']))\n",
    "        test_labels = test_labels.reshape((test_labels.shape[0], 1))\n",
    "        \n",
    "        folds.append(\n",
    "            {\n",
    "                'X_train':train_features,\n",
    "                'X_test':test_features,\n",
    "                'y_train':train_labels,\n",
    "                'y_test':test_labels\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = augmentation(train_data, indpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty char count per sequence\n",
    "final_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in final_data['Sequence']]\n",
    "# incomplete sequence flag\n",
    "final_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in final_data['Sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 759\n",
      "Ratio empty/total: 0.2565922920892495\n"
     ]
    }
   ],
   "source": [
    "plt.hist(final_data['Empty_count'][final_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(final_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(final_data['has_empty'])/final_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(final_data['Empty_count'][(final_data['Empty_count'] != 0) & (final_data['label_original'] == -1)])\n",
    "axs[1].hist(final_data['Empty_count'][(final_data['Empty_count'] != 0) & (final_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_original</th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_original  has_empty\n",
       "0              -1        327\n",
       "1               1        432"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Final preparations for full augmented training data & k-fold training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Create dictionary of all characters in the NT sequence \n",
    "##################################################################################\n",
    "all_char_set = set({})\n",
    "for val in [set(val) for val in train_data['Sequence']]:\n",
    "    all_char_set = all_char_set.union(val)\n",
    "all_char_list = list(all_char_set)\n",
    "all_char_list.sort()\n",
    "all_char_dict = {}\n",
    "for i in range(len(all_char_list)):\n",
    "    all_char_dict[all_char_list[i]] = i\n",
    "\n",
    "##################################################################################\n",
    "##### Generate k-fold training data\n",
    "##################################################################################    \n",
    "\n",
    "folds_df = build_kfold_df(train_data, k=n_fold)\n",
    "folds = build_folds_from_kfold_df(folds_df, \n",
    "                                  indpe_data, \n",
    "                                  all_char_dict)\n",
    "\n",
    "## Write the k-fold dataset to file\n",
    "foldPath = os.path.join(outPath, expName, \"{}fold\".format(n_fold))\n",
    "if(not os.path.isdir(foldPath)):\n",
    "    os.makedirs(foldPath)\n",
    "pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))\n",
    "\n",
    "##################################################################################\n",
    "##### Create OHE of sequence\n",
    "##################################################################################\n",
    "\n",
    "train_data = augmentation(train_data, indpe_data)\n",
    "\n",
    "train_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                        for val in train_data[\"Sequence\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Fix the labels\n",
    "##################################################################################\n",
    "train_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                 for val in train_data[\"label_original\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Extract features and labels, create folds\n",
    "##################################################################################\n",
    "\n",
    "train_features = np.array(list(train_data['OHE_Sequence']))\n",
    "train_labels = np.array(list(train_data['label']))\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], 1))\n",
    "\n",
    "input_seq_shape = train_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prep for Independent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Create OHE of sequence\n",
    "##################################################################################\n",
    "indpe_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                        for val in indpe_data[\"Sequence\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Fix the labels\n",
    "##################################################################################\n",
    "indpe_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                 for val in indpe_data[\"label_original\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Extract features and labels, create folds\n",
    "##################################################################################\n",
    "\n",
    "indpe_features = np.array(list(indpe_data['OHE_Sequence']))\n",
    "indpe_labels = np.array(list(indpe_data['label']))\n",
    "indpe_labels = indpe_labels.reshape((indpe_labels.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2365, 41, 21) (2365, 1) (477, 41, 21) (477, 1)\n",
      "(2365, 41, 21) (2365, 1) (477, 41, 21) (477, 1)\n",
      "(2366, 41, 21) (2366, 1) (476, 41, 21) (476, 1)\n",
      "(2364, 41, 21) (2364, 1) (476, 41, 21) (476, 1)\n",
      "(2366, 41, 21) (2366, 1) (476, 41, 21) (476, 1)\n"
     ]
    }
   ],
   "source": [
    "for fold in folds:\n",
    "    print(fold['X_train'].shape, fold['y_train'].shape, fold['X_test'].shape, fold['y_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/Test model on Fold #0.\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9794\n",
      "Epoch 1: val_loss improved from inf to 0.94786, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 3s 25ms/step - loss: 0.9794 - val_loss: 0.9479\n",
      "Epoch 2/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.9413\n",
      "Epoch 2: val_loss improved from 0.94786 to 0.92372, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.9405 - val_loss: 0.9237\n",
      "Epoch 3/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.9167\n",
      "Epoch 3: val_loss improved from 0.92372 to 0.89837, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.9155 - val_loss: 0.8984\n",
      "Epoch 4/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.8724\n",
      "Epoch 4: val_loss improved from 0.89837 to 0.85620, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.8726 - val_loss: 0.8562\n",
      "Epoch 5/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.8295\n",
      "Epoch 5: val_loss improved from 0.85620 to 0.81709, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.8271 - val_loss: 0.8171\n",
      "Epoch 6/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7551\n",
      "Epoch 6: val_loss improved from 0.81709 to 0.78848, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7551 - val_loss: 0.7885\n",
      "Epoch 7/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.7333\n",
      "Epoch 7: val_loss improved from 0.78848 to 0.77967, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.7262 - val_loss: 0.7797\n",
      "Epoch 8/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.6952\n",
      "Epoch 8: val_loss improved from 0.77967 to 0.77676, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6931 - val_loss: 0.7768\n",
      "Epoch 9/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.6691\n",
      "Epoch 9: val_loss did not improve from 0.77676\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6753 - val_loss: 0.7769\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6524\n",
      "Epoch 10: val_loss did not improve from 0.77676\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6524 - val_loss: 0.7913\n",
      "Epoch 11/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.6543\n",
      "Epoch 11: val_loss improved from 0.77676 to 0.76155, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6559 - val_loss: 0.7615\n",
      "Epoch 12/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6135\n",
      "Epoch 12: val_loss improved from 0.76155 to 0.76080, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6145 - val_loss: 0.7608\n",
      "Epoch 13/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.6178\n",
      "Epoch 13: val_loss did not improve from 0.76080\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6150 - val_loss: 0.7993\n",
      "Epoch 14/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.6142\n",
      "Epoch 14: val_loss improved from 0.76080 to 0.75504, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold0.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6045 - val_loss: 0.7550\n",
      "Epoch 15/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.5926\n",
      "Epoch 15: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5878 - val_loss: 0.7675\n",
      "Epoch 16/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.5658\n",
      "Epoch 16: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5690 - val_loss: 0.7708\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5515\n",
      "Epoch 17: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5515 - val_loss: 0.7561\n",
      "Epoch 18/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.5545\n",
      "Epoch 18: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5539 - val_loss: 0.7573\n",
      "Epoch 19/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5224\n",
      "Epoch 19: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5223 - val_loss: 0.7583\n",
      "Epoch 20/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.5227\n",
      "Epoch 20: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5221 - val_loss: 0.7746\n",
      "Epoch 21/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4901\n",
      "Epoch 21: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4886 - val_loss: 0.7943\n",
      "Epoch 22/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4816\n",
      "Epoch 22: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4765 - val_loss: 0.7796\n",
      "Epoch 23/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4524\n",
      "Epoch 23: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4582 - val_loss: 0.8349\n",
      "Epoch 24/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4566\n",
      "Epoch 24: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4588 - val_loss: 0.8019\n",
      "Epoch 25/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.4523\n",
      "Epoch 25: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4533 - val_loss: 0.8085\n",
      "Epoch 26/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4414\n",
      "Epoch 26: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4376 - val_loss: 0.7938\n",
      "Epoch 27/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4296\n",
      "Epoch 27: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4276 - val_loss: 0.8154\n",
      "Epoch 28/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4087\n",
      "Epoch 28: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4146 - val_loss: 0.8747\n",
      "Epoch 29/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.4008\n",
      "Epoch 29: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3982 - val_loss: 0.7954\n",
      "Epoch 30/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3899\n",
      "Epoch 30: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3807 - val_loss: 0.8343\n",
      "Epoch 31/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3749\n",
      "Epoch 31: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.3754 - val_loss: 0.8337\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3679\n",
      "Epoch 32: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3721 - val_loss: 0.8276\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3655\n",
      "Epoch 33: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3655 - val_loss: 0.8023\n",
      "Epoch 34/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3791\n",
      "Epoch 34: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3749 - val_loss: 0.8501\n",
      "Epoch 35/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3393\n",
      "Epoch 35: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3396 - val_loss: 0.8156\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3482\n",
      "Epoch 36: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3482 - val_loss: 0.8784\n",
      "Epoch 37/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3561\n",
      "Epoch 37: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3551 - val_loss: 0.8072\n",
      "Epoch 38/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3163\n",
      "Epoch 38: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3184 - val_loss: 0.8786\n",
      "Epoch 39/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3115\n",
      "Epoch 39: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3191 - val_loss: 0.8523\n",
      "Epoch 40/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.3228\n",
      "Epoch 40: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3241 - val_loss: 0.8796\n",
      "Epoch 41/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3223\n",
      "Epoch 41: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3175 - val_loss: 0.9154\n",
      "Epoch 42/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.3211\n",
      "Epoch 42: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3224 - val_loss: 0.8516\n",
      "Epoch 43/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2957\n",
      "Epoch 43: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2983 - val_loss: 0.9075\n",
      "Epoch 44/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3133\n",
      "Epoch 44: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3096 - val_loss: 0.8372\n",
      "Epoch 45/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2790\n",
      "Epoch 45: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2802 - val_loss: 0.8831\n",
      "Epoch 46/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2881\n",
      "Epoch 46: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2841 - val_loss: 0.8927\n",
      "Epoch 47/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2743\n",
      "Epoch 47: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2793 - val_loss: 0.8720\n",
      "Epoch 48/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2791\n",
      "Epoch 48: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2765 - val_loss: 0.9010\n",
      "Epoch 49/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2732\n",
      "Epoch 49: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2734 - val_loss: 0.8828\n",
      "Epoch 50/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2610\n",
      "Epoch 50: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2589 - val_loss: 0.8948\n",
      "Epoch 51/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2570\n",
      "Epoch 51: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2504 - val_loss: 0.8893\n",
      "Epoch 52/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2451\n",
      "Epoch 52: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2550 - val_loss: 0.9197\n",
      "Epoch 53/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2755\n",
      "Epoch 53: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2741 - val_loss: 0.8824\n",
      "Epoch 54/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2773\n",
      "Epoch 54: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2780 - val_loss: 0.8836\n",
      "Epoch 55/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2635\n",
      "Epoch 55: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2631 - val_loss: 0.9147\n",
      "Epoch 56/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2452\n",
      "Epoch 56: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2437 - val_loss: 0.9336\n",
      "Epoch 57/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2735\n",
      "Epoch 57: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2667 - val_loss: 0.9117\n",
      "Epoch 58/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2467\n",
      "Epoch 58: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2462 - val_loss: 0.9270\n",
      "Epoch 59/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2354\n",
      "Epoch 59: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2352 - val_loss: 0.9145\n",
      "Epoch 60/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2456\n",
      "Epoch 60: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2444 - val_loss: 0.9136\n",
      "Epoch 61/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2413\n",
      "Epoch 61: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2417 - val_loss: 0.9780\n",
      "Epoch 62/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2383\n",
      "Epoch 62: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2400 - val_loss: 0.8899\n",
      "Epoch 63/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2340\n",
      "Epoch 63: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2412 - val_loss: 0.9476\n",
      "Epoch 64/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2321\n",
      "Epoch 64: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2344 - val_loss: 0.9645\n",
      "Epoch 65/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2378\n",
      "Epoch 65: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2395 - val_loss: 0.9045\n",
      "Epoch 66/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2179\n",
      "Epoch 66: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2192 - val_loss: 0.9979\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2176\n",
      "Epoch 67: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2176 - val_loss: 1.0133\n",
      "Epoch 68/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2081\n",
      "Epoch 68: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2078 - val_loss: 1.0623\n",
      "Epoch 69/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2330\n",
      "Epoch 69: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2331 - val_loss: 0.9890\n",
      "Epoch 70/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2182\n",
      "Epoch 70: val_loss did not improve from 0.75504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2159 - val_loss: 0.9381\n",
      "Epoch 71/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2176\n",
      "Epoch 71: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2184 - val_loss: 1.0674\n",
      "Epoch 72/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.2178\n",
      "Epoch 72: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2198 - val_loss: 0.9368\n",
      "Epoch 73/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2140\n",
      "Epoch 73: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2124 - val_loss: 1.0481\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2382\n",
      "Epoch 74: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2382 - val_loss: 0.9732\n",
      "Epoch 75/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2155\n",
      "Epoch 75: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2137 - val_loss: 1.0099\n",
      "Epoch 76/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2028\n",
      "Epoch 76: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1996 - val_loss: 0.9917\n",
      "Epoch 77/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.2129\n",
      "Epoch 77: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2200 - val_loss: 0.9330\n",
      "Epoch 78/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2008\n",
      "Epoch 78: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1964 - val_loss: 1.0660\n",
      "Epoch 79/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2176\n",
      "Epoch 79: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2105 - val_loss: 0.9626\n",
      "Epoch 80/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2035\n",
      "Epoch 80: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2022 - val_loss: 0.9623\n",
      "Epoch 81/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2020\n",
      "Epoch 81: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2042 - val_loss: 0.9834\n",
      "Epoch 82/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.1784\n",
      "Epoch 82: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.1779 - val_loss: 1.0506\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2076\n",
      "Epoch 83: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2076 - val_loss: 0.9741\n",
      "Epoch 84/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1936\n",
      "Epoch 84: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1977 - val_loss: 1.0107\n",
      "Epoch 85/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2007\n",
      "Epoch 85: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2006 - val_loss: 0.9582\n",
      "Epoch 86/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.1936\n",
      "Epoch 86: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1959 - val_loss: 0.9411\n",
      "Epoch 87/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2078\n",
      "Epoch 87: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2077 - val_loss: 0.9469\n",
      "Epoch 88/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2015\n",
      "Epoch 88: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2036 - val_loss: 0.9600\n",
      "Epoch 89/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.1977\n",
      "Epoch 89: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2006 - val_loss: 1.0370\n",
      "Epoch 90/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.1804\n",
      "Epoch 90: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1836 - val_loss: 1.0261\n",
      "Epoch 91/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2054\n",
      "Epoch 91: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2020 - val_loss: 0.9574\n",
      "Epoch 92/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1985\n",
      "Epoch 92: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2029 - val_loss: 0.9863\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1926\n",
      "Epoch 93: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1926 - val_loss: 0.9642\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1907\n",
      "Epoch 94: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1907 - val_loss: 1.0039\n",
      "Epoch 95/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1767\n",
      "Epoch 95: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1751 - val_loss: 1.0288\n",
      "Epoch 96/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1896\n",
      "Epoch 96: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1920 - val_loss: 1.0276\n",
      "Epoch 97/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.1850\n",
      "Epoch 97: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1848 - val_loss: 1.0620\n",
      "Epoch 98/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1814\n",
      "Epoch 98: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1839 - val_loss: 1.0773\n",
      "Epoch 99/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1893\n",
      "Epoch 99: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1878 - val_loss: 1.0544\n",
      "Epoch 100/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1836\n",
      "Epoch 100: val_loss did not improve from 0.75504\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1841 - val_loss: 1.0902\n",
      "\n",
      "Train/Test model on Fold #1.\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9779\n",
      "Epoch 1: val_loss improved from inf to 0.95296, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 3s 24ms/step - loss: 0.9779 - val_loss: 0.9530\n",
      "Epoch 2/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.9457\n",
      "Epoch 2: val_loss improved from 0.95296 to 0.92824, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.9456 - val_loss: 0.9282\n",
      "Epoch 3/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.9240\n",
      "Epoch 3: val_loss improved from 0.92824 to 0.90596, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.9237 - val_loss: 0.9060\n",
      "Epoch 4/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.9009\n",
      "Epoch 4: val_loss improved from 0.90596 to 0.88230, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.8977 - val_loss: 0.8823\n",
      "Epoch 5/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.8726\n",
      "Epoch 5: val_loss improved from 0.88230 to 0.83383, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.8698 - val_loss: 0.8338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.8175\n",
      "Epoch 6: val_loss improved from 0.83383 to 0.76843, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.8180 - val_loss: 0.7684\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7758\n",
      "Epoch 7: val_loss improved from 0.76843 to 0.71691, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.7758 - val_loss: 0.7169\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.7468\n",
      "Epoch 8: val_loss improved from 0.71691 to 0.71043, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.7468 - val_loss: 0.7104\n",
      "Epoch 9/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.7211\n",
      "Epoch 9: val_loss improved from 0.71043 to 0.69377, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7211 - val_loss: 0.6938\n",
      "Epoch 10/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.7035\n",
      "Epoch 10: val_loss improved from 0.69377 to 0.67089, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.7035 - val_loss: 0.6709\n",
      "Epoch 11/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.6747\n",
      "Epoch 11: val_loss did not improve from 0.67089\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6799 - val_loss: 0.6840\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6864\n",
      "Epoch 12: val_loss improved from 0.67089 to 0.65907, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.6864 - val_loss: 0.6591\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6494\n",
      "Epoch 13: val_loss improved from 0.65907 to 0.63609, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.6494 - val_loss: 0.6361\n",
      "Epoch 14/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.6361\n",
      "Epoch 14: val_loss did not improve from 0.63609\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6386 - val_loss: 0.6496\n",
      "Epoch 15/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.6156\n",
      "Epoch 15: val_loss improved from 0.63609 to 0.62384, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.6177 - val_loss: 0.6238\n",
      "Epoch 16/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.5974\n",
      "Epoch 16: val_loss did not improve from 0.62384\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5952 - val_loss: 0.6413\n",
      "Epoch 17/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5990\n",
      "Epoch 17: val_loss improved from 0.62384 to 0.61519, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.5977 - val_loss: 0.6152\n",
      "Epoch 18/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.5814\n",
      "Epoch 18: val_loss improved from 0.61519 to 0.60878, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.5832 - val_loss: 0.6088\n",
      "Epoch 19/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.5558\n",
      "Epoch 19: val_loss improved from 0.60878 to 0.60871, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.5539 - val_loss: 0.6087\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5495\n",
      "Epoch 20: val_loss did not improve from 0.60871\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5495 - val_loss: 0.6156\n",
      "Epoch 21/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.5282\n",
      "Epoch 21: val_loss improved from 0.60871 to 0.60654, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold1.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.5308 - val_loss: 0.6065\n",
      "Epoch 22/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.5190\n",
      "Epoch 22: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5152 - val_loss: 0.6126\n",
      "Epoch 23/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.5126\n",
      "Epoch 23: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5089 - val_loss: 0.6164\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4778\n",
      "Epoch 24: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.4778 - val_loss: 0.6143\n",
      "Epoch 25/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4634\n",
      "Epoch 25: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4617 - val_loss: 0.6140\n",
      "Epoch 26/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4572\n",
      "Epoch 26: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4569 - val_loss: 0.6430\n",
      "Epoch 27/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4337\n",
      "Epoch 27: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4426 - val_loss: 0.6229\n",
      "Epoch 28/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4220\n",
      "Epoch 28: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.4215 - val_loss: 0.6491\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4077\n",
      "Epoch 29: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.4077 - val_loss: 0.6645\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4096\n",
      "Epoch 30: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.4096 - val_loss: 0.6562\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3952\n",
      "Epoch 31: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3952 - val_loss: 0.6594\n",
      "Epoch 32/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3824\n",
      "Epoch 32: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3776 - val_loss: 0.6710\n",
      "Epoch 33/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3782\n",
      "Epoch 33: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3781 - val_loss: 0.6698\n",
      "Epoch 34/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3686\n",
      "Epoch 34: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3679 - val_loss: 0.6689\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3931\n",
      "Epoch 35: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.3931 - val_loss: 0.6928\n",
      "Epoch 36/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3624\n",
      "Epoch 36: val_loss did not improve from 0.60654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 12ms/step - loss: 0.3627 - val_loss: 0.6977\n",
      "Epoch 37/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3585\n",
      "Epoch 37: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3547 - val_loss: 0.6551\n",
      "Epoch 38/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3275\n",
      "Epoch 38: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.3299 - val_loss: 0.6934\n",
      "Epoch 39/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3277\n",
      "Epoch 39: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.3274 - val_loss: 0.6885\n",
      "Epoch 40/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3341\n",
      "Epoch 40: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3327 - val_loss: 0.7020\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3277\n",
      "Epoch 41: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3277 - val_loss: 0.6828\n",
      "Epoch 42/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3294\n",
      "Epoch 42: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3302 - val_loss: 0.6806\n",
      "Epoch 43/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3121\n",
      "Epoch 43: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3106 - val_loss: 0.6967\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3009\n",
      "Epoch 44: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3009 - val_loss: 0.6907\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2986\n",
      "Epoch 45: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2986 - val_loss: 0.7145\n",
      "Epoch 46/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2965\n",
      "Epoch 46: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2975 - val_loss: 0.6933\n",
      "Epoch 47/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2975\n",
      "Epoch 47: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2952 - val_loss: 0.7262\n",
      "Epoch 48/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3124\n",
      "Epoch 48: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3111 - val_loss: 0.7223\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2913\n",
      "Epoch 49: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2913 - val_loss: 0.7157\n",
      "Epoch 50/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2785\n",
      "Epoch 50: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2820 - val_loss: 0.7155\n",
      "Epoch 51/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2815\n",
      "Epoch 51: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.2798 - val_loss: 0.7343\n",
      "Epoch 52/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2580\n",
      "Epoch 52: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.2581 - val_loss: 0.7262\n",
      "Epoch 53/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2663\n",
      "Epoch 53: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2644 - val_loss: 0.7304\n",
      "Epoch 54/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2608\n",
      "Epoch 54: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2635 - val_loss: 0.7622\n",
      "Epoch 55/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2539\n",
      "Epoch 55: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2529 - val_loss: 0.7510\n",
      "Epoch 56/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2466\n",
      "Epoch 56: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2517 - val_loss: 0.7569\n",
      "Epoch 57/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2574\n",
      "Epoch 57: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.2581 - val_loss: 0.7597\n",
      "Epoch 58/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2429\n",
      "Epoch 58: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2484 - val_loss: 0.7654\n",
      "Epoch 59/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2420\n",
      "Epoch 59: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2389 - val_loss: 0.7553\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2557\n",
      "Epoch 60: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2557 - val_loss: 0.7516\n",
      "Epoch 61/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2549\n",
      "Epoch 61: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2528 - val_loss: 0.7613\n",
      "Epoch 62/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2631\n",
      "Epoch 62: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2614 - val_loss: 0.7493\n",
      "Epoch 63/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2429\n",
      "Epoch 63: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2385 - val_loss: 0.7630\n",
      "Epoch 64/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2481\n",
      "Epoch 64: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2454 - val_loss: 0.7679\n",
      "Epoch 65/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2419\n",
      "Epoch 65: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2379 - val_loss: 0.7971\n",
      "Epoch 66/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2303\n",
      "Epoch 66: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2313 - val_loss: 0.7799\n",
      "Epoch 67/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2218\n",
      "Epoch 67: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2231 - val_loss: 0.7825\n",
      "Epoch 68/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2329\n",
      "Epoch 68: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2369 - val_loss: 0.7929\n",
      "Epoch 69/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2211\n",
      "Epoch 69: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2211 - val_loss: 0.7937\n",
      "Epoch 70/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2321\n",
      "Epoch 70: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2322 - val_loss: 0.7965\n",
      "Epoch 71/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2349\n",
      "Epoch 71: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2342 - val_loss: 0.7716\n",
      "Epoch 72/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2052\n",
      "Epoch 72: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2117 - val_loss: 0.8036\n",
      "Epoch 73/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2334\n",
      "Epoch 73: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2326 - val_loss: 0.7659\n",
      "Epoch 74/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2274\n",
      "Epoch 74: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.2242 - val_loss: 0.7922\n",
      "Epoch 75/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2047\n",
      "Epoch 75: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2120 - val_loss: 0.7869\n",
      "Epoch 76/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2140\n",
      "Epoch 76: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2123 - val_loss: 0.7976\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2063\n",
      "Epoch 77: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2063 - val_loss: 0.8116\n",
      "Epoch 78/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2288\n",
      "Epoch 78: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2305 - val_loss: 0.7938\n",
      "Epoch 79/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2097\n",
      "Epoch 79: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2148 - val_loss: 0.7909\n",
      "Epoch 80/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2027\n",
      "Epoch 80: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2001 - val_loss: 0.8415\n",
      "Epoch 81/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1935\n",
      "Epoch 81: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1989 - val_loss: 0.8453\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2001\n",
      "Epoch 82: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2001 - val_loss: 0.8358\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2025\n",
      "Epoch 83: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2025 - val_loss: 0.8211\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2243\n",
      "Epoch 84: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2243 - val_loss: 0.7849\n",
      "Epoch 85/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2186\n",
      "Epoch 85: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2132 - val_loss: 0.8696\n",
      "Epoch 86/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2060\n",
      "Epoch 86: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2017 - val_loss: 0.8202\n",
      "Epoch 87/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.1995\n",
      "Epoch 87: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2011 - val_loss: 0.9020\n",
      "Epoch 88/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1922\n",
      "Epoch 88: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1937 - val_loss: 0.8196\n",
      "Epoch 89/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2043\n",
      "Epoch 89: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2032 - val_loss: 0.9160\n",
      "Epoch 90/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2093\n",
      "Epoch 90: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2089 - val_loss: 0.8240\n",
      "Epoch 91/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.1865\n",
      "Epoch 91: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1897 - val_loss: 0.8398\n",
      "Epoch 92/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1973\n",
      "Epoch 92: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1956 - val_loss: 0.8300\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1860\n",
      "Epoch 93: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1860 - val_loss: 0.8303\n",
      "Epoch 94/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.1790\n",
      "Epoch 94: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1786 - val_loss: 0.8618\n",
      "Epoch 95/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1964\n",
      "Epoch 95: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1944 - val_loss: 0.8772\n",
      "Epoch 96/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1753\n",
      "Epoch 96: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1819 - val_loss: 0.8803\n",
      "Epoch 97/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2028\n",
      "Epoch 97: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1985 - val_loss: 0.8440\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1864\n",
      "Epoch 98: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1864 - val_loss: 0.9365\n",
      "Epoch 99/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1835\n",
      "Epoch 99: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1888 - val_loss: 0.8388\n",
      "Epoch 100/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2011\n",
      "Epoch 100: val_loss did not improve from 0.60654\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1960 - val_loss: 0.8514\n",
      "\n",
      "Train/Test model on Fold #2.\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9848\n",
      "Epoch 1: val_loss improved from inf to 0.94810, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 3s 27ms/step - loss: 0.9848 - val_loss: 0.9481\n",
      "Epoch 2/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.9505\n",
      "Epoch 2: val_loss improved from 0.94810 to 0.92100, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.9500 - val_loss: 0.9210\n",
      "Epoch 3/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.9222\n",
      "Epoch 3: val_loss improved from 0.92100 to 0.89277, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.9212 - val_loss: 0.8928\n",
      "Epoch 4/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.8832\n",
      "Epoch 4: val_loss improved from 0.89277 to 0.84587, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.8808 - val_loss: 0.8459\n",
      "Epoch 5/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.8309\n",
      "Epoch 5: val_loss improved from 0.84587 to 0.79687, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.8275 - val_loss: 0.7969\n",
      "Epoch 6/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.7729\n",
      "Epoch 6: val_loss improved from 0.79687 to 0.76978, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7714 - val_loss: 0.7698\n",
      "Epoch 7/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.7426\n",
      "Epoch 7: val_loss improved from 0.76978 to 0.75648, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7459 - val_loss: 0.7565\n",
      "Epoch 8/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.7308\n",
      "Epoch 8: val_loss improved from 0.75648 to 0.74517, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7296 - val_loss: 0.7452\n",
      "Epoch 9/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.7016\n",
      "Epoch 9: val_loss improved from 0.74517 to 0.73267, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6972 - val_loss: 0.7327\n",
      "Epoch 10/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6835\n",
      "Epoch 10: val_loss improved from 0.73267 to 0.72646, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6837 - val_loss: 0.7265\n",
      "Epoch 11/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6678\n",
      "Epoch 11: val_loss improved from 0.72646 to 0.71728, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6660 - val_loss: 0.7173\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6582\n",
      "Epoch 12: val_loss improved from 0.71728 to 0.71053, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6582 - val_loss: 0.7105\n",
      "Epoch 13/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.6545\n",
      "Epoch 13: val_loss improved from 0.71053 to 0.70083, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6503 - val_loss: 0.7008\n",
      "Epoch 14/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6339\n",
      "Epoch 14: val_loss improved from 0.70083 to 0.69784, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6335 - val_loss: 0.6978\n",
      "Epoch 15/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6123\n",
      "Epoch 15: val_loss improved from 0.69784 to 0.69523, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6078 - val_loss: 0.6952\n",
      "Epoch 16/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.6053\n",
      "Epoch 16: val_loss improved from 0.69523 to 0.68803, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6023 - val_loss: 0.6880\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5917\n",
      "Epoch 17: val_loss did not improve from 0.68803\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5917 - val_loss: 0.6919\n",
      "Epoch 18/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5686\n",
      "Epoch 18: val_loss did not improve from 0.68803\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5695 - val_loss: 0.6882\n",
      "Epoch 19/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.5656\n",
      "Epoch 19: val_loss improved from 0.68803 to 0.67704, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.5658 - val_loss: 0.6770\n",
      "Epoch 20/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5471\n",
      "Epoch 20: val_loss improved from 0.67704 to 0.67312, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.5482 - val_loss: 0.6731\n",
      "Epoch 21/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.5326\n",
      "Epoch 21: val_loss did not improve from 0.67312\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5362 - val_loss: 0.6777\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5244\n",
      "Epoch 22: val_loss did not improve from 0.67312\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5244 - val_loss: 0.6781\n",
      "Epoch 23/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.5039\n",
      "Epoch 23: val_loss did not improve from 0.67312\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4985 - val_loss: 0.6771\n",
      "Epoch 24/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.5013\n",
      "Epoch 24: val_loss did not improve from 0.67312\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4996 - val_loss: 0.6771\n",
      "Epoch 25/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4849\n",
      "Epoch 25: val_loss did not improve from 0.67312\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4892 - val_loss: 0.6789\n",
      "Epoch 26/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4802\n",
      "Epoch 26: val_loss improved from 0.67312 to 0.67229, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold2.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.4804 - val_loss: 0.6723\n",
      "Epoch 27/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4536\n",
      "Epoch 27: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4554 - val_loss: 0.6948\n",
      "Epoch 28/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4521\n",
      "Epoch 28: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4501 - val_loss: 0.6805\n",
      "Epoch 29/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4288\n",
      "Epoch 29: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4290 - val_loss: 0.6922\n",
      "Epoch 30/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4082\n",
      "Epoch 30: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4090 - val_loss: 0.6992\n",
      "Epoch 31/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4106\n",
      "Epoch 31: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4101 - val_loss: 0.6868\n",
      "Epoch 32/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4104\n",
      "Epoch 32: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4113 - val_loss: 0.7086\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3947\n",
      "Epoch 33: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3947 - val_loss: 0.7116\n",
      "Epoch 34/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3940\n",
      "Epoch 34: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3916 - val_loss: 0.6957\n",
      "Epoch 35/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3670\n",
      "Epoch 35: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3656 - val_loss: 0.7878\n",
      "Epoch 36/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3747\n",
      "Epoch 36: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3718 - val_loss: 0.7099\n",
      "Epoch 37/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3640\n",
      "Epoch 37: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3668 - val_loss: 0.7819\n",
      "Epoch 38/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3682\n",
      "Epoch 38: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3665 - val_loss: 0.7170\n",
      "Epoch 39/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3490\n",
      "Epoch 39: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3483 - val_loss: 0.7148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3551\n",
      "Epoch 40: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3551 - val_loss: 0.7439\n",
      "Epoch 41/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3320\n",
      "Epoch 41: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3319 - val_loss: 0.7385\n",
      "Epoch 42/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3325\n",
      "Epoch 42: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3342 - val_loss: 0.7936\n",
      "Epoch 43/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3376\n",
      "Epoch 43: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3353 - val_loss: 0.7326\n",
      "Epoch 44/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3290\n",
      "Epoch 44: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3302 - val_loss: 0.7432\n",
      "Epoch 45/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3202\n",
      "Epoch 45: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3181 - val_loss: 0.7625\n",
      "Epoch 46/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3058\n",
      "Epoch 46: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3102 - val_loss: 0.7637\n",
      "Epoch 47/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3047\n",
      "Epoch 47: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3089 - val_loss: 0.7733\n",
      "Epoch 48/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3245\n",
      "Epoch 48: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3235 - val_loss: 0.7314\n",
      "Epoch 49/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3199\n",
      "Epoch 49: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3174 - val_loss: 0.7544\n",
      "Epoch 50/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2891\n",
      "Epoch 50: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2908 - val_loss: 0.7981\n",
      "Epoch 51/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2914\n",
      "Epoch 51: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2880 - val_loss: 0.7855\n",
      "Epoch 52/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2722\n",
      "Epoch 52: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2749 - val_loss: 0.7934\n",
      "Epoch 53/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2789\n",
      "Epoch 53: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2807 - val_loss: 0.8276\n",
      "Epoch 54/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2846\n",
      "Epoch 54: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2857 - val_loss: 0.7758\n",
      "Epoch 55/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2595\n",
      "Epoch 55: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2598 - val_loss: 0.8515\n",
      "Epoch 56/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2789\n",
      "Epoch 56: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2790 - val_loss: 0.8332\n",
      "Epoch 57/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2791\n",
      "Epoch 57: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2811 - val_loss: 0.8253\n",
      "Epoch 58/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2804\n",
      "Epoch 58: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2761 - val_loss: 0.7803\n",
      "Epoch 59/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2593\n",
      "Epoch 59: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2637 - val_loss: 0.8141\n",
      "Epoch 60/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2381\n",
      "Epoch 60: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2376 - val_loss: 0.8321\n",
      "Epoch 61/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2624\n",
      "Epoch 61: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2618 - val_loss: 0.8340\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2693\n",
      "Epoch 62: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2693 - val_loss: 0.8087\n",
      "Epoch 63/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2606\n",
      "Epoch 63: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2600 - val_loss: 0.8396\n",
      "Epoch 64/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2433\n",
      "Epoch 64: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2462 - val_loss: 0.7960\n",
      "Epoch 65/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2446\n",
      "Epoch 65: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2451 - val_loss: 0.8178\n",
      "Epoch 66/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2423\n",
      "Epoch 66: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2417 - val_loss: 0.8812\n",
      "Epoch 67/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2427\n",
      "Epoch 67: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2435 - val_loss: 0.8137\n",
      "Epoch 68/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2246\n",
      "Epoch 68: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2257 - val_loss: 0.8617\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2357\n",
      "Epoch 69: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2357 - val_loss: 0.8546\n",
      "Epoch 70/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2462\n",
      "Epoch 70: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2445 - val_loss: 0.8052\n",
      "Epoch 71/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.2237\n",
      "Epoch 71: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2314 - val_loss: 0.8427\n",
      "Epoch 72/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2473\n",
      "Epoch 72: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2488 - val_loss: 0.8062\n",
      "Epoch 73/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2270\n",
      "Epoch 73: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2278 - val_loss: 0.8594\n",
      "Epoch 74/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2142\n",
      "Epoch 74: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2206 - val_loss: 0.8380\n",
      "Epoch 75/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2180\n",
      "Epoch 75: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2210 - val_loss: 0.8599\n",
      "Epoch 76/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2249\n",
      "Epoch 76: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2239 - val_loss: 0.8991\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2158\n",
      "Epoch 77: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2158 - val_loss: 0.8927\n",
      "Epoch 78/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2359\n",
      "Epoch 78: val_loss did not improve from 0.67229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2389 - val_loss: 0.8287\n",
      "Epoch 79/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2140\n",
      "Epoch 79: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2135 - val_loss: 0.8810\n",
      "Epoch 80/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.2191\n",
      "Epoch 80: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2294 - val_loss: 0.8339\n",
      "Epoch 81/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2359\n",
      "Epoch 81: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2309 - val_loss: 0.8355\n",
      "Epoch 82/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2071\n",
      "Epoch 82: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2056 - val_loss: 0.8793\n",
      "Epoch 83/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2145\n",
      "Epoch 83: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2142 - val_loss: 0.8618\n",
      "Epoch 84/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2188\n",
      "Epoch 84: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2147 - val_loss: 0.8556\n",
      "Epoch 85/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2168\n",
      "Epoch 85: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2127 - val_loss: 0.8744\n",
      "Epoch 86/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.1976\n",
      "Epoch 86: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2019 - val_loss: 0.8962\n",
      "Epoch 87/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2082\n",
      "Epoch 87: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2088 - val_loss: 0.8507\n",
      "Epoch 88/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2037\n",
      "Epoch 88: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2059 - val_loss: 0.8869\n",
      "Epoch 89/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1916\n",
      "Epoch 89: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2015 - val_loss: 1.0156\n",
      "Epoch 90/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2030\n",
      "Epoch 90: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1984 - val_loss: 0.8902\n",
      "Epoch 91/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2245\n",
      "Epoch 91: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2171 - val_loss: 0.8198\n",
      "Epoch 92/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2021\n",
      "Epoch 92: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2043 - val_loss: 0.8930\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2140\n",
      "Epoch 93: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2140 - val_loss: 0.8368\n",
      "Epoch 94/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2049\n",
      "Epoch 94: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2038 - val_loss: 0.8649\n",
      "Epoch 95/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1990\n",
      "Epoch 95: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1994 - val_loss: 0.8735\n",
      "Epoch 96/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.1982\n",
      "Epoch 96: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2028 - val_loss: 0.8991\n",
      "Epoch 97/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1952\n",
      "Epoch 97: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1996 - val_loss: 0.8504\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1930\n",
      "Epoch 98: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1930 - val_loss: 0.9507\n",
      "Epoch 99/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1984\n",
      "Epoch 99: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1989 - val_loss: 0.9376\n",
      "Epoch 100/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2079\n",
      "Epoch 100: val_loss did not improve from 0.67229\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2073 - val_loss: 0.9235\n",
      "\n",
      "Train/Test model on Fold #3.\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9815\n",
      "Epoch 1: val_loss improved from inf to 0.94550, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 3s 24ms/step - loss: 0.9815 - val_loss: 0.9455\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.9483\n",
      "Epoch 2: val_loss improved from 0.94550 to 0.92259, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.9483 - val_loss: 0.9226\n",
      "Epoch 3/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.9205\n",
      "Epoch 3: val_loss improved from 0.92259 to 0.89539, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.9194 - val_loss: 0.8954\n",
      "Epoch 4/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.8911\n",
      "Epoch 4: val_loss improved from 0.89539 to 0.85404, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.8883 - val_loss: 0.8540\n",
      "Epoch 5/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.8548\n",
      "Epoch 5: val_loss improved from 0.85404 to 0.78254, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.8552 - val_loss: 0.7825\n",
      "Epoch 6/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.8092\n",
      "Epoch 6: val_loss improved from 0.78254 to 0.71699, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.8092 - val_loss: 0.7170\n",
      "Epoch 7/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.7725\n",
      "Epoch 7: val_loss improved from 0.71699 to 0.66497, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7703 - val_loss: 0.6650\n",
      "Epoch 8/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.7558\n",
      "Epoch 8: val_loss improved from 0.66497 to 0.64798, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.7521 - val_loss: 0.6480\n",
      "Epoch 9/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.7311\n",
      "Epoch 9: val_loss improved from 0.64798 to 0.62338, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7283 - val_loss: 0.6234\n",
      "Epoch 10/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.7010\n",
      "Epoch 10: val_loss improved from 0.62338 to 0.61739, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 12ms/step - loss: 0.6978 - val_loss: 0.6174\n",
      "Epoch 11/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.7055\n",
      "Epoch 11: val_loss improved from 0.61739 to 0.60942, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.6999 - val_loss: 0.6094\n",
      "Epoch 12/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.6760\n",
      "Epoch 12: val_loss improved from 0.60942 to 0.58600, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6744 - val_loss: 0.5860\n",
      "Epoch 13/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.6635\n",
      "Epoch 13: val_loss improved from 0.58600 to 0.58175, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6656 - val_loss: 0.5818\n",
      "Epoch 14/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.6402\n",
      "Epoch 14: val_loss improved from 0.58175 to 0.57109, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6381 - val_loss: 0.5711\n",
      "Epoch 15/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.6205\n",
      "Epoch 15: val_loss improved from 0.57109 to 0.56630, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6215 - val_loss: 0.5663\n",
      "Epoch 16/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.6162\n",
      "Epoch 16: val_loss improved from 0.56630 to 0.56002, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6165 - val_loss: 0.5600\n",
      "Epoch 17/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5960\n",
      "Epoch 17: val_loss improved from 0.56002 to 0.54785, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5977 - val_loss: 0.5478\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5848\n",
      "Epoch 18: val_loss improved from 0.54785 to 0.54014, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5848 - val_loss: 0.5401\n",
      "Epoch 19/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.5658\n",
      "Epoch 19: val_loss improved from 0.54014 to 0.53413, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.5673 - val_loss: 0.5341\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5597\n",
      "Epoch 20: val_loss improved from 0.53413 to 0.53181, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.5597 - val_loss: 0.5318\n",
      "Epoch 21/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.5448\n",
      "Epoch 21: val_loss did not improve from 0.53181\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5491 - val_loss: 0.5458\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5416\n",
      "Epoch 22: val_loss did not improve from 0.53181\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5416 - val_loss: 0.5649\n",
      "Epoch 23/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.5189\n",
      "Epoch 23: val_loss improved from 0.53181 to 0.52552, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.5135 - val_loss: 0.5255\n",
      "Epoch 24/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.5014\n",
      "Epoch 24: val_loss improved from 0.52552 to 0.51833, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.4987 - val_loss: 0.5183\n",
      "Epoch 25/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4876\n",
      "Epoch 25: val_loss improved from 0.51833 to 0.51626, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.4849 - val_loss: 0.5163\n",
      "Epoch 26/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.4812\n",
      "Epoch 26: val_loss did not improve from 0.51626\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4809 - val_loss: 0.5285\n",
      "Epoch 27/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4700\n",
      "Epoch 27: val_loss improved from 0.51626 to 0.50982, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4729 - val_loss: 0.5098\n",
      "Epoch 28/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4655\n",
      "Epoch 28: val_loss improved from 0.50982 to 0.50655, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4665 - val_loss: 0.5065\n",
      "Epoch 29/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4416\n",
      "Epoch 29: val_loss improved from 0.50655 to 0.50066, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4406 - val_loss: 0.5007\n",
      "Epoch 30/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4393\n",
      "Epoch 30: val_loss did not improve from 0.50066\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4391 - val_loss: 0.5135\n",
      "Epoch 31/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.4153\n",
      "Epoch 31: val_loss did not improve from 0.50066\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4167 - val_loss: 0.5254\n",
      "Epoch 32/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4133\n",
      "Epoch 32: val_loss did not improve from 0.50066\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4183 - val_loss: 0.5052\n",
      "Epoch 33/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4001\n",
      "Epoch 33: val_loss did not improve from 0.50066\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4004 - val_loss: 0.5410\n",
      "Epoch 34/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3974\n",
      "Epoch 34: val_loss did not improve from 0.50066\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3975 - val_loss: 0.5085\n",
      "Epoch 35/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3891\n",
      "Epoch 35: val_loss did not improve from 0.50066\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3926 - val_loss: 0.5178\n",
      "Epoch 36/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3730\n",
      "Epoch 36: val_loss did not improve from 0.50066\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3696 - val_loss: 0.5083\n",
      "Epoch 37/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3700\n",
      "Epoch 37: val_loss improved from 0.50066 to 0.49731, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold3.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.3722 - val_loss: 0.4973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3737\n",
      "Epoch 38: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3737 - val_loss: 0.5216\n",
      "Epoch 39/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3633\n",
      "Epoch 39: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3650 - val_loss: 0.4986\n",
      "Epoch 40/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3699\n",
      "Epoch 40: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3703 - val_loss: 0.5253\n",
      "Epoch 41/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3417\n",
      "Epoch 41: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3371 - val_loss: 0.5459\n",
      "Epoch 42/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3492\n",
      "Epoch 42: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3455 - val_loss: 0.5121\n",
      "Epoch 43/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3345\n",
      "Epoch 43: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3360 - val_loss: 0.5160\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.3276\n",
      "Epoch 44: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3276 - val_loss: 0.5179\n",
      "Epoch 45/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3194\n",
      "Epoch 45: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3186 - val_loss: 0.5975\n",
      "Epoch 46/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3298\n",
      "Epoch 46: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3256 - val_loss: 0.5374\n",
      "Epoch 47/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3075\n",
      "Epoch 47: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3094 - val_loss: 0.5261\n",
      "Epoch 48/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2952\n",
      "Epoch 48: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2965 - val_loss: 0.5364\n",
      "Epoch 49/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3165\n",
      "Epoch 49: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3194 - val_loss: 0.5253\n",
      "Epoch 50/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2835\n",
      "Epoch 50: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2863 - val_loss: 0.6100\n",
      "Epoch 51/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3028\n",
      "Epoch 51: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2972 - val_loss: 0.5533\n",
      "Epoch 52/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2782\n",
      "Epoch 52: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2811 - val_loss: 0.5481\n",
      "Epoch 53/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2789\n",
      "Epoch 53: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2860 - val_loss: 0.5336\n",
      "Epoch 54/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2806\n",
      "Epoch 54: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2886 - val_loss: 0.5470\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2944\n",
      "Epoch 55: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2944 - val_loss: 0.5342\n",
      "Epoch 56/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.2795\n",
      "Epoch 56: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2862 - val_loss: 0.5251\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2764\n",
      "Epoch 57: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2764 - val_loss: 0.5533\n",
      "Epoch 58/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2855\n",
      "Epoch 58: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2846 - val_loss: 0.5361\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2676\n",
      "Epoch 59: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2676 - val_loss: 0.5524\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2691\n",
      "Epoch 60: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2691 - val_loss: 0.5381\n",
      "Epoch 61/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2566\n",
      "Epoch 61: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2544 - val_loss: 0.5525\n",
      "Epoch 62/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2569\n",
      "Epoch 62: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2582 - val_loss: 0.5439\n",
      "Epoch 63/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2649\n",
      "Epoch 63: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2605 - val_loss: 0.5790\n",
      "Epoch 64/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2563\n",
      "Epoch 64: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2571 - val_loss: 0.5932\n",
      "Epoch 65/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2612\n",
      "Epoch 65: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2665 - val_loss: 0.5763\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2399\n",
      "Epoch 66: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2399 - val_loss: 0.5871\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2620\n",
      "Epoch 67: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2620 - val_loss: 0.5833\n",
      "Epoch 68/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2430\n",
      "Epoch 68: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2436 - val_loss: 0.5963\n",
      "Epoch 69/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2288\n",
      "Epoch 69: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2250 - val_loss: 0.6693\n",
      "Epoch 70/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2477\n",
      "Epoch 70: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2399 - val_loss: 0.5762\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2415\n",
      "Epoch 71: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2415 - val_loss: 0.5981\n",
      "Epoch 72/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2339\n",
      "Epoch 72: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2306 - val_loss: 0.6059\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2413\n",
      "Epoch 73: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2413 - val_loss: 0.5991\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2320\n",
      "Epoch 74: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2320 - val_loss: 0.5976\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2353\n",
      "Epoch 75: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2353 - val_loss: 0.5858\n",
      "Epoch 76/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2462\n",
      "Epoch 76: val_loss did not improve from 0.49731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2455 - val_loss: 0.5785\n",
      "Epoch 77/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2207\n",
      "Epoch 77: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2237 - val_loss: 0.6458\n",
      "Epoch 78/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2446\n",
      "Epoch 78: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.2435 - val_loss: 0.5835\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2344\n",
      "Epoch 79: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2344 - val_loss: 0.6146\n",
      "Epoch 80/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2043\n",
      "Epoch 80: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2104 - val_loss: 0.6370\n",
      "Epoch 81/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2352\n",
      "Epoch 81: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2361 - val_loss: 0.6031\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2083\n",
      "Epoch 82: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2083 - val_loss: 0.6507\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2132\n",
      "Epoch 83: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2132 - val_loss: 0.6295\n",
      "Epoch 84/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2235\n",
      "Epoch 84: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2279 - val_loss: 0.6388\n",
      "Epoch 85/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2113\n",
      "Epoch 85: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2076 - val_loss: 0.6253\n",
      "Epoch 86/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2134\n",
      "Epoch 86: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2190 - val_loss: 0.6068\n",
      "Epoch 87/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2258\n",
      "Epoch 87: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2262 - val_loss: 0.6215\n",
      "Epoch 88/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2090\n",
      "Epoch 88: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2063 - val_loss: 0.6508\n",
      "Epoch 89/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2032\n",
      "Epoch 89: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2052 - val_loss: 0.6716\n",
      "Epoch 90/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2203\n",
      "Epoch 90: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2194 - val_loss: 0.6466\n",
      "Epoch 91/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2085\n",
      "Epoch 91: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2064 - val_loss: 0.6575\n",
      "Epoch 92/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.1947\n",
      "Epoch 92: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1916 - val_loss: 0.6716\n",
      "Epoch 93/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.1926\n",
      "Epoch 93: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1948 - val_loss: 0.6541\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.2071\n",
      "Epoch 94: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2071 - val_loss: 0.6726\n",
      "Epoch 95/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1955\n",
      "Epoch 95: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1967 - val_loss: 0.6496\n",
      "Epoch 96/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2131\n",
      "Epoch 96: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2145 - val_loss: 0.6615\n",
      "Epoch 97/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2041\n",
      "Epoch 97: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2069 - val_loss: 0.6604\n",
      "Epoch 98/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.2077\n",
      "Epoch 98: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2067 - val_loss: 0.6557\n",
      "Epoch 99/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1898\n",
      "Epoch 99: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1913 - val_loss: 0.6622\n",
      "Epoch 100/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1942\n",
      "Epoch 100: val_loss did not improve from 0.49731\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2009 - val_loss: 0.6742\n",
      "\n",
      "Train/Test model on Fold #4.\n",
      "Epoch 1/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.9723\n",
      "Epoch 1: val_loss improved from inf to 0.94359, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 2s 22ms/step - loss: 0.9720 - val_loss: 0.9436\n",
      "Epoch 2/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.9324\n",
      "Epoch 2: val_loss improved from 0.94359 to 0.91825, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.9330 - val_loss: 0.9183\n",
      "Epoch 3/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.9078\n",
      "Epoch 3: val_loss improved from 0.91825 to 0.89367, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.9053 - val_loss: 0.8937\n",
      "Epoch 4/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.8596\n",
      "Epoch 4: val_loss improved from 0.89367 to 0.85593, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.8572 - val_loss: 0.8559\n",
      "Epoch 5/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.8058\n",
      "Epoch 5: val_loss improved from 0.85593 to 0.82135, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.8071 - val_loss: 0.8213\n",
      "Epoch 6/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.7588\n",
      "Epoch 6: val_loss improved from 0.82135 to 0.80286, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7573 - val_loss: 0.8029\n",
      "Epoch 7/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.7335\n",
      "Epoch 7: val_loss improved from 0.80286 to 0.78578, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7263 - val_loss: 0.7858\n",
      "Epoch 8/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.7102\n",
      "Epoch 8: val_loss improved from 0.78578 to 0.77558, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.7030 - val_loss: 0.7756\n",
      "Epoch 9/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.6778\n",
      "Epoch 9: val_loss did not improve from 0.77558\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6834 - val_loss: 0.7761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6755\n",
      "Epoch 10: val_loss improved from 0.77558 to 0.75784, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.6755 - val_loss: 0.7578\n",
      "Epoch 11/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.6472\n",
      "Epoch 11: val_loss did not improve from 0.75784\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.6472 - val_loss: 0.7618\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.6472\n",
      "Epoch 12: val_loss improved from 0.75784 to 0.74547, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.6472 - val_loss: 0.7455\n",
      "Epoch 13/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.6371\n",
      "Epoch 13: val_loss improved from 0.74547 to 0.73409, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.6306 - val_loss: 0.7341\n",
      "Epoch 14/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.6213\n",
      "Epoch 14: val_loss improved from 0.73409 to 0.73389, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.6180 - val_loss: 0.7339\n",
      "Epoch 15/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.5940\n",
      "Epoch 15: val_loss did not improve from 0.73389\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5956 - val_loss: 0.7375\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.5855\n",
      "Epoch 16: val_loss improved from 0.73389 to 0.72870, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.5855 - val_loss: 0.7287\n",
      "Epoch 17/100\n",
      "31/37 [========================>.....] - ETA: 0s - loss: 0.5829\n",
      "Epoch 17: val_loss improved from 0.72870 to 0.72071, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 12ms/step - loss: 0.5862 - val_loss: 0.7207\n",
      "Epoch 18/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.5699\n",
      "Epoch 18: val_loss did not improve from 0.72071\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5697 - val_loss: 0.7287\n",
      "Epoch 19/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.5646\n",
      "Epoch 19: val_loss improved from 0.72071 to 0.71307, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.5614 - val_loss: 0.7131\n",
      "Epoch 20/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.5501\n",
      "Epoch 20: val_loss did not improve from 0.71307\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5483 - val_loss: 0.7207\n",
      "Epoch 21/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.5329\n",
      "Epoch 21: val_loss did not improve from 0.71307\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5318 - val_loss: 0.7137\n",
      "Epoch 22/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.5080\n",
      "Epoch 22: val_loss did not improve from 0.71307\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.5096 - val_loss: 0.7134\n",
      "Epoch 23/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4978\n",
      "Epoch 23: val_loss did not improve from 0.71307\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.5016 - val_loss: 0.7255\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.4922\n",
      "Epoch 24: val_loss did not improve from 0.71307\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4922 - val_loss: 0.7172\n",
      "Epoch 25/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.4836\n",
      "Epoch 25: val_loss improved from 0.71307 to 0.71271, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.4813 - val_loss: 0.7127\n",
      "Epoch 26/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4726\n",
      "Epoch 26: val_loss improved from 0.71271 to 0.70242, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\bestModel-fold4.hdf5\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4721 - val_loss: 0.7024\n",
      "Epoch 27/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.4611\n",
      "Epoch 27: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4603 - val_loss: 0.7172\n",
      "Epoch 28/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.4505\n",
      "Epoch 28: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.4524 - val_loss: 0.7167\n",
      "Epoch 29/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4318\n",
      "Epoch 29: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4353 - val_loss: 0.7247\n",
      "Epoch 30/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4257\n",
      "Epoch 30: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4235 - val_loss: 0.7361\n",
      "Epoch 31/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.4202\n",
      "Epoch 31: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4161 - val_loss: 0.7176\n",
      "Epoch 32/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.4056\n",
      "Epoch 32: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4031 - val_loss: 0.7419\n",
      "Epoch 33/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3983\n",
      "Epoch 33: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.4023 - val_loss: 0.7726\n",
      "Epoch 34/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.3869\n",
      "Epoch 34: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3827 - val_loss: 0.7550\n",
      "Epoch 35/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3668\n",
      "Epoch 35: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3667 - val_loss: 0.7593\n",
      "Epoch 36/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3748\n",
      "Epoch 36: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3728 - val_loss: 0.7570\n",
      "Epoch 37/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3427\n",
      "Epoch 37: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3379 - val_loss: 0.8235\n",
      "Epoch 38/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3455\n",
      "Epoch 38: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3467 - val_loss: 0.7812\n",
      "Epoch 39/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3424\n",
      "Epoch 39: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3413 - val_loss: 0.8091\n",
      "Epoch 40/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.3279\n",
      "Epoch 40: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3222 - val_loss: 0.7791\n",
      "Epoch 41/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3313\n",
      "Epoch 41: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3291 - val_loss: 0.7829\n",
      "Epoch 42/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.3280\n",
      "Epoch 42: val_loss did not improve from 0.70242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 10ms/step - loss: 0.3267 - val_loss: 0.7726\n",
      "Epoch 43/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3114\n",
      "Epoch 43: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3130 - val_loss: 0.8084\n",
      "Epoch 44/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3047\n",
      "Epoch 44: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3043 - val_loss: 0.8114\n",
      "Epoch 45/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.3105\n",
      "Epoch 45: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3114 - val_loss: 0.8212\n",
      "Epoch 46/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.3033\n",
      "Epoch 46: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.3033 - val_loss: 0.7856\n",
      "Epoch 47/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2984\n",
      "Epoch 47: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2950 - val_loss: 0.7995\n",
      "Epoch 48/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2911\n",
      "Epoch 48: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2907 - val_loss: 0.8286\n",
      "Epoch 49/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2700\n",
      "Epoch 49: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2698 - val_loss: 0.8725\n",
      "Epoch 50/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2940\n",
      "Epoch 50: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2947 - val_loss: 0.7869\n",
      "Epoch 51/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2820\n",
      "Epoch 51: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2851 - val_loss: 0.8429\n",
      "Epoch 52/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2719\n",
      "Epoch 52: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2730 - val_loss: 0.8263\n",
      "Epoch 53/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2671\n",
      "Epoch 53: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2663 - val_loss: 0.8104\n",
      "Epoch 54/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2634\n",
      "Epoch 54: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2592 - val_loss: 0.8896\n",
      "Epoch 55/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2624\n",
      "Epoch 55: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2617 - val_loss: 0.8372\n",
      "Epoch 56/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2617\n",
      "Epoch 56: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2601 - val_loss: 0.8289\n",
      "Epoch 57/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2643\n",
      "Epoch 57: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2631 - val_loss: 0.8674\n",
      "Epoch 58/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2551\n",
      "Epoch 58: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2532 - val_loss: 0.9117\n",
      "Epoch 59/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2574\n",
      "Epoch 59: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2553 - val_loss: 0.8384\n",
      "Epoch 60/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2617\n",
      "Epoch 60: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2653 - val_loss: 0.8876\n",
      "Epoch 61/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2467\n",
      "Epoch 61: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2532 - val_loss: 0.9160\n",
      "Epoch 62/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2516\n",
      "Epoch 62: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2516 - val_loss: 0.8524\n",
      "Epoch 63/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2365\n",
      "Epoch 63: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2342 - val_loss: 0.8812\n",
      "Epoch 64/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2388\n",
      "Epoch 64: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2404 - val_loss: 0.8586\n",
      "Epoch 65/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2269\n",
      "Epoch 65: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2295 - val_loss: 0.8946\n",
      "Epoch 66/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2459\n",
      "Epoch 66: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2422 - val_loss: 0.8761\n",
      "Epoch 67/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2374\n",
      "Epoch 67: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2386 - val_loss: 0.9071\n",
      "Epoch 68/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2395\n",
      "Epoch 68: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2395 - val_loss: 0.9255\n",
      "Epoch 69/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2146\n",
      "Epoch 69: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2187 - val_loss: 0.9343\n",
      "Epoch 70/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2109\n",
      "Epoch 70: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2146 - val_loss: 0.9539\n",
      "Epoch 71/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2172\n",
      "Epoch 71: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2164 - val_loss: 0.9170\n",
      "Epoch 72/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.2241\n",
      "Epoch 72: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2225 - val_loss: 0.9451\n",
      "Epoch 73/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2326\n",
      "Epoch 73: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2312 - val_loss: 0.8797\n",
      "Epoch 74/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2127\n",
      "Epoch 74: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2143 - val_loss: 0.9105\n",
      "Epoch 75/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2087\n",
      "Epoch 75: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2096 - val_loss: 0.9649\n",
      "Epoch 76/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2294\n",
      "Epoch 76: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2274 - val_loss: 0.8711\n",
      "Epoch 77/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2216\n",
      "Epoch 77: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2237 - val_loss: 0.9077\n",
      "Epoch 78/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2215\n",
      "Epoch 78: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2216 - val_loss: 0.8888\n",
      "Epoch 79/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2099\n",
      "Epoch 79: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2132 - val_loss: 0.9672\n",
      "Epoch 80/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2124\n",
      "Epoch 80: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2111 - val_loss: 0.9514\n",
      "Epoch 81/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.1956\n",
      "Epoch 81: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1948 - val_loss: 0.9602\n",
      "Epoch 82/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2059\n",
      "Epoch 82: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2051 - val_loss: 0.9646\n",
      "Epoch 83/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2092\n",
      "Epoch 83: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2086 - val_loss: 0.9385\n",
      "Epoch 84/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1924\n",
      "Epoch 84: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1958 - val_loss: 0.9955\n",
      "Epoch 85/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2011\n",
      "Epoch 85: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2001 - val_loss: 0.9431\n",
      "Epoch 86/100\n",
      "35/37 [===========================>..] - ETA: 0s - loss: 0.2019\n",
      "Epoch 86: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2045 - val_loss: 0.9320\n",
      "Epoch 87/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.2093\n",
      "Epoch 87: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.2079 - val_loss: 0.9509\n",
      "Epoch 88/100\n",
      "36/37 [============================>.] - ETA: 0s - loss: 0.1909\n",
      "Epoch 88: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 9ms/step - loss: 0.1915 - val_loss: 1.0042\n",
      "Epoch 89/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.1888\n",
      "Epoch 89: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1883 - val_loss: 1.0145\n",
      "Epoch 90/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2132\n",
      "Epoch 90: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2063 - val_loss: 0.9898\n",
      "Epoch 91/100\n",
      "34/37 [==========================>...] - ETA: 0s - loss: 0.1952\n",
      "Epoch 91: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1968 - val_loss: 0.9945\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1916\n",
      "Epoch 92: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1916 - val_loss: 0.9817\n",
      "Epoch 93/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.1920\n",
      "Epoch 93: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1893 - val_loss: 0.9604\n",
      "Epoch 94/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.2078\n",
      "Epoch 94: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.2061 - val_loss: 0.9460\n",
      "Epoch 95/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.1947\n",
      "Epoch 95: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1995 - val_loss: 0.9552\n",
      "Epoch 96/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1897\n",
      "Epoch 96: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1932 - val_loss: 0.9993\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1796\n",
      "Epoch 97: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 11ms/step - loss: 0.1796 - val_loss: 1.0295\n",
      "Epoch 98/100\n",
      "32/37 [========================>.....] - ETA: 0s - loss: 0.1879\n",
      "Epoch 98: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1879 - val_loss: 0.9498\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - ETA: 0s - loss: 0.1681\n",
      "Epoch 99: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1681 - val_loss: 1.0228\n",
      "Epoch 100/100\n",
      "33/37 [=========================>....] - ETA: 0s - loss: 0.1846\n",
      "Epoch 100: val_loss did not improve from 0.70242\n",
      "37/37 [==============================] - 0s 10ms/step - loss: 0.1839 - val_loss: 1.0240\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Train/Test model on all folds, generate evaluations\n",
    "##################################################################################\n",
    "\n",
    "## Create and set directory to save model\n",
    "modelPath = os.path.join(outPath, expName, \"{}fold\".format(n_fold), \"models\")\n",
    "if(not os.path.isdir(modelPath)):\n",
    "    os.makedirs(modelPath)\n",
    "\n",
    "i = -1\n",
    "for fold in folds:\n",
    "    i += 1\n",
    "    \n",
    "    print(\"\\nTrain/Test model on Fold #\"+str(i)+\".\")\n",
    "    \n",
    "    model = DLNN_CORENup(input_seq_shape = input_seq_shape)\n",
    "    \n",
    "    ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    modelCallbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
    "                                           monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                           save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "    ]\n",
    "    \n",
    "    # adding random shuffling of the dataset for training purpose\n",
    "    index_arr = np.arange(fold[\"X_train\"].shape[0])\n",
    "    index_arr = np.random.permutation(index_arr)\n",
    "    \n",
    "    model.fit(x = fold[\"X_train\"][index_arr], y = fold[\"y_train\"][index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "              callbacks = modelCallbacks, validation_data = (fold[\"X_test\"], fold[\"y_test\"]))\n",
    "    \n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Prediction and metrics for TRAIN dataset\n",
    "    ##################################################################################\n",
    "\n",
    "    y_pred = model.predict(fold[\"X_train\"])\n",
    "    label_pred = pred2label(y_pred)\n",
    "    \n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "    prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "    mcc = matthews_corrcoef(fold[\"y_train\"], label_pred)\n",
    "\n",
    "    conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "    auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "    \n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Train\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Prediction and metrics for TEST dataset\n",
    "    ##################################################################################\n",
    "\n",
    "    y_pred = model.predict(fold[\"X_test\"])\n",
    "    label_pred = pred2label(y_pred)\n",
    "    \n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "    prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "    mcc = matthews_corrcoef(fold[\"y_test\"], label_pred)\n",
    "\n",
    "    conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "    auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "    \n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Test\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "    \n",
    "    del model\n",
    "    tf.keras.backend.clear_session()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.729241</td>\n",
       "      <td>0.72766</td>\n",
       "      <td>0.807623</td>\n",
       "      <td>0.738051</td>\n",
       "      <td>0.720358</td>\n",
       "      <td>0.459661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.889400</td>\n",
       "      <td>0.88930</td>\n",
       "      <td>0.949517</td>\n",
       "      <td>0.889397</td>\n",
       "      <td>0.889403</td>\n",
       "      <td>0.779566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Test        0.729241    0.72766  0.807623     0.738051     0.720358  0.459661\n",
       "Train       0.889400    0.88930  0.949517     0.889397     0.889403  0.779566"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Test\t0.731321\t0.727759\t0.806616\t0.745579\t0.717011\t0.463860\n",
    "# Train\t0.856681\t0.845510\t0.927639\t0.873001\t0.840358\t0.714348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10, 5 - larger network\n",
    "\n",
    "# Accuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Test\t0.735525\t0.740360\t0.809042\t0.728803\t0.742189\t0.472127\n",
    "# Train\t0.951550\t0.963889\t0.987920\t0.938102\t0.964995\t0.903821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.677149</td>\n",
       "      <td>0.664093</td>\n",
       "      <td>[0.0, 0.0041841004184100415, 0.037656903765690...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.004201680672268907, 0.004201...</td>\n",
       "      <td>[1.9778378, 0.97783774, 0.95496964, 0.95408726...</td>\n",
       "      <td>0.747020</td>\n",
       "      <td>0.719665</td>\n",
       "      <td>0.634454</td>\n",
       "      <td>0.355434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.729560</td>\n",
       "      <td>0.751152</td>\n",
       "      <td>[0.0, 0.004201680672268907, 0.0546218487394958...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0041841004184100415, 0.00418...</td>\n",
       "      <td>[1.9945351, 0.9945351, 0.94062114, 0.9379019, ...</td>\n",
       "      <td>0.837576</td>\n",
       "      <td>0.684874</td>\n",
       "      <td>0.774059</td>\n",
       "      <td>0.460808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>[0.0, 0.004201680672268907, 0.0504201680672268...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.008403361344537815, 0.008403...</td>\n",
       "      <td>[1.995671, 0.99567103, 0.9788978, 0.9704835, 0...</td>\n",
       "      <td>0.788468</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.414644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.817227</td>\n",
       "      <td>0.815900</td>\n",
       "      <td>[0.0, 0.004201680672268907, 0.1554621848739495...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.004201680672268907...</td>\n",
       "      <td>[1.9959517, 0.99595165, 0.96836454, 0.9662632,...</td>\n",
       "      <td>0.893069</td>\n",
       "      <td>0.819328</td>\n",
       "      <td>0.815126</td>\n",
       "      <td>0.634459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.716387</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>[0.0, 0.004201680672268907, 0.0378151260504201...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.008403361344537815, 0.008403...</td>\n",
       "      <td>[1.9946105, 0.99461055, 0.96656466, 0.96255755...</td>\n",
       "      <td>0.771979</td>\n",
       "      <td>0.701681</td>\n",
       "      <td>0.731092</td>\n",
       "      <td>0.432960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold Train_Test  Accuracy  Precision  \\\n",
       "1     0       Test  0.677149   0.664093   \n",
       "3     1       Test  0.729560   0.751152   \n",
       "5     2       Test  0.705882   0.684211   \n",
       "7     3       Test  0.817227   0.815900   \n",
       "9     4       Test  0.716387   0.722944   \n",
       "\n",
       "                                                 TPR  \\\n",
       "1  [0.0, 0.0041841004184100415, 0.037656903765690...   \n",
       "3  [0.0, 0.004201680672268907, 0.0546218487394958...   \n",
       "5  [0.0, 0.004201680672268907, 0.0504201680672268...   \n",
       "7  [0.0, 0.004201680672268907, 0.1554621848739495...   \n",
       "9  [0.0, 0.004201680672268907, 0.0378151260504201...   \n",
       "\n",
       "                                                 FPR  \\\n",
       "1  [0.0, 0.0, 0.0, 0.004201680672268907, 0.004201...   \n",
       "3  [0.0, 0.0, 0.0, 0.0041841004184100415, 0.00418...   \n",
       "5  [0.0, 0.0, 0.0, 0.008403361344537815, 0.008403...   \n",
       "7  [0.0, 0.0, 0.0, 0.0, 0.0, 0.004201680672268907...   \n",
       "9  [0.0, 0.0, 0.0, 0.008403361344537815, 0.008403...   \n",
       "\n",
       "                                  TPR_FPR_Thresholds       AUC  Sensitivity  \\\n",
       "1  [1.9778378, 0.97783774, 0.95496964, 0.95408726...  0.747020     0.719665   \n",
       "3  [1.9945351, 0.9945351, 0.94062114, 0.9379019, ...  0.837576     0.684874   \n",
       "5  [1.995671, 0.99567103, 0.9788978, 0.9704835, 0...  0.788468     0.764706   \n",
       "7  [1.9959517, 0.99595165, 0.96836454, 0.9662632,...  0.893069     0.819328   \n",
       "9  [1.9946105, 0.99461055, 0.96656466, 0.96255755...  0.771979     0.701681   \n",
       "\n",
       "   Specificity       MCC  \n",
       "1     0.634454  0.355434  \n",
       "3     0.774059  0.460808  \n",
       "5     0.647059  0.414644  \n",
       "7     0.815126  0.634459  \n",
       "9     0.731092  0.432960  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df[evaluations_df[\"Train_Test\"] == \"Test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to customize the DLNN architecture with parameters\n",
    "##################################################################################\n",
    "\n",
    "def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "                 conv_filters_per_layer_1 = 25, kernel_length_1 = 10, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "                 max_pool_width_1 = 2, max_pool_stride_1 = 2, ## 1st Maxpool layer parameters\n",
    "                 lstm_decode_units = 10, ## LSTM layer parameters\n",
    "                 conv_filters_per_layer_2 = 25,  kernel_length_2 = 5, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "                 max_pool_width_2 = 2, max_pool_stride_2 = 2, ## 2nd Maxpool layer parameters\n",
    "                 dense_decode_units = 128, ## Dense layer parameters\n",
    "                 prob = 0.5, learn_rate = 0.0005, loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  SEQUENCE  ##################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "                                strides = conv_strides_1, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = \"same\")(input1)\n",
    "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "    x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x2 = tf.keras.layers.GRU(lstm_decode_units, return_sequences = True, \n",
    "                             kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "    ## Conv Path\n",
    "\n",
    "    x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, \n",
    "                                strides = conv_strides_2, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = 'same')(x1)\n",
    "    x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "    x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "    x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  Classifier  ################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    y = tf.keras.layers.Dense(dense_decode_units, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'relu')(x4)\n",
    "    \n",
    "    y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(1, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'sigmoid')(y)\n",
    "\n",
    "    ## Generate Model from input and output\n",
    "    model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), \n",
    "                      loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), \n",
    "                      loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 41, 21)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 41, 25)       5275        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 41, 25)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 20, 25)       0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20, 25)       0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 20, 25)       3150        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 20, 25)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 20, 10)       1110        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 10, 25)      0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20, 10)       0           ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10, 25)       0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 200)          0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 250)          0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 450)          0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          57728       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 67,392\n",
      "Trainable params: 67,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DLNN_CORENup().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent data testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-fold Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of each k-fold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.62449</td>\n",
       "      <td>0.250615</td>\n",
       "      <td>0.671636</td>\n",
       "      <td>0.631527</td>\n",
       "      <td>0.623092</td>\n",
       "      <td>0.192428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent   0.62449   0.250615  0.671636     0.631527     0.623092  0.192428"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    label_pred = pred2label(y_pred)\n",
    "\n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(indpe_labels, label_pred)\n",
    "    prec = precision_score(indpe_labels,label_pred)\n",
    "    mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "    conf = confusion_matrix(indpe_labels, label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(indpe_labels, y_pred)\n",
    "    auc = roc_auc_score(indpe_labels, y_pred)\n",
    "\n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.605714</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>[0.0, 0.0, 0.0049261083743842365, 0.0049261083...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.000978473581213...</td>\n",
       "      <td>[1.9944124, 0.9944125, 0.98207283, 0.9796978, ...</td>\n",
       "      <td>0.657472</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>0.167607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.637551</td>\n",
       "      <td>0.253579</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0049261083743842365, 0.00492...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.001956947162426...</td>\n",
       "      <td>[1.9933574, 0.99335736, 0.9888502, 0.9848104, ...</td>\n",
       "      <td>0.670553</td>\n",
       "      <td>0.610837</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.192616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.584490</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>[0.0, 0.0, 0.0049261083743842365, 0.0049261083...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.000978473581213...</td>\n",
       "      <td>[1.9964402, 0.9964401, 0.9926347, 0.9849546, 0...</td>\n",
       "      <td>0.658122</td>\n",
       "      <td>0.669951</td>\n",
       "      <td>0.567515</td>\n",
       "      <td>0.176871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.635102</td>\n",
       "      <td>0.255020</td>\n",
       "      <td>[0.0, 0.0, 0.009852216748768473, 0.00985221674...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.000978473581213...</td>\n",
       "      <td>[1.9969572, 0.9969572, 0.9938554, 0.9921739, 0...</td>\n",
       "      <td>0.679552</td>\n",
       "      <td>0.625616</td>\n",
       "      <td>0.636986</td>\n",
       "      <td>0.198788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.659592</td>\n",
       "      <td>0.272340</td>\n",
       "      <td>[0.0, 0.0, 0.009852216748768473, 0.00985221674...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.000978473581213...</td>\n",
       "      <td>[1.9931064, 0.9931064, 0.9873237, 0.98139614, ...</td>\n",
       "      <td>0.692480</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>0.665362</td>\n",
       "      <td>0.226257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold   Train_Test  Accuracy  Precision  \\\n",
       "0     0  Independent  0.605714   0.236842   \n",
       "1     1  Independent  0.637551   0.253579   \n",
       "2     2  Independent  0.584490   0.235294   \n",
       "3     3  Independent  0.635102   0.255020   \n",
       "4     4  Independent  0.659592   0.272340   \n",
       "\n",
       "                                                 TPR  \\\n",
       "0  [0.0, 0.0, 0.0049261083743842365, 0.0049261083...   \n",
       "1  [0.0, 0.0, 0.0, 0.0049261083743842365, 0.00492...   \n",
       "2  [0.0, 0.0, 0.0049261083743842365, 0.0049261083...   \n",
       "3  [0.0, 0.0, 0.009852216748768473, 0.00985221674...   \n",
       "4  [0.0, 0.0, 0.009852216748768473, 0.00985221674...   \n",
       "\n",
       "                                                 FPR  \\\n",
       "0  [0.0, 0.0009784735812133072, 0.000978473581213...   \n",
       "1  [0.0, 0.0009784735812133072, 0.001956947162426...   \n",
       "2  [0.0, 0.0009784735812133072, 0.000978473581213...   \n",
       "3  [0.0, 0.0009784735812133072, 0.000978473581213...   \n",
       "4  [0.0, 0.0009784735812133072, 0.000978473581213...   \n",
       "\n",
       "                                  TPR_FPR_Thresholds       AUC  Sensitivity  \\\n",
       "0  [1.9944124, 0.9944125, 0.98207283, 0.9796978, ...  0.657472     0.620690   \n",
       "1  [1.9933574, 0.99335736, 0.9888502, 0.9848104, ...  0.670553     0.610837   \n",
       "2  [1.9964402, 0.9964401, 0.9926347, 0.9849546, 0...  0.658122     0.669951   \n",
       "3  [1.9969572, 0.9969572, 0.9938554, 0.9921739, 0...  0.679552     0.625616   \n",
       "4  [1.9931064, 0.9931064, 0.9873237, 0.98139614, ...  0.692480     0.630542   \n",
       "\n",
       "   Specificity       MCC  \n",
       "0     0.602740  0.167607  \n",
       "1     0.642857  0.192616  \n",
       "2     0.567515  0.176871  \n",
       "3     0.636986  0.198788  \n",
       "4     0.665362  0.226257  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean score with k-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.634286</td>\n",
       "      <td>0.255489</td>\n",
       "      <td>0.68445</td>\n",
       "      <td>0.630542</td>\n",
       "      <td>0.635029</td>\n",
       "      <td>0.200847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision      AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Independent  0.634286   0.255489  0.68445     0.630542     0.635029  0.200847"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "total_pred = np.zeros(indpe_labels.shape)\n",
    "all_preds = []\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    total_pred += y_pred\n",
    "    all_preds.append(y_pred)\n",
    "    \n",
    "total_pred = total_pred / n_fold\n",
    "label_pred = pred2label(total_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, total_pred)\n",
    "auc = roc_auc_score(indpe_labels, total_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting score with k-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.642449</td>\n",
       "      <td>0.262626</td>\n",
       "      <td>0.67124</td>\n",
       "      <td>0.640394</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.214625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision      AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Independent  0.642449   0.262626  0.67124     0.640394     0.642857  0.214625"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "total_pred = np.zeros(indpe_labels.shape)\n",
    "all_preds = []\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    vote_pred = pred2label(y_pred)\n",
    "    total_pred += vote_pred\n",
    "    all_preds.append(vote_pred)\n",
    "    \n",
    "total_pred = total_pred / n_fold\n",
    "label_pred = pred2label(total_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, total_pred)\n",
    "auc = roc_auc_score(indpe_labels, total_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using New Model\n",
    "\n",
    "Train model on full data from training. Predict and evaluate on Independent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.9742 - accuracy: 0.5210\n",
      "Epoch 1: val_loss improved from inf to 0.93720, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 3s 27ms/step - loss: 0.9742 - accuracy: 0.5210 - val_loss: 0.9372 - val_accuracy: 0.5946\n",
      "Epoch 2/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.9295 - accuracy: 0.5583\n",
      "Epoch 2: val_loss improved from 0.93720 to 0.90029, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.9281 - accuracy: 0.5620 - val_loss: 0.9003 - val_accuracy: 0.6588\n",
      "Epoch 3/100\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.8831 - accuracy: 0.6074\n",
      "Epoch 3: val_loss improved from 0.90029 to 0.83881, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.8826 - accuracy: 0.6093 - val_loss: 0.8388 - val_accuracy: 0.6926\n",
      "Epoch 4/100\n",
      "40/42 [===========================>..] - ETA: 0s - loss: 0.8164 - accuracy: 0.6809\n",
      "Epoch 4: val_loss improved from 0.83881 to 0.77656, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.8140 - accuracy: 0.6833 - val_loss: 0.7766 - val_accuracy: 0.7128\n",
      "Epoch 5/100\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.7761 - accuracy: 0.7099\n",
      "Epoch 5: val_loss improved from 0.77656 to 0.75221, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.7784 - accuracy: 0.7089 - val_loss: 0.7522 - val_accuracy: 0.7027\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.7421 - accuracy: 0.7213\n",
      "Epoch 6: val_loss improved from 0.75221 to 0.74592, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.7421 - accuracy: 0.7213 - val_loss: 0.7459 - val_accuracy: 0.7095\n",
      "Epoch 7/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.7241 - accuracy: 0.7344\n",
      "Epoch 7: val_loss improved from 0.74592 to 0.72437, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.7213 - accuracy: 0.7348 - val_loss: 0.7244 - val_accuracy: 0.7027\n",
      "Epoch 8/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.7040 - accuracy: 0.7364\n",
      "Epoch 8: val_loss improved from 0.72437 to 0.71255, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.7049 - accuracy: 0.7378 - val_loss: 0.7126 - val_accuracy: 0.7162\n",
      "Epoch 9/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.6816 - accuracy: 0.7441\n",
      "Epoch 9: val_loss improved from 0.71255 to 0.70455, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.6851 - accuracy: 0.7453 - val_loss: 0.7046 - val_accuracy: 0.7264\n",
      "Epoch 10/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.6639 - accuracy: 0.7627\n",
      "Epoch 10: val_loss improved from 0.70455 to 0.69233, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.6680 - accuracy: 0.7633 - val_loss: 0.6923 - val_accuracy: 0.7162\n",
      "Epoch 11/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.6588 - accuracy: 0.7529\n",
      "Epoch 11: val_loss improved from 0.69233 to 0.68088, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.6573 - accuracy: 0.7513 - val_loss: 0.6809 - val_accuracy: 0.7230\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6386 - accuracy: 0.7686\n",
      "Epoch 12: val_loss improved from 0.68088 to 0.67546, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.6386 - accuracy: 0.7686 - val_loss: 0.6755 - val_accuracy: 0.7264\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6288 - accuracy: 0.7701\n",
      "Epoch 13: val_loss improved from 0.67546 to 0.66405, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.6288 - accuracy: 0.7701 - val_loss: 0.6640 - val_accuracy: 0.7264\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.6170 - accuracy: 0.7795\n",
      "Epoch 14: val_loss improved from 0.66405 to 0.65532, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.6170 - accuracy: 0.7795 - val_loss: 0.6553 - val_accuracy: 0.7365\n",
      "Epoch 15/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.5931 - accuracy: 0.7812\n",
      "Epoch 15: val_loss improved from 0.65532 to 0.64963, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.5915 - accuracy: 0.7814 - val_loss: 0.6496 - val_accuracy: 0.7365\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.5927 - accuracy: 0.7806\n",
      "Epoch 16: val_loss improved from 0.64963 to 0.64179, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.5927 - accuracy: 0.7806 - val_loss: 0.6418 - val_accuracy: 0.7399\n",
      "Epoch 17/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.5745 - accuracy: 0.7854\n",
      "Epoch 17: val_loss improved from 0.64179 to 0.63107, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.5758 - accuracy: 0.7855 - val_loss: 0.6311 - val_accuracy: 0.7534\n",
      "Epoch 18/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.5624 - accuracy: 0.7952\n",
      "Epoch 18: val_loss improved from 0.63107 to 0.62697, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.5633 - accuracy: 0.7926 - val_loss: 0.6270 - val_accuracy: 0.7399\n",
      "Epoch 19/100\n",
      "36/42 [========================>.....] - ETA: 0s - loss: 0.5417 - accuracy: 0.8003\n",
      "Epoch 19: val_loss did not improve from 0.62697\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.5506 - accuracy: 0.7968 - val_loss: 0.6386 - val_accuracy: 0.7399\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.5421 - accuracy: 0.8009\n",
      "Epoch 20: val_loss improved from 0.62697 to 0.60962, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.5421 - accuracy: 0.8009 - val_loss: 0.6096 - val_accuracy: 0.7568\n",
      "Epoch 21/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.5307 - accuracy: 0.8080\n",
      "Epoch 21: val_loss improved from 0.60962 to 0.60416, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 11ms/step - loss: 0.5273 - accuracy: 0.8092 - val_loss: 0.6042 - val_accuracy: 0.7601\n",
      "Epoch 22/100\n",
      "36/42 [========================>.....] - ETA: 0s - loss: 0.5070 - accuracy: 0.8199\n",
      "Epoch 22: val_loss improved from 0.60416 to 0.60045, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.5196 - accuracy: 0.8099 - val_loss: 0.6004 - val_accuracy: 0.7669\n",
      "Epoch 23/100\n",
      "36/42 [========================>.....] - ETA: 0s - loss: 0.4998 - accuracy: 0.8199\n",
      "Epoch 23: val_loss improved from 0.60045 to 0.59808, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.4930 - accuracy: 0.8238 - val_loss: 0.5981 - val_accuracy: 0.7568\n",
      "Epoch 24/100\n",
      "40/42 [===========================>..] - ETA: 0s - loss: 0.4839 - accuracy: 0.8191\n",
      "Epoch 24: val_loss improved from 0.59808 to 0.58518, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.4832 - accuracy: 0.8189 - val_loss: 0.5852 - val_accuracy: 0.7736\n",
      "Epoch 25/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.4648 - accuracy: 0.8340\n",
      "Epoch 25: val_loss did not improve from 0.58518\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.4681 - accuracy: 0.8310 - val_loss: 0.5936 - val_accuracy: 0.7669\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4800 - accuracy: 0.8257\n",
      "Epoch 26: val_loss did not improve from 0.58518\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.4800 - accuracy: 0.8257 - val_loss: 0.5885 - val_accuracy: 0.7703\n",
      "Epoch 27/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.4444 - accuracy: 0.8471\n",
      "Epoch 27: val_loss improved from 0.58518 to 0.57396, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4451 - accuracy: 0.8452 - val_loss: 0.5740 - val_accuracy: 0.7838\n",
      "Epoch 28/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.4403 - accuracy: 0.8518\n",
      "Epoch 28: val_loss improved from 0.57396 to 0.56063, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4480 - accuracy: 0.8467 - val_loss: 0.5606 - val_accuracy: 0.7872\n",
      "Epoch 29/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.4245 - accuracy: 0.8619\n",
      "Epoch 29: val_loss improved from 0.56063 to 0.55666, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4239 - accuracy: 0.8618 - val_loss: 0.5567 - val_accuracy: 0.7973\n",
      "Epoch 30/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.4376 - accuracy: 0.8495\n",
      "Epoch 30: val_loss improved from 0.55666 to 0.55503, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4309 - accuracy: 0.8535 - val_loss: 0.5550 - val_accuracy: 0.7872\n",
      "Epoch 31/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.4161 - accuracy: 0.8553\n",
      "Epoch 31: val_loss improved from 0.55503 to 0.55034, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.4114 - accuracy: 0.8580 - val_loss: 0.5503 - val_accuracy: 0.7973\n",
      "Epoch 32/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.4059 - accuracy: 0.8668\n",
      "Epoch 32: val_loss did not improve from 0.55034\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.4063 - accuracy: 0.8651 - val_loss: 0.5702 - val_accuracy: 0.7905\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.4042 - accuracy: 0.8666\n",
      "Epoch 33: val_loss improved from 0.55034 to 0.54326, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.4042 - accuracy: 0.8666 - val_loss: 0.5433 - val_accuracy: 0.7905\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.8696\n",
      "Epoch 34: val_loss improved from 0.54326 to 0.54247, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.3924 - accuracy: 0.8696 - val_loss: 0.5425 - val_accuracy: 0.7872\n",
      "Epoch 35/100\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.3844 - accuracy: 0.8734\n",
      "Epoch 35: val_loss did not improve from 0.54247\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.3826 - accuracy: 0.8723 - val_loss: 0.5969 - val_accuracy: 0.7770\n",
      "Epoch 36/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.3763 - accuracy: 0.8678\n",
      "Epoch 36: val_loss improved from 0.54247 to 0.53974, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.3785 - accuracy: 0.8681 - val_loss: 0.5397 - val_accuracy: 0.7905\n",
      "Epoch 37/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.3846 - accuracy: 0.8678\n",
      "Epoch 37: val_loss improved from 0.53974 to 0.52571, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.3834 - accuracy: 0.8696 - val_loss: 0.5257 - val_accuracy: 0.7973\n",
      "Epoch 38/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.3642 - accuracy: 0.8822\n",
      "Epoch 38: val_loss did not improve from 0.52571\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.3604 - accuracy: 0.8835 - val_loss: 0.5390 - val_accuracy: 0.8074\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.3351 - accuracy: 0.8933\n",
      "Epoch 39: val_loss did not improve from 0.52571\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.3351 - accuracy: 0.8933 - val_loss: 0.5303 - val_accuracy: 0.7973\n",
      "Epoch 40/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.3576 - accuracy: 0.8809\n",
      "Epoch 40: val_loss improved from 0.52571 to 0.51668, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.3607 - accuracy: 0.8787 - val_loss: 0.5167 - val_accuracy: 0.8041\n",
      "Epoch 41/100\n",
      "40/42 [===========================>..] - ETA: 0s - loss: 0.3235 - accuracy: 0.9004\n",
      "Epoch 41: val_loss did not improve from 0.51668\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.3239 - accuracy: 0.9005 - val_loss: 0.5370 - val_accuracy: 0.8041\n",
      "Epoch 42/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.3533 - accuracy: 0.8813\n",
      "Epoch 42: val_loss did not improve from 0.51668\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.3458 - accuracy: 0.8858 - val_loss: 0.5280 - val_accuracy: 0.7939\n",
      "Epoch 43/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.3274 - accuracy: 0.8919\n",
      "Epoch 43: val_loss did not improve from 0.51668\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.3240 - accuracy: 0.8937 - val_loss: 0.5217 - val_accuracy: 0.7939\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.8884\n",
      "Epoch 44: val_loss did not improve from 0.51668\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.3307 - accuracy: 0.8884 - val_loss: 0.5203 - val_accuracy: 0.8007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.3106 - accuracy: 0.8978\n",
      "Epoch 45: val_loss improved from 0.51668 to 0.51394, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.3133 - accuracy: 0.8967 - val_loss: 0.5139 - val_accuracy: 0.8108\n",
      "Epoch 46/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.3103 - accuracy: 0.8974\n",
      "Epoch 46: val_loss did not improve from 0.51394\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.3166 - accuracy: 0.8944 - val_loss: 0.6146 - val_accuracy: 0.7872\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.3057 - accuracy: 0.9031\n",
      "Epoch 47: val_loss did not improve from 0.51394\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.3057 - accuracy: 0.9031 - val_loss: 0.5244 - val_accuracy: 0.8108\n",
      "Epoch 48/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.3176 - accuracy: 0.8941\n",
      "Epoch 48: val_loss did not improve from 0.51394\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.3163 - accuracy: 0.8944 - val_loss: 0.5304 - val_accuracy: 0.8041\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.9068\n",
      "Epoch 49: val_loss did not improve from 0.51394\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2953 - accuracy: 0.9068 - val_loss: 0.5219 - val_accuracy: 0.8074\n",
      "Epoch 50/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.3180 - accuracy: 0.8988\n",
      "Epoch 50: val_loss improved from 0.51394 to 0.49788, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.3101 - accuracy: 0.9020 - val_loss: 0.4979 - val_accuracy: 0.8176\n",
      "Epoch 51/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.9032\n",
      "Epoch 51: val_loss did not improve from 0.49788\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.3060 - accuracy: 0.9020 - val_loss: 0.5685 - val_accuracy: 0.7939\n",
      "Epoch 52/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.2862 - accuracy: 0.9112\n",
      "Epoch 52: val_loss did not improve from 0.49788\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2882 - accuracy: 0.9110 - val_loss: 0.5092 - val_accuracy: 0.8142\n",
      "Epoch 53/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2682 - accuracy: 0.9193\n",
      "Epoch 53: val_loss did not improve from 0.49788\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2677 - accuracy: 0.9207 - val_loss: 0.5263 - val_accuracy: 0.8074\n",
      "Epoch 54/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2973 - accuracy: 0.9096\n",
      "Epoch 54: val_loss improved from 0.49788 to 0.48563, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2974 - accuracy: 0.9087 - val_loss: 0.4856 - val_accuracy: 0.8311\n",
      "Epoch 55/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2810 - accuracy: 0.9101\n",
      "Epoch 55: val_loss did not improve from 0.48563\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2875 - accuracy: 0.9053 - val_loss: 0.4946 - val_accuracy: 0.8277\n",
      "Epoch 56/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2748 - accuracy: 0.9139\n",
      "Epoch 56: val_loss did not improve from 0.48563\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2781 - accuracy: 0.9110 - val_loss: 0.4869 - val_accuracy: 0.8311\n",
      "Epoch 57/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2692 - accuracy: 0.9113\n",
      "Epoch 57: val_loss did not improve from 0.48563\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2702 - accuracy: 0.9113 - val_loss: 0.5439 - val_accuracy: 0.8041\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.9102\n",
      "Epoch 58: val_loss did not improve from 0.48563\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2788 - accuracy: 0.9102 - val_loss: 0.4963 - val_accuracy: 0.8243\n",
      "Epoch 59/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2731 - accuracy: 0.9174\n",
      "Epoch 59: val_loss did not improve from 0.48563\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2703 - accuracy: 0.9189 - val_loss: 0.4925 - val_accuracy: 0.8209\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2763 - accuracy: 0.9102\n",
      "Epoch 60: val_loss did not improve from 0.48563\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2763 - accuracy: 0.9102 - val_loss: 0.4925 - val_accuracy: 0.8209\n",
      "Epoch 61/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2651 - accuracy: 0.9231\n",
      "Epoch 61: val_loss did not improve from 0.48563\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2660 - accuracy: 0.9222 - val_loss: 0.5124 - val_accuracy: 0.8311\n",
      "Epoch 62/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2673 - accuracy: 0.9189\n",
      "Epoch 62: val_loss improved from 0.48563 to 0.48177, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2616 - accuracy: 0.9219 - val_loss: 0.4818 - val_accuracy: 0.8480\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2629 - accuracy: 0.9147\n",
      "Epoch 63: val_loss did not improve from 0.48177\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2629 - accuracy: 0.9147 - val_loss: 0.4974 - val_accuracy: 0.8277\n",
      "Epoch 64/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2516 - accuracy: 0.9240\n",
      "Epoch 64: val_loss did not improve from 0.48177\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2538 - accuracy: 0.9226 - val_loss: 0.5056 - val_accuracy: 0.8277\n",
      "Epoch 65/100\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.2520 - accuracy: 0.9195\n",
      "Epoch 65: val_loss did not improve from 0.48177\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2466 - accuracy: 0.9234 - val_loss: 0.4942 - val_accuracy: 0.8378\n",
      "Epoch 66/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2472 - accuracy: 0.9280\n",
      "Epoch 66: val_loss did not improve from 0.48177\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2492 - accuracy: 0.9256 - val_loss: 0.4999 - val_accuracy: 0.8345\n",
      "Epoch 67/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2459 - accuracy: 0.9260\n",
      "Epoch 67: val_loss did not improve from 0.48177\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2451 - accuracy: 0.9252 - val_loss: 0.4897 - val_accuracy: 0.8311\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9113\n",
      "Epoch 68: val_loss did not improve from 0.48177\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2686 - accuracy: 0.9113 - val_loss: 0.4995 - val_accuracy: 0.8243\n",
      "Epoch 69/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2810 - accuracy: 0.9050\n",
      "Epoch 69: val_loss did not improve from 0.48177\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2774 - accuracy: 0.9076 - val_loss: 0.4945 - val_accuracy: 0.8176\n",
      "Epoch 70/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.9211\n",
      "Epoch 70: val_loss improved from 0.48177 to 0.47350, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA_trainFoldAug\\5fold\\models\\_fullModel.hdf5\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2592 - accuracy: 0.9211 - val_loss: 0.4735 - val_accuracy: 0.8412\n",
      "Epoch 71/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.2574 - accuracy: 0.9253\n",
      "Epoch 71: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2574 - accuracy: 0.9252 - val_loss: 0.5183 - val_accuracy: 0.8311\n",
      "Epoch 72/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9287\n",
      "Epoch 72: val_loss did not improve from 0.47350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2418 - accuracy: 0.9290 - val_loss: 0.4932 - val_accuracy: 0.8345\n",
      "Epoch 73/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.2463 - accuracy: 0.9242\n",
      "Epoch 73: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2467 - accuracy: 0.9230 - val_loss: 0.5029 - val_accuracy: 0.8345\n",
      "Epoch 74/100\n",
      "40/42 [===========================>..] - ETA: 0s - loss: 0.2309 - accuracy: 0.9285\n",
      "Epoch 74: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2306 - accuracy: 0.9294 - val_loss: 0.4902 - val_accuracy: 0.8480\n",
      "Epoch 75/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2443 - accuracy: 0.9223\n",
      "Epoch 75: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2472 - accuracy: 0.9192 - val_loss: 0.5180 - val_accuracy: 0.8345\n",
      "Epoch 76/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.2379 - accuracy: 0.9264\n",
      "Epoch 76: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2379 - accuracy: 0.9271 - val_loss: 0.4963 - val_accuracy: 0.8345\n",
      "Epoch 77/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2361 - accuracy: 0.9295\n",
      "Epoch 77: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2387 - accuracy: 0.9275 - val_loss: 0.4796 - val_accuracy: 0.8480\n",
      "Epoch 78/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2398 - accuracy: 0.9272\n",
      "Epoch 78: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2374 - accuracy: 0.9264 - val_loss: 0.5195 - val_accuracy: 0.8378\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9361\n",
      "Epoch 79: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2252 - accuracy: 0.9361 - val_loss: 0.5070 - val_accuracy: 0.8412\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2147 - accuracy: 0.9343\n",
      "Epoch 80: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2147 - accuracy: 0.9343 - val_loss: 0.5764 - val_accuracy: 0.7939\n",
      "Epoch 81/100\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.2281 - accuracy: 0.9355\n",
      "Epoch 81: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2272 - accuracy: 0.9358 - val_loss: 0.4962 - val_accuracy: 0.8378\n",
      "Epoch 82/100\n",
      "41/42 [============================>.] - ETA: 0s - loss: 0.2316 - accuracy: 0.9287\n",
      "Epoch 82: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2322 - accuracy: 0.9282 - val_loss: 0.5125 - val_accuracy: 0.8480\n",
      "Epoch 83/100\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.2303 - accuracy: 0.9299\n",
      "Epoch 83: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2318 - accuracy: 0.9271 - val_loss: 0.4886 - val_accuracy: 0.8378\n",
      "Epoch 84/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2338 - accuracy: 0.9272\n",
      "Epoch 84: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2354 - accuracy: 0.9264 - val_loss: 0.4983 - val_accuracy: 0.8412\n",
      "Epoch 85/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2430 - accuracy: 0.9215\n",
      "Epoch 85: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2413 - accuracy: 0.9234 - val_loss: 0.4978 - val_accuracy: 0.8547\n",
      "Epoch 86/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2200 - accuracy: 0.9367\n",
      "Epoch 86: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2240 - accuracy: 0.9339 - val_loss: 0.4800 - val_accuracy: 0.8581\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2156 - accuracy: 0.9376\n",
      "Epoch 87: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2156 - accuracy: 0.9376 - val_loss: 0.5500 - val_accuracy: 0.8412\n",
      "Epoch 88/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2182 - accuracy: 0.9303\n",
      "Epoch 88: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2183 - accuracy: 0.9305 - val_loss: 0.5070 - val_accuracy: 0.8378\n",
      "Epoch 89/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2298 - accuracy: 0.9291\n",
      "Epoch 89: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2301 - accuracy: 0.9301 - val_loss: 0.4975 - val_accuracy: 0.8480\n",
      "Epoch 90/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.1976 - accuracy: 0.9400\n",
      "Epoch 90: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2010 - accuracy: 0.9380 - val_loss: 0.5044 - val_accuracy: 0.8412\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9358\n",
      "Epoch 91: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2204 - accuracy: 0.9358 - val_loss: 0.5302 - val_accuracy: 0.8142\n",
      "Epoch 92/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2275 - accuracy: 0.9346\n",
      "Epoch 92: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2260 - accuracy: 0.9350 - val_loss: 0.4986 - val_accuracy: 0.8311\n",
      "Epoch 93/100\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.1980 - accuracy: 0.9467\n",
      "Epoch 93: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.1946 - accuracy: 0.9493 - val_loss: 0.5152 - val_accuracy: 0.8412\n",
      "Epoch 94/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2150 - accuracy: 0.9362\n",
      "Epoch 94: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2155 - accuracy: 0.9380 - val_loss: 0.4977 - val_accuracy: 0.8480\n",
      "Epoch 95/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.1849 - accuracy: 0.9485\n",
      "Epoch 95: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.1941 - accuracy: 0.9455 - val_loss: 0.5318 - val_accuracy: 0.8345\n",
      "Epoch 96/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2081 - accuracy: 0.9396\n",
      "Epoch 96: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2096 - accuracy: 0.9403 - val_loss: 0.4945 - val_accuracy: 0.8480\n",
      "Epoch 97/100\n",
      "39/42 [==========================>...] - ETA: 0s - loss: 0.2034 - accuracy: 0.9419\n",
      "Epoch 97: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2057 - accuracy: 0.9406 - val_loss: 0.5248 - val_accuracy: 0.8446\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - ETA: 0s - loss: 0.2009 - accuracy: 0.9395\n",
      "Epoch 98: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2009 - accuracy: 0.9395 - val_loss: 0.5093 - val_accuracy: 0.8480\n",
      "Epoch 99/100\n",
      "38/42 [==========================>...] - ETA: 0s - loss: 0.2063 - accuracy: 0.9408\n",
      "Epoch 99: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2066 - accuracy: 0.9410 - val_loss: 0.4985 - val_accuracy: 0.8547\n",
      "Epoch 100/100\n",
      "37/42 [=========================>....] - ETA: 0s - loss: 0.2053 - accuracy: 0.9375\n",
      "Epoch 100: val_loss did not improve from 0.47350\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.2092 - accuracy: 0.9369 - val_loss: 0.5225 - val_accuracy: 0.8277\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "model = DLNN_CORENup(input_seq_shape = input_seq_shape,\n",
    "                     metrics='accuracy')\n",
    "    \n",
    "## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "current_model_path = os.path.join(modelPath, \"_fullModel.hdf5\")\n",
    "modelCallbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
    "                                       monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                       save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "]\n",
    "\n",
    "# adding random shuffling of the dataset for training purpose\n",
    "index_arr = np.arange(train_features.shape[0])\n",
    "index_arr = np.random.permutation(index_arr)\n",
    "\n",
    "# model.fit(x = train_features[index_arr], y = train_labels[index_arr], \n",
    "#           batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "#           callbacks = modelCallbacks, validation_data = (indpe_features, indpe_labels))\n",
    "model.fit(x = train_features[index_arr], y = train_labels[index_arr], \n",
    "          batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "          callbacks = modelCallbacks, validation_split = 0.1)\n",
    "\n",
    "model = tf.keras.models.load_model(current_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.633469</td>\n",
       "      <td>0.24898</td>\n",
       "      <td>0.679311</td>\n",
       "      <td>0.600985</td>\n",
       "      <td>0.639922</td>\n",
       "      <td>0.182844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.96400</td>\n",
       "      <td>0.991648</td>\n",
       "      <td>0.977688</td>\n",
       "      <td>0.963489</td>\n",
       "      <td>0.941271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.633469    0.24898  0.679311     0.600985     0.639922  0.182844\n",
       "Train        0.970588    0.96400  0.991648     0.977688     0.963489  0.941271"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Train dataset\n",
    "##################################################################################\n",
    "\n",
    "y_pred = model.predict(train_features)\n",
    "label_pred = pred2label(y_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(train_labels, label_pred)\n",
    "prec = precision_score(train_labels,label_pred)\n",
    "mcc = matthews_corrcoef(train_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(train_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(train_labels, y_pred)\n",
    "auc = roc_auc_score(train_labels, y_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Train\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "y_pred = model.predict(indpe_features)\n",
    "label_pred = pred2label(y_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, y_pred)\n",
    "auc = roc_auc_score(indpe_labels, y_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10, 5 - larger n/w\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.68898\t0.281863\t0.683085\t0.566502\t0.713307\t0.220747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5, 10\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.687347\t0.276119\t0.67988\t0.546798\t0.715264\t0.207522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after balancing the augmentation - large n/w - factor 3\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.68898\t0.265789\t0.680907\t0.497537\t0.727006\t0.18049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before balancing - large n/w - factor 3\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.733878\t0.273063\t0.666128\t0.364532\t0.807241\t0.153875\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.715918\t0.280967\t0.673619\t0.458128\t0.767123\t0.188607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.64      0.74      1022\n",
      "           1       0.25      0.60      0.35       203\n",
      "\n",
      "    accuracy                           0.63      1225\n",
      "   macro avg       0.57      0.62      0.55      1225\n",
      "weighted avg       0.78      0.63      0.68      1225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(indpe_labels, np.round(y_pred).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10, 5 - larger n/w\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.89      0.71      0.79      1022\n",
    "#            1       0.28      0.57      0.38       203\n",
    "\n",
    "#     accuracy                           0.69      1225\n",
    "#    macro avg       0.59      0.64      0.58      1225\n",
    "# weighted avg       0.79      0.69      0.72      1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5, 10 - larger n/w\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.89      0.72      0.79      1022\n",
    "#            1       0.28      0.55      0.37       203\n",
    "\n",
    "#     accuracy                           0.69      1225\n",
    "#    macro avg       0.58      0.63      0.58      1225\n",
    "# weighted avg       0.79      0.69      0.72      1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after balancing the augmentation - large n/w - factor 3\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.73      0.80      1022\n",
    "#            1       0.27      0.50      0.35       203\n",
    "\n",
    "#     accuracy                           0.69      1225\n",
    "#    macro avg       0.57      0.61      0.57      1225\n",
    "# weighted avg       0.78      0.69      0.72      1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before balancing - large n/w - factor 3\n",
    "\n",
    "# precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.75      0.81      1022\n",
    "#            1       0.29      0.50      0.37       203\n",
    "\n",
    "#     accuracy                           0.71      1225\n",
    "#    macro avg       0.59      0.63      0.59      1225\n",
    "# weighted avg       0.79      0.71      0.74      1225\n",
    "\n",
    "# precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.86      0.81      0.84      1022\n",
    "#            1       0.27      0.36      0.31       203\n",
    "\n",
    "#     accuracy                           0.73      1225\n",
    "#    macro avg       0.57      0.59      0.57      1225\n",
    "# weighted avg       0.77      0.73      0.75      1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 490)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(indpe_labels), np.sum(np.round(y_pred).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (203, 331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
