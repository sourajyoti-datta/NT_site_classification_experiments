{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 5\n",
    "expName = \"NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\"\n",
    "outPath = \"Results\"\n",
    "foldName = \"folds.pickle\"\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "seed = None\n",
    "\n",
    "input_data_folder = \"Data\"\n",
    "training_data_file = \"Training-datasets-PredNTS.txt\"\n",
    "independent_data_file = \"independent dataset-PredNTS.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.is_gpu_available(cuda_only=True))\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define all CUSTOM functions\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_encode_nt(sequence, char_dict):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),len(char_dict)))\n",
    "    \n",
    "    i = 0\n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in char_dict.keys()):\n",
    "            seq_encoded[i][char_dict[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            raise ValueError('Incorrect character in NT sequence: '+sequence)\n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Build k-fold functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def labels_1d_to_2d(labels_1d):\n",
    "    labels_2d = np.eye(2)[labels_1d]\n",
    "    return labels_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to customize the DLNN architecture with parameters\n",
    "##################################################################################\n",
    "\n",
    "def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "                 conv_filters_per_layer_1 = 50, kernel_length_1 = 5, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "                 max_pool_width_1 = 2, max_pool_stride_1 = 2, ## 1st Maxpool layer parameters\n",
    "                 lstm_decode_units = 50, ## LSTM layer parameters\n",
    "                 conv_filters_per_layer_2 = 50,  kernel_length_2 = 10, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "                 max_pool_width_2 = 2, max_pool_stride_2 = 2, ## 2nd Maxpool layer parameters\n",
    "                 dense_decode_units = 370, ## Dense layer parameters\n",
    "                 prob = 0.5, learn_rate = 0.0003, loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  SEQUENCE  ##################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "                                strides = conv_strides_1, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = \"same\")(input1)\n",
    "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "    x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "    ## Conv Path\n",
    "\n",
    "    x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, \n",
    "                                strides = conv_strides_2, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = 'same')(x1)\n",
    "    x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "    x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "    x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  Classifier  ################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    y = tf.keras.layers.Dense(dense_decode_units, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'relu')(x4)\n",
    "    \n",
    "    y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(1, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'sigmoid')(y)\n",
    "\n",
    "    ## Generate Model from input and output\n",
    "    model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), \n",
    "                      loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), \n",
    "                      loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Function to customize the DLNN architecture with parameters\n",
    "# ##################################################################################\n",
    "\n",
    "# def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "#                  conv_filters_per_layer_1 = 10, kernel_length_1 = 10, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "#                  max_pool_width_1 = 3, max_pool_stride_1 = 3, ## 1st Maxpool layer parameters\n",
    "#                  lstm_decode_units = 10, ## LSTM layer parameters\n",
    "#                  conv_filters_per_layer_2 = 10,  kernel_length_2 = 5, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "#                  max_pool_width_2 = 3, max_pool_stride_2 = 3, ## 2nd Maxpool layer parameters\n",
    "#                  dense_decode_units = 32, ## Dense layer parameters\n",
    "#                  prob = 0.5, learn_rate = 0.0005, \n",
    "#                  loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "#     beta = 0.001\n",
    "    \n",
    "#     ######################################################################################################\n",
    "#     ########  SEQUENCE  ##################################################################################\n",
    "#     ######################################################################################################\n",
    "    \n",
    "#     input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "#     x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "#                                 strides = conv_strides_1, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                                 padding = \"same\")(input1)\n",
    "#     x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "#     x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "#     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "#     ## LSTM Path\n",
    "\n",
    "#     x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    \n",
    "#     x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "#     x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "#     ## Conv Path\n",
    "\n",
    "#     x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, strides = conv_strides_2, \n",
    "#                                 kernel_regularizer = tf.keras.regularizers.l2(beta), padding = 'same')(x1)\n",
    "#     x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "#     x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "#     x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "#     x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "#     x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "#     ######################################################################################################\n",
    "#     ########  Classifier  ################################################################################\n",
    "#     ######################################################################################################\n",
    "    \n",
    "#     y = tf.keras.layers.Dense(dense_decode_units, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                               activation = 'relu')(x4)\n",
    "    \n",
    "#     y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "#     y = tf.keras.layers.Dense(1, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta),\n",
    "#                               activation = 'sigmoid')(y)\n",
    "\n",
    "#     ## Generate Model from input and output\n",
    "#     model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "#     ## Compile model\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss, metrics = metrics)\n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 41, 21)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 41, 50)       5300        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 41, 50)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 20, 50)       0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20, 50)       0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 20, 50)       25050       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 20, 50)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 20, 50)       20200       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 10, 50)      0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20, 50)       0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10, 50)       0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1000)         0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 500)          0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1500)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 370)          555370      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 370)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            371         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 606,291\n",
      "Trainable params: 606,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DLNN_CORENup().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### read training file\n",
    "##################################################################################\n",
    "train_file_path = os.path.join(input_data_folder, training_data_file)\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t', header=None)\n",
    "train_data.columns = ['Sequence', 'name', 'id', 'flag', 'label_original', 'type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### read independent data file\n",
    "##################################################################################\n",
    "indpe_file_path = os.path.join(input_data_folder, independent_data_file)\n",
    "indpe_data = pd.read_csv(indpe_file_path, sep='\\t', header=None)\n",
    "indpe_data.columns = ['Sequence', 'name', 'id', 'flag', 'label_original', 'type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty char count per sequence\n",
    "train_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in train_data['Sequence']]\n",
    "# incomplete sequence flag\n",
    "train_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in train_data['Sequence']]\n",
    "\n",
    "indpe_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in indpe_data['Sequence']]\n",
    "indpe_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in indpe_data['Sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_get_stats(source_data, target_data):\n",
    "    \n",
    "    # empty char count per sequence\n",
    "    source_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in source_data['Sequence']]\n",
    "    # incomplete sequence flag\n",
    "    source_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in source_data['Sequence']]\n",
    "    \n",
    "    target_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in target_data['Sequence']]\n",
    "    target_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in target_data['Sequence']]\n",
    "\n",
    "    # 0:1\n",
    "    train_label_nonempty_ratio = source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    train_label_ratio = (source_data.shape[0]-sum(source_data[\"label_original\"] == 1)) / sum(source_data[\"label_original\"] == 1)\n",
    "\n",
    "    # 0:1\n",
    "    indpe_label_nonempty_ratio = target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    indpe_label_ratio = (target_data.shape[0]-sum(target_data[\"label_original\"] == 1)) / sum(target_data[\"label_original\"] == 1)\n",
    "\n",
    "    print('Current train_label_nonempty_ratio:', train_label_nonempty_ratio, 'train_label_ratio:', train_label_ratio)\n",
    "    print('Target indpe_label_nonempty_ratio:', indpe_label_nonempty_ratio, 'indpe_label_ratio:', indpe_label_ratio)\n",
    "\n",
    "    increase_0_data_factor = int(round(indpe_label_ratio/train_label_ratio)) - 1\n",
    "    increase_empty_data_factor = int(round(indpe_label_nonempty_ratio/train_label_nonempty_ratio)) - 1\n",
    "    \n",
    "    return increase_0_data_factor, increase_empty_data_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD7CAYAAABzGc+QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAANmklEQVR4nO3df6jd9X3H8edrxv6jssblLsucaVaRgvujUS7ipisOO2fjqLo/pDLabBXSQgWFjhFa6PwzbtPBxnDEKWbDuW6oU9p0NRNBClN2I1Hjjy4qkRliEmepyv7Y1Pf+uN+U4/Wce0/u+ZWPeT7gcL7n8/187+fN537v637P95zvOakqJEnt+blZFyBJWh0DXJIaZYBLUqMMcElqlAEuSY0ywCWpUSsGeJJzkzye5IUkzye5uWu/NcmhJPu625bJlytJOi4rvQ88yQZgQ1U9neQsYC9wLXA98G5V/fnEq5QkfcSalTpU1WHgcLf8TpIXgXNWM9i6detq06ZNq9lUkk5Ze/fufbOq5pa2rxjgvZJsAi4EngIuBW5K8hVgAfhmVf1kue03bdrEwsLCiQwpSae8JK/1ax/6RcwkZwIPALdU1dvAncB5wGYWj9BvH7DdtiQLSRaOHTt2onVLkgYYKsCTnM5ieN9XVQ8CVNWRqnq/qj4A7gIu7rdtVe2sqvmqmp+b+8gzAEnSKg3zLpQAdwMvVtUdPe0berpdB+wff3mSpEGGOQd+KfBl4Lkk+7q2bwE3JNkMFHAQ+NoE6pMkDTDMu1B+BKTPqt3jL0eSNCyvxJSkRhngktQoA1ySGmWAS1KjTuhKTJ0aNm3//szGPrjj6pmNLbXGI3BJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqP8QoeT2Cy/WEHSyc8jcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjvJBnCF5QI+lk5BG4JDXKAJekRhngktSoFQM8yblJHk/yQpLnk9zctZ+dZE+SA9392smXK0k6bpgj8PeAb1bVBcAlwDeSXABsBx6rqvOBx7rHkqQpWTHAq+pwVT3dLb8DvAicA1wD7Oq67QKunVCNkqQ+TugceJJNwIXAU8D6qjrcrXoDWD/e0iRJyxn6feBJzgQeAG6pqreT/GxdVVWSGrDdNmAbwMaNG0erVtLYzPL6hoM7rp7Z2B8nQx2BJzmdxfC+r6oe7JqPJNnQrd8AHO23bVXtrKr5qpqfm5sbR82SJIZ7F0qAu4EXq+qOnlWPAFu75a3Aw+MvT5I0yDCnUC4Fvgw8l2Rf1/YtYAfwT0luBF4Drp9IhZKkvlYM8Kr6EZABq68YbzmSpGF5JaYkNcoAl6RGGeCS1CgDXJIa5Rc6SPilHWqTR+CS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEY184UOfuC+JH2YR+CS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRjVzIY+kj49ZXZh3cMfVMxl3UjwCl6RGGeCS1CgDXJIatWKAJ7knydEk+3vabk1yKMm+7rZlsmVKkpYa5gj8XuCqPu1/UVWbu9vu8ZYlSVrJigFeVU8Ab02hFknSCRjlHPhNSZ7tTrGsHVtFkqShrDbA7wTOAzYDh4HbB3VMsi3JQpKFY8eOrXI4SdJSqwrwqjpSVe9X1QfAXcDFy/TdWVXzVTU/Nze32jolSUusKsCTbOh5eB2wf1BfSdJkrHgpfZL7gcuBdUleB/4EuDzJZqCAg8DXJleiJKmfFQO8qm7o03z3BGqRJJ0Ar8SUpEYZ4JLUKANckhplgEtSo/xCB51UZvVB/1KLPAKXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEatmXUBkjQtm7Z/f2ZjH9xx9dh/pkfgktQoA1ySGmWAS1KjDHBJatSKAZ7kniRHk+zvaTs7yZ4kB7r7tZMtU5K01DBH4PcCVy1p2w48VlXnA491jyVJU7RigFfVE8BbS5qvAXZ1y7uAa8dbliRpJas9B76+qg53y28A68dUjyRpSCO/iFlVBdSg9Um2JVlIsnDs2LFRh5MkdVYb4EeSbADo7o8O6lhVO6tqvqrm5+bmVjmcJGmp1Qb4I8DWbnkr8PB4ypEkDWuYtxHeD/w78Jkkrye5EdgB/HaSA8Dnu8eSpCla8cOsquqGAauuGHMtkqQT4JWYktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhq1ZpSNkxwE3gHeB96rqvlxFCVJWtlIAd75rap6cww/R5J0AjyFIkmNGjXAC3g0yd4k28ZRkCRpOKOeQrmsqg4l+UVgT5KXquqJ3g5dsG8D2Lhx44jDSZKOG+kIvKoOdfdHgYeAi/v02VlV81U1Pzc3N8pwkqQeqw7wJGckOev4MnAlsH9chUmSljfKKZT1wENJjv+cf6iqfx1LVZKkFa06wKvqVeCzY6xFknQCfBuhJDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEaNFOBJrkry4yQvJ9k+rqIkSStbdYAnOQ34a+ALwAXADUkuGFdhkqTljXIEfjHwclW9WlX/C/wjcM14ypIkrWSUAD8H+K+ex693bZKkKVgz6QGSbAO2dQ/fTfLjSY+5SuuAN2ddxDKsbzTWNxrrG1FuG6nGT/VrHCXADwHn9jz+la7tQ6pqJ7BzhHGmIslCVc3Puo5BrG801jca6xvdJGoc5RTKfwDnJ/nVJJ8AvgQ8Mp6yJEkrWfUReFW9l+Qm4IfAacA9VfX82CqTJC1rpHPgVbUb2D2mWmbtZD/NY32jsb7RWN/oxl5jqmrcP1OSNAVeSi9JjTqlAjzJuUkeT/JCkueT3Nynz+VJfppkX3f7zpRrPJjkuW7shT7rk+Qvu48veDbJRVOs7TM987IvydtJblnSZ6rzl+SeJEeT7O9pOzvJniQHuvu1A7bd2vU5kGTrFOv7syQvdb+/h5J8csC2y+4LE6zv1iSHen6HWwZsO/GP0hhQ33d7ajuYZN+Abacxf30zZWr7YFWdMjdgA3BRt3wW8J/ABUv6XA58b4Y1HgTWLbN+C/ADIMAlwFMzqvM04A3gU7OcP+BzwEXA/p62PwW2d8vbgdv6bHc28Gp3v7ZbXjul+q4E1nTLt/Wrb5h9YYL13Qr80RC//1eATwOfAJ5Z+rc0qfqWrL8d+M4M569vpkxrHzyljsCr6nBVPd0tvwO8SHtXj14D/F0tehL4ZJINM6jjCuCVqnptBmP/TFU9Aby1pPkaYFe3vAu4ts+mvwPsqaq3quonwB7gqmnUV1WPVtV73cMnWbyGYiYGzN8wpvJRGsvVlyTA9cD94x53WMtkylT2wVMqwHsl2QRcCDzVZ/WvJ3kmyQ+S/Np0K6OAR5Ps7a5iXepk+QiDLzH4D2eW8wewvqoOd8tvAOv79DlZ5vGrLD6j6melfWGSbupO8dwz4On/yTB/vwkcqaoDA9ZPdf6WZMpU9sFTMsCTnAk8ANxSVW8vWf00i6cFPgv8FfAvUy7vsqq6iMVPefxGks9NefwVdRdufRH45z6rZz1/H1KLz1VPyrdaJfk28B5w34Aus9oX7gTOAzYDh1k8TXEyuoHlj76nNn/LZcok98FTLsCTnM7iRN9XVQ8uXV9Vb1fVu93ybuD0JOumVV9VHerujwIPsfhUtddQH2EwYV8Anq6qI0tXzHr+OkeOn1bq7o/26TPTeUzyB8DvAr/f/YF/xBD7wkRU1ZGqer+qPgDuGjDurOdvDfB7wHcH9ZnW/A3IlKnsg6dUgHfnzO4GXqyqOwb0+aWuH0kuZnGO/ntK9Z2R5Kzjyyy+2LV/SbdHgK9k0SXAT3ueqk3LwCOfWc5fj0eA46/obwUe7tPnh8CVSdZ2pwiu7NomLslVwB8DX6yq/xnQZ5h9YVL19b6mct2AcWf9URqfB16qqtf7rZzW/C2TKdPZByf5Cu3JdgMuY/GpzLPAvu62Bfg68PWuz03A8yy+qv4k8BtTrO/T3bjPdDV8u2vvrS8sfpHGK8BzwPyU5/AMFgP553vaZjZ/LP4jOQz8H4vnEG8EfgF4DDgA/Btwdtd3Hvjbnm2/Crzc3f5wivW9zOK5z+P74N90fX8Z2L3cvjCl+v6+27eeZTGINiytr3u8hcV3Xbwyzfq69nuP73M9fWcxf4MyZSr7oFdiSlKjTqlTKJL0cWKAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUqP8HZ8QuFg1RO/MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 183\n",
      "Ratio empty/total: 0.07682619647355164\n"
     ]
    }
   ],
   "source": [
    "plt.hist(train_data['Empty_count'][train_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(train_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(train_data['has_empty'])/train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(train_data['Empty_count'][(train_data['Empty_count'] != 0) & (train_data['label_original'] == -1)])\n",
    "axs[1].hist(train_data['Empty_count'][(train_data['Empty_count'] != 0) & (train_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_original</th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_original  has_empty\n",
       "0              -1         39\n",
       "1               1        144"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 68\n",
      "Ratio empty/total: 0.05551020408163265\n"
     ]
    }
   ],
   "source": [
    "plt.hist(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(indpe_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(indpe_data['has_empty'])/indpe_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(indpe_data['Empty_count'][(indpe_data['Empty_count'] != 0) & (indpe_data['label_original'] == -1)])\n",
    "axs[1].hist(indpe_data['Empty_count'][(indpe_data['Empty_count'] != 0) & (indpe_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_original</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                has_empty\n",
       "label_original           \n",
       "-1                     52\n",
       " 1                     16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indpe_data.groupby([\"label_original\"]).sum().filter(['has_empty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train_label_nonempty_ratio: 0.2708333333333333 train_label_ratio: 1.0\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n"
     ]
    }
   ],
   "source": [
    "_,_ = print_and_get_stats(train_data, indpe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation on Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sequence_truncate(seq, random_length):\n",
    "    rep_str = \"\".join(['-']*random_length)\n",
    "    if np.random.choice((True, False)):\n",
    "        return_seq = rep_str + seq[random_length:]\n",
    "    else:\n",
    "        return_seq = seq[:-random_length] + rep_str\n",
    "    return return_seq\n",
    "\n",
    "def random_sequence_truncate(seq):\n",
    "    random_length = np.random.randint(1, 20)\n",
    "    return_seq = sequence_truncate(seq, random_length)\n",
    "    return return_seq\n",
    "\n",
    "def repeat_truncate_sequence_steps(seq, factor):\n",
    "    random_length = random.sample(range(1, int(len(seq)/2)), \n",
    "                                  factor)\n",
    "    return_seqs = []\n",
    "    for i in range(factor):\n",
    "        ret_seq = sequence_truncate(seq, random_length[i])\n",
    "        return_seqs.append(ret_seq)\n",
    "    return return_seqs\n",
    "\n",
    "def truncate_sequence_by_len(seq, ran_len):\n",
    "    return_seqs = []\n",
    "    ret_seq = sequence_truncate(seq, ran_len)\n",
    "    return ret_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_get_stats(source_data, target_data):\n",
    "    \n",
    "    # empty char count per sequence\n",
    "    source_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in source_data['Sequence']]\n",
    "    # incomplete sequence flag\n",
    "    source_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in source_data['Sequence']]\n",
    "    \n",
    "    target_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in target_data['Sequence']]\n",
    "    target_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in target_data['Sequence']]\n",
    "\n",
    "    # 0:1\n",
    "    train_label_nonempty_ratio = source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    train_label_ratio = (source_data.shape[0]-sum(source_data[\"label_original\"] == 1)) / sum(source_data[\"label_original\"] == 1)\n",
    "\n",
    "    # 0:1\n",
    "    indpe_label_nonempty_ratio = target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    indpe_label_ratio = (target_data.shape[0]-sum(target_data[\"label_original\"] == 1)) / sum(target_data[\"label_original\"] == 1)\n",
    "\n",
    "    print('Current train_label_nonempty_ratio:', train_label_nonempty_ratio, 'train_label_ratio:', train_label_ratio)\n",
    "    print('Target indpe_label_nonempty_ratio:', indpe_label_nonempty_ratio, 'indpe_label_ratio:', indpe_label_ratio)\n",
    "\n",
    "    increase_0_data_factor = int(round(indpe_label_ratio/train_label_ratio)) - 1\n",
    "    increase_empty_data_factor = int(round(indpe_label_nonempty_ratio/train_label_nonempty_ratio)) - 1\n",
    "    \n",
    "    return increase_0_data_factor, increase_empty_data_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=10)\n",
    "\n",
    "bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "cdf = np.cumsum(hist)\n",
    "cdf = cdf / cdf[-1]\n",
    "values = np.random.rand(100)\n",
    "value_bins = np.searchsorted(cdf, values)\n",
    "random_from_cdf = bin_midpoints[value_bins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAASsklEQVR4nO3de4yldX3H8fenoG1FUlA2qwLrUktI1AiSCWqlFEUpt4g2xkK04qVZNdhqY2JWTcTYNMF4aytWugpdrIjWC0oKKhu0URNFZ+kqC6hQusquK7uI4jWxq9/+Mc+a02HOzJlzf+a8X8nmPJffc86XM7/58MzvPM/vpKqQJLXP70y6AElSfwxwSWopA1ySWsoAl6SWMsAlqaUOHeeLHXXUUbVx48ZxvqRmyPbt2++rqnWTeG37tkapW98ea4Bv3LiR+fn5cb6kZkiS707qte3bGqVufdshFElqKQNcklrKAJekljLAJamlDHBJaikDXJJaygCXpJYywCWppQxwSWqpsd6JOU02br5+1cfsuvTcEVQiTV4/vw/g78SkeQYuSS01s2fgWruSXAmcB+yrqic22z4KnNA0OQL4cVWdtMSxu4CfAr8GDlTV3BhKlvpigGst2gpcBnzw4Iaq+ouDy0neCTywzPHPqKr7RladNCQGuNacqvpiko1L7UsS4AXAM8dalDQCjoFr1vwJcG9V3dllfwE3JtmeZNNyT5RkU5L5JPP79+8feqHSSgxwzZoLgWuW2X9qVZ0MnA1cnOS0bg2raktVzVXV3Lp1E/keCc04A1wzI8mhwJ8DH+3Wpqr2NI/7gGuBU8ZTnbR6BrhmybOAb1XV7qV2JjksyeEHl4EzgZ1jrE9aFQNca06Sa4CvACck2Z3k5c2uC1g0fJLkMUluaFbXA19O8g3ga8D1VfXZcdUtrZZXoWjNqaoLu2x/yRLbvg+c0yzfDZw40uKkITLAJa1pa3magBWHUJJcmWRfkp0d296e5FtJvpnk2iRHjLRKSdKD9DIGvhU4a9G2bcATq+pJwHeANwy5LknSClYM8Kr6InD/om03VtWBZvWrwDEjqE2StIxhXIXyMuAzQ3geSdIqDPQhZpI3AQeAq5dpswnYBLBhw4ZBXm7i1vKHIZLap+8z8CQvYWHKzhdWVXVr5+3GkjQafZ2BJzkLeD3wp1X1i+GWJEnqRS+XES51V9tlwOHAtiQ7klw+4jolSYuseAbe5a62K0ZQiyRpFZwLRZJaygCXpJYywCWppQxwSWopA1ySWsoAl6SWMsAlqaUMcElqKQNcklrKAJekljLAteZ0+RrAtyTZ08zdsyPJOV2OPSvJt5PclWTz+KqWVs8vNdZvraH5zreyMOHaBxdtf3dVvaPbQUkOAd4LPBvYDXw9yXVVdfuoCpUG4Rm41pylvgawR6cAd1XV3VX1K+AjwPlDLU4aIs/ANUteneTFwDzwuqr60aL9RwP3dKzvBp7S7cmm8dum+v0rSu3kGbhmxfuAxwEnAXuBdw76hH7blCbNANdMqKp7q+rXVfUb4P0sDJcstgc4tmP9mGabNJUMcM2EJI/uWH0esHOJZl8Hjk9yXJKHAhcA142jPqkfjoFrzWm+BvB04Kgku4FLgNOTnAQUsAt4RdP2McAHquqcqjqQ5NXA54BDgCur6rbx/xdIvTHAteas5msAq+r7wDkd6zcAN4yoNGmoHEKRpJYywCWppQxwSWopA1ySWmrFAO8yMdAjkmxLcmfzeORoy5QkLdbLGfhW4KxF2zYDN1XV8cBNzbokaYxWDPAuEwOdD1zVLF8FPHe4ZUmSVtLvGPj6qtrbLP8AWD+keiRJPRr4Q8yqKhbubltSkk1J5pPM79+/f9CXkyQ1+g3wew/OLdE87uvW0BnbJGk0+g3w64CLmuWLgE8PpxxJUq96uYzwGuArwAlJdid5OXAp8OwkdwLPatYlSWO04mRWXSYGAjhjyLVIklbBOzElqaUMcElqKQNcklrKAJeklvIbeST1bePm6/s6btel5w65ktnkGbgktZQBLkktZYBrzekyh/3bk3wryTeTXJvkiC7H7kpya5IdSebHVrTUBwNca9FWHjyH/TbgiVX1JOA7wBuWOf4ZVXVSVc2NqD5pKAxwrTlLzWFfVTdW1YFm9avAMWMvTBoyA1yz6GXAZ7rsK+DGJNuTbFruSZwqWZNmgGumJHkTcAC4ukuTU6vqZOBs4OIkp3V7LqdK1qQZ4JoZSV4CnAe8sPkikgepqj3N4z7gWuCUsRUorZIBrpmQ5Czg9cBzquoXXdocluTwg8vAmcDOpdpK08AA15rTZQ77y4DDgW3NJYKXN20fk+SG5tD1wJeTfAP4GnB9VX12Av8JUk+8lV5rTpc57K/o0vb7wDnN8t3AiSMsTRoqz8AlqaUMcElqKQNcklrKAJekljLAJamlDHBJaikDXJJaaqAAT/K3SW5LsjPJNUl+b1iFSZKW13eAJzka+BtgrqqeCBwCXDCswiRJyxt0COVQ4PeTHAo8DPj+4CVJknrRd4A3s7a9A/gesBd4oKpuXNzOOZMlaTT6ngslyZHA+cBxwI+BjyV5UVV9qLNdVW0BtgDMzc0tOYWnJK0VGzdf39dxuy49d9XHDDKE8izgf6pqf1X9L/BJ4I8HeD5J0ioMEuDfA56a5GFJApwB3DGcsiRJKxlkDPxm4OPALcCtzXNtGVJdkqQVDDQfeFVdAlwypFokSavgnZiS1FIGuCS1lAEuSS1lgEtSSxngWpOSXJlkX5KdHdsekWRbkjubxyO7HHtR0+bOJBeNr2ppdQxwrVVbgbMWbdsM3FRVxwM3Nev/T5JHsHBl1VOAU4BLugW9NGkGuNakqvoicP+izecDVzXLVwHPXeLQPwO2VdX9VfUjYBsP/h+BNBUMcM2S9VW1t1n+AbB+iTZHA/d0rO9utj2IE7Vp0gxwzaSqKmCgydWqaktVzVXV3Lp164ZUmdQ7A1yz5N4kjwZoHvct0WYPcGzH+jHNNmnqGOCaJdcBB68quQj49BJtPgecmeTI5sPLM5tt0tQxwLUmJbkG+ApwQpLdSV4OXAo8O8mdLEyHfGnTdi7JBwCq6n7g74CvN//e2myTps5Ak1lJ06qqLuyy64wl2s4Df9WxfiVw5YhKk4bGM3BJaikDXJJaygCXpJYywCWppQxwSWopr0KRpCVs3Hz9pEtYkWfgktRSBrgktZQBLkktZYBLUksNFOBJjkjy8STfSnJHkqcNqzBJ0vIGvQrlH4HPVtXzkzwUeNgQapIk9aDvAE/yB8BpwEsAqupXwK+GU5YkaSWDnIEfB+wH/jXJicB24DVV9fPORkk2AZsANmzYMMDLaVr1c73srkvPHUEl0mwZZAz8UOBk4H1V9WTg5yzxLd9+7ZQkjcYgAb4b2F1VNzfrH2ch0CVJY9B3gFfVD4B7kpzQbDoDuH0oVUmSVjToVSh/DVzdXIFyN/DSwUuSJPVioACvqh3A3HBKkSSthndiamYkOSHJjo5/P0ny2kVtTk/yQEebN0+oXGlFTiermVFV3wZOAkhyCLAHuHaJpl+qqvPGWJrUF8/ANavOAP67qr476UKkfhngmlUXANd02fe0JN9I8pkkTxhnUdJqGOCaOc1VU88BPrbE7luAx1bVicB7gE8t8zybkswnmd+/f/9IapWWY4BrFp0N3FJV9y7eUVU/qaqfNcs3AA9JctRST+Jdxpo0A1yz6EK6DJ8keVSSNMunsPA78sMx1ib1zKtQNFOSHAY8G3hFx7ZXAlTV5cDzgVclOQD8ErigqmoStUorMcA1U5rZMh+5aNvlHcuXAZeNuy6pHw6hSFJLeQY+Bv3Mlw39z5nd7+uN07jfE2kt8gxcklrKAJekljLAJamlDHBJaikDXJJaygCXpJYywCWppQxwSWopA1ySWsoAl6SW8lZ6Sa3Qhikixs0zcElqqYEDPMkhSf4ryX8MoyBJUm+GcQb+GuCOITyPJGkVBgrwJMcA5wIfGE45kqReDfoh5j8ArwcO79YgySZgE8CGDRu6PpEfUEjS6vR9Bp7kPGBfVW1frp3f3C1JozHIEMrTgeck2QV8BHhmkg8NpSpJ0or6DvCqekNVHVNVG4ELgM9X1YuGVpk0Ikl2Jbk1yY4k80vsT5J/SnJXkm8mOXkSdUor8UYezapnVNV9XfadDRzf/HsK8L7mUZoqQ7mRp6r+s6rOG8ZzSVPgfOCDteCrwBFJHj3poqTFPAPXLCrgxiQF/EtVbVm0/2jgno713c22vZ2NernCqt+rq3Zdem5fx2m2eCu9ZtGpVXUyC0MlFyc5rZ8n8QorTZoBrplTVXuax33AtcApi5rsAY7tWD+m2SZNFQNcMyXJYUkOP7gMnAnsXNTsOuDFzdUoTwUeqKq9SFPGMXDNmvXAtUlgof9/uKo+m+SVAFV1OXADcA5wF/AL4KUTqlValgGumVJVdwMnLrH98o7lAi4eZ11SPxxCkaSWMsAlqaUMcElqKQNcklrKDzGnmHOkS1qOZ+CS1FKegUtTyL++1AvPwCWppQxwSWopA1ySWsoAl6SWMsAlqaUMcElqKQNcklrKAJekljLAJamlDHBJaqm+AzzJsUm+kOT2JLclec0wC5MkLW+QuVAOAK+rqluaL4ndnmRbVd0+pNokScvo+wy8qvZW1S3N8k+BO4Cjh1WYJGl5QxkDT7IReDJw8xL7NiWZTzK/f//+Ybyc1Jdehv2SnJ7kgSQ7mn9vnkStUi8Gnk42ycOBTwCvraqfLN5fVVuALQBzc3M16OtJA+h12O9LVXXeBOqTVmWgM/AkD2EhvK+uqk8OpyRpNBz201ozyFUoAa4A7qiqdw2vJGn0lhv2A56W5BtJPpPkCcs8h8ODmqhBzsCfDvwl8MyO8cJzhlSXNDIrDPvdAjy2qk4E3gN8qtvzVNWWqpqrqrl169aNrF6pm77HwKvqy0CGWIs0cisN+3UGelXdkOSfkxxVVfeNs06pF96JqZnRy7Bfkkc17UhyCgu/Iz8cX5VS7/xSY82Sg8N+tybZ0Wx7I7ABoKouB54PvCrJAeCXwAVV5dVTmkoGuGZGL8N+VXUZcNl4KpIGY4BLGruNm6+fdAlrgmPgktRSBrgktZQBLkktZYBLUksZ4JLUUga4JLWUAS5JLWWAS1JLGeCS1FIGuCS1lAEuSS1lgEtSSxngktRSBrgktZQBLkktZYBLUksZ4JLUUga4JLWUAS5JLTVQgCc5K8m3k9yVZPOwipJGZaU+m+R3k3y02X9zko0TKFPqSd8BnuQQ4L3A2cDjgQuTPH5YhUnD1mOffTnwo6r6I+DdwNvGW6XUu0HOwE8B7qqqu6vqV8BHgPOHU5Y0Er302fOBq5rljwNnJMkYa5R6dugAxx4N3NOxvht4yuJGSTYBm5rVnyX59gCvOWxHAfdNuogO01YPTFlNeduy9Tx2hcN76bO/bVNVB5I8ADxyqdecor49VT+jLqa9xonXl+X/1luybw8S4D2pqi3AllG/Tj+SzFfV3KTrOGja6oHpq2ma6pmWvj1N70k3017jtNfXzSBDKHuAYzvWj2m2SdOqlz772zZJDgX+APjhWKqTVmmQAP86cHyS45I8FLgAuG44ZUkj0UufvQ64qFl+PvD5qqox1ij1rO8hlGZ88NXA54BDgCur6rahVTYeE//zd5Fpqwemr6a+6+nWZ5O8FZivquuAK4B/S3IXcD8LIT/tpu1ntJRpr3Ha61tSPLmQpHbyTkxJaikDXJJaamYDPMmuJLcm2ZFkfgKvf2WSfUl2dmx7RJJtSe5sHo+cgprekmRP8z7tSHLOGOs5NskXktye5LYkr2m2T/R9miaT7sdL1DN1/brHGifWzwcxswHeeEZVnTSh6z+3Amct2rYZuKmqjgduatYnXRPAu5v36aSqumGM9RwAXldVjweeClzc3Po+6fdp2kyyHy+2lenr14ttZbr6ed9mPcAnpqq+yMJVDp06b+O+CnjuFNQ0MVW1t6puaZZ/CtzBwp2SE32f1N009uvFpq2fD2KWA7yAG5Nsb26Jngbrq2pvs/wDYP0ki+nw6iTfbP70nMifv82sgE8GbmZ636dJmMZ+vFhbfl4T7+erNcsBfmpVnczCzHQXJzlt0gV1am4emYZrPN8HPA44CdgLvHPcBSR5OPAJ4LVV9ZPOfVP0Pk3KVPfjxab45zXxft6PmQ3wqtrTPO4DrmVhprpJuzfJowGax30Troequreqfl1VvwHez5jfpyQPYSG8r66qTzabp+59mpQp7ceLTf3Pa9L9vF8zGeBJDkty+MFl4Exg5/JHjUXnbdwXAZ+eYC3Ab3/hDnoeY3yfmmlcrwDuqKp3deyauvdpEqa4Hy829T+vSfbzQczknZhJ/pCFsxVYmE7gw1X192Ou4RrgdBamsbwXuAT4FPDvwAbgu8ALqmpsH7Z0qel0Fv6sLGAX8IqO8cxR13Mq8CXgVuA3zeY3sjAOPrH3aVpMQz9ebBr79WLT1s8HMZMBLklrwUwOoUjSWmCAS1JLGeCS1FIGuCS1lAEuSS1lgEtSSxngktRS/wex+id480Jg6QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "plt.hist(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], 10)\n",
    "plt.subplot(122)\n",
    "plt.hist(np.round(random_from_cdf).astype(int), 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6963, 8)\n",
      "Current train_label_nonempty_ratio: 0.9213483146067416 train_label_ratio: 1.0306211723534557\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n",
      "(4581, 8)\n",
      "\n",
      "##### After removing duplicates #####\n",
      "\n",
      "Current train_label_nonempty_ratio: 1.0 train_label_ratio: 1.0469168900804289\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n"
     ]
    }
   ],
   "source": [
    "# before balancing\n",
    "\n",
    "# train_data = train_data_backup\n",
    "\n",
    "########\n",
    "\n",
    "factor = 1\n",
    "\n",
    "data = train_data\n",
    "neg_data = data[data['label_original'] == -1].reset_index(drop=True)\n",
    "pos_data = data[data['label_original'] == 1].reset_index(drop=True)\n",
    "# neg_data = data.reset_index(drop=True)\n",
    "\n",
    "not_empty_neg_idxs = np.where(neg_data['has_empty'] != True)[0]\n",
    "not_empty_pos_idxs = np.where(pos_data['has_empty'] != True)[0]\n",
    "not_empty_pos_idxs = np.random.permutation(not_empty_pos_idxs)[0:int(not_empty_neg_idxs.shape[0]/factor)]\n",
    "\n",
    "##### Getting missing distribution\n",
    "hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=20)\n",
    "bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "cdf = np.cumsum(hist)\n",
    "cdf = cdf / cdf[-1]\n",
    "\n",
    "##### Negative records augmentation\n",
    "neg_values = np.random.rand(not_empty_neg_idxs.shape[0])\n",
    "neg_value_bins = np.searchsorted(cdf, neg_values)\n",
    "neg_random_from_cdf = bin_midpoints[neg_value_bins]\n",
    "neg_random_from_cdf = np.round(neg_random_from_cdf).astype(int)\n",
    "\n",
    "for idx, ran_len in zip(list(not_empty_neg_idxs), list(list(neg_random_from_cdf))):\n",
    "    record = neg_data.iloc[idx].to_dict()\n",
    "    seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "    record['Sequence'] = seq\n",
    "    neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "##### Positive records augmentation\n",
    "pos_values = np.random.rand(not_empty_pos_idxs.shape[0])\n",
    "pos_value_bins = np.searchsorted(cdf, pos_values)\n",
    "pos_random_from_cdf = bin_midpoints[pos_value_bins]\n",
    "pos_random_from_cdf = np.round(pos_random_from_cdf).astype(int)\n",
    "    \n",
    "for idx, ran_len in zip(list(not_empty_pos_idxs), list(list(pos_random_from_cdf))):\n",
    "    record = pos_data.iloc[idx].to_dict()\n",
    "    seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "    record['Sequence'] = seq\n",
    "    pos_data = pos_data.append(record, ignore_index=True)\n",
    "    \n",
    "#     seqs = repeat_truncate_sequence_steps(record['Sequence'], factor)\n",
    "#     for seq in seqs:\n",
    "#         record['Sequence'] = seq\n",
    "#         neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "final_data = pd.concat((neg_data, pos_data, data))\n",
    "print(final_data.shape)\n",
    "\n",
    "##########################\n",
    "\n",
    "_ , _ = print_and_get_stats(final_data, indpe_data)\n",
    "\n",
    "##########################\n",
    "\n",
    "final_data = final_data.drop_duplicates().reset_index(drop=True)\n",
    "print(final_data.shape)\n",
    "\n",
    "print('\\n##### After removing duplicates #####\\n')\n",
    "##########\n",
    "\n",
    "_ , _ = print_and_get_stats(final_data, indpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after balancing\n",
    "\n",
    "# # train_data = train_data_backup\n",
    "\n",
    "# ########\n",
    "\n",
    "# factor = 3\n",
    "\n",
    "# data = train_data\n",
    "# neg_data = data[data['label_original'] == -1].reset_index(drop=True)\n",
    "# pos_data = data[data['label_original'] == 1].reset_index(drop=True)\n",
    "# # neg_data = data.reset_index(drop=True)\n",
    "\n",
    "# not_empty_neg_idxs = np.where(neg_data['has_empty'] != True)[0]\n",
    "# not_empty_pos_idxs = np.where(pos_data['has_empty'] != True)[0]\n",
    "# not_empty_pos_idxs = np.random.permutation(not_empty_pos_idxs)[0:int(not_empty_neg_idxs.shape[0]/factor)]\n",
    "# not_empty_neg_idxs = np.random.permutation(not_empty_neg_idxs)[0:int(not_empty_pos_idxs.shape[0]/factor)]\n",
    "\n",
    "\n",
    "# ##### Getting missing distribution\n",
    "# hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=20)\n",
    "# bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "# cdf = np.cumsum(hist)\n",
    "# cdf = cdf / cdf[-1]\n",
    "\n",
    "# ##### Negative records augmentation\n",
    "# neg_values = np.random.rand(not_empty_neg_idxs.shape[0])\n",
    "# neg_value_bins = np.searchsorted(cdf, neg_values)\n",
    "# neg_random_from_cdf = bin_midpoints[neg_value_bins]\n",
    "# neg_random_from_cdf = np.round(neg_random_from_cdf).astype(int)\n",
    "\n",
    "# for idx, ran_len in zip(list(not_empty_neg_idxs), list(list(neg_random_from_cdf))):\n",
    "#     record = neg_data.iloc[idx].to_dict()\n",
    "#     seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "#     record['Sequence'] = seq\n",
    "#     neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "# ##### Positive records augmentation\n",
    "# pos_values = np.random.rand(not_empty_pos_idxs.shape[0])\n",
    "# pos_value_bins = np.searchsorted(cdf, pos_values)\n",
    "# pos_random_from_cdf = bin_midpoints[pos_value_bins]\n",
    "# pos_random_from_cdf = np.round(pos_random_from_cdf).astype(int)\n",
    "    \n",
    "# for idx, ran_len in zip(list(not_empty_pos_idxs), list(list(pos_random_from_cdf))):\n",
    "#     record = pos_data.iloc[idx].to_dict()\n",
    "#     seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "#     record['Sequence'] = seq\n",
    "#     pos_data = pos_data.append(record, ignore_index=True)\n",
    "    \n",
    "# #     seqs = repeat_truncate_sequence_steps(record['Sequence'], factor)\n",
    "# #     for seq in seqs:\n",
    "# #         record['Sequence'] = seq\n",
    "# #         neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "# final_data = pd.concat((neg_data, pos_data, data))\n",
    "# print(final_data.shape)\n",
    "\n",
    "# ##########################\n",
    "\n",
    "# _ , _ = print_and_get_stats(final_data, indpe_data)\n",
    "\n",
    "# ##########################\n",
    "\n",
    "# final_data = final_data.drop_duplicates().reset_index(drop=True)\n",
    "# print(final_data.shape)\n",
    "\n",
    "# print('\\n##### After removing duplicates #####\\n')\n",
    "# ##########\n",
    "\n",
    "# _ , _ = print_and_get_stats(final_data, indpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty char count per sequence\n",
    "final_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in final_data['Sequence']]\n",
    "# incomplete sequence flag\n",
    "final_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in final_data['Sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 2382\n",
      "Ratio empty/total: 0.5199738048461034\n"
     ]
    }
   ],
   "source": [
    "plt.hist(final_data['Empty_count'][final_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(final_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(final_data['has_empty'])/final_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAFTCAYAAACqHeQ+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAh30lEQVR4nO3df9RldX0f+vcnoGkbNECZUsIPRy3JLeY2hE4JuYmW1jQBTINmrWuhvRF/9I62uJb22h/ErBW9XcsUk6itTYvBQsHWoqZopJE0Emrq9TaYDAb5IVpGM1xmOsAIBiRYW+Bz/3j24PHheebHM895zt4zr9daZ529v/u7z/5wOJwv72d/9z7V3QEAAGCcvmPRBQAAALA6oQ0AAGDEhDYAAIARE9oAAABGTGgDAAAYMaENAABgxIQ2AEatqn6zqi5ZdB0AsCjld9oAWG9V9djM6p9K8s0kTw7rr+/uD25QHTuSnJjkieH4X0jygSRXdvdTB7D/5iR/mORZ3f3E/CoFgNUdvegCADj8dPcxe5eH4PS3u/u3l/erqqM3IAz99e7+7ar67iR/Ock/S/JDSV4z5+MCwLowPRKADVNV51bVzqr6R1V1f5J/XVXHVdVvVNWeqvrasHzKzD6/U1V/e1h+dVV9pqp+eej7h1V1/oEcu7sf6e4bkvyNJJdU1fcPr/myqvqDqnq0qu6rqrfP7Pbp4fmPquqxqvrhqnphVf2nqnqoqr5aVR+sqmPX4e0BgBUJbQBstD+b5Pgkz0uyNUtj0b8e1k9L8o0kv7KP/X8oyZeSnJDkF5NcVVV1oAfv7t9LsjPJi4emP07yqiTHJnlZkr9TVS8ftr1keD62u4/p7t9NUkn+SZLvSfLnk5ya5O0HenwAOFhCGwAb7akkb+vub3b3N7r7oe6+vrsf7+6vJ3lHlqYxrube7n5/dz+Z5NokJ2XpurWD8d+yFBzT3b/T3Xd091PdfXuS6/Z1/O7e3t03DfXvSfLu/dQLAIfENW0AbLQ93f3f965U1Z9K8p4k5yU5bmh+TlUdNQSz5e7fu9Ddjw8n2Y5Zod++nJzk4eH4P5Tk8iTfn+TZSb4zya+ttmNVnZil6+JenOQ5WfoD6NcO8vgAcMCcaQNgoy2/bfFbknxfkh/q7ufmW1MSD3jK48Goqr+UpdD2maHp3yW5Icmp3f3dSd43c+yVbrH8C0P7/zrU+3/Mq1YASIQ2ABbvOVm6ju2Pqur4JG+bx0Gq6rlV9ZNJPpTk33b3HTPHf7i7/3tVnZ3kb87stidL0zlfsKzex5I8UlUnJ/kH86gXAPYS2gBYtH+a5E8m+WqSW5L8x3V+/f9QVV9Pcl+Sn8vSNWizt/v/u0n+8dDn55N8ZO+G7n48S9fY/b9V9UdVdU6S/zvJWUkeSfKJJB9d53oB4Nv4cW0AAIARc6YNAABgxIQ2AACAERPaAAAARkxoAwAAGDGhDY5wVbWjqn7sAPt2Vf25NR5nzfsCrCffe8DUCG3AaNWSd1bVQ8PjnVXlR4yBw1ZV/ZWq+lRVPVJVOxZdDzAOQhswZluTvDzJDyT5C0n+epLXL7IggDn74yRXx4+2AzOENuBpVXV2Vf3u8CPCu6vqV6rq2cu6XVBVX6mqr1bVL1XVd8zs/9qquruqvlZVv1VVzzvEki5J8q7u3tndu5K8K8mrD/E1AZ42tu+97v697v43Sb5yKK8DHF6ENmDWk0n+XpITkvxwkpcm+bvL+rwiyZYkZyW5MMlrk6SqLkzy1iQ/nWRTkv8nyXUrHaSqLhv+B2nFx0zXFyX5/Mz654c2gPUytu89gGcQ2oCndfet3X1Ldz/R3TuS/GqSv7ys2zu7++Hu/v+S/NMkFw/tb0jyT7r77u5+IskvJDlzpb86d/fl3X3sao+ZrsckeWRm/ZEkx7iuDVgvI/zeA3gGoQ14WlV9b1X9RlXdX1WPZul/QE5Y1u2+meV7k3zPsPy8JP9s5q/GDyepJCcfQkmPJXnuzPpzkzzW3X0IrwnwtBF+7wE8g9AGzLoiyReTnN7dz83StJ/lZ7VOnVk+Lcl/G5bvS/L6ZX89/pPd/V+WH6Sq3lpVj632mOl6V5ZuQrLXDwxtAOtlbN97AM8gtAGznpPk0SSPVdX/kuTvrNDnH1TVcVV1apI3Jfnw0P6+JD9bVS9Kkqr67qr631c6SHf/Qncfs9pjpusHkvxfVXVyVX1PkrckuWZd/kkBlozqe6+qvqOq/kSSZy2t1p9Y4cYowBFGaANm/f0kfzPJ15O8P9/6H5NZH09ya5LbknwiyVVJ0t0fS/LOJB8aphjdmeT8Q6znV5P8hyR3DK/3iaENYL2M7XvvJUm+keTGLJ3V+0aSTx7iawITVy4NAQAAGC9n2gAAAEZMaAMAABgxoQ0AAGDEhDYAAIARE9oAAABGTGgDAAAYMaENAABgxIQ2AACAERPaAAAARkxoAwAAGLGjF11Akpxwwgm9efPmRZcBwAa49dZbv9rdmxZdx1QYIwGODPsaH0cR2jZv3pxt27YtugwANkBV3bvoGqbEGAlwZNjX+Gh6JAAAwIgJbQAAACMmtAEAAIyY0AYAADBiQhsAAMCICW0AAAAjJrQBAACMmNAGAAAwYkIbAADAiAltAAAAIya0AQAAjNjRiy6A+dp82ScWXUKSZMflL1t0CQDwtLGMj4kxEtg/Z9oAYI2q6tSq+lRVfaGq7qqqNw3tx1fVTVV1z/B83NBeVfXeqtpeVbdX1VmL/ScAYAqENgBYuyeSvKW7z0hyTpJLq+qMJJclubm7T09y87CeJOcnOX14bE1yxcaXDMDUCG0AsEbdvbu7Pzcsfz3J3UlOTnJhkmuHbtcmefmwfGGSD/SSW5IcW1UnbWzVAEyN0AYA66CqNif5wSSfTXJid+8eNt2f5MRh+eQk983stnNoA4BVCW0AcIiq6pgk1yd5c3c/OrutuztJH+Trba2qbVW1bc+ePetYKQBTJLQBwCGoqmdlKbB9sLs/OjQ/sHfa4/D84NC+K8mpM7ufMrR9m+6+sru3dPeWTZs2za94ACZBaAOANaqqSnJVkru7+90zm25IcsmwfEmSj8+0v2q4i+Q5SR6ZmUYJACvyO20AsHY/kuRnktxRVbcNbW9NcnmSj1TV65Lcm+SVw7Ybk1yQZHuSx5O8ZkOrBWCShDYAWKPu/kySWmXzS1fo30kunWtRABx2TI8EAAAYMWfaAAAWaPNln1h0CUmSHZe/bNElAKtwpg0AAGDEhDYAAIARE9oAAABGTGgDAAAYMTciAQCAkRvLDWsSN61ZBGfaAAAARkxoAwAAGDGhDQAAYMSENgAAgBET2gAAAEbM3SPZEO54BAAAa+NMGwAAwIjtN7RV1alV9amq+kJV3VVVbxraj6+qm6rqnuH5uKG9quq9VbW9qm6vqrPm/Q8BAABwuDqQM21PJHlLd5+R5Jwkl1bVGUkuS3Jzd5+e5OZhPUnOT3L68Nia5Ip1rxoAAOAIsd/Q1t27u/tzw/LXk9yd5OQkFya5duh2bZKXD8sXJvlAL7klybFVddJ6Fw4AAHAkOKhr2qpqc5IfTPLZJCd29+5h0/1JThyWT05y38xuO4c2AAAADtIBh7aqOibJ9Une3N2Pzm7r7k7SB3PgqtpaVduqatuePXsOZlcAAIAjxgGFtqp6VpYC2we7+6ND8wN7pz0Ozw8O7buSnDqz+ylD27fp7iu7e0t3b9m0adNa6wcAADisHcjdIyvJVUnu7u53z2y6Icklw/IlST4+0/6q4S6S5yR5ZGYaJQAAAAfhQH5c+0eS/EySO6rqtqHtrUkuT/KRqnpdknuTvHLYdmOSC5JsT/J4ktesZ8EAAABHkv2Gtu7+TJJaZfNLV+jfSS49xLoAAADIQd49EgD4lqq6uqoerKo7Z9o+XFW3DY8de2epVNXmqvrGzLb3LaxwACblQKZHAgAruybJryT5wN6G7v4be5er6l1JHpnp/+XuPnOjigPg8CC0AcAadfenh98wfYbhRl6vTPJXN7QoAA47pkcCwHy8OMkD3X3PTNvzq+oPquo/V9WLF1UYANPiTBsAzMfFSa6bWd+d5LTufqiq/mKSX6+qF3X3o8t3rKqtSbYmyWmnnbYhxQIwXs60AcA6q6qjk/x0kg/vbevub3b3Q8PyrUm+nOR7V9q/u6/s7i3dvWXTpk0bUTIAIya0AcD6+7EkX+zunXsbqmpTVR01LL8gyelJvrKg+gCYEKENANaoqq5L8rtJvq+qdlbV64ZNF+Xbp0YmyUuS3D78BMC/T/KG7n54w4oFYLJc0wYAa9TdF6/S/uoV2q5Pcv28awLg8ONMGwAAwIgJbQAAACMmtAEAAIyYa9qAUdl82ScWXUKSZMflL1t0CQAASZxpAwAAGDWhDQAAYMSENgAAgBET2gAAAEZMaAMAABgxoQ0AAGDEhDYAAIARE9oAAABGTGgDAAAYMaENAABgxIQ2AACAERPaAAAARkxoAwAAGDGhDQAAYMSENgAAgBET2gAAAEZMaAMAABgxoQ0AAGDEhDYAAIARE9oAYI2q6uqqerCq7pxpe3tV7aqq24bHBTPbfraqtlfVl6rqJxZTNQBTI7QBwNpdk+S8Fdrf091nDo8bk6SqzkhyUZIXDfv8y6o6asMqBWCyhDYAWKPu/nSShw+w+4VJPtTd3+zuP0yyPcnZcysOgMOG0AYA6++NVXX7MH3yuKHt5CT3zfTZObQ9Q1VtraptVbVtz549864VgJET2gBgfV2R5IVJzkyyO8m7DvYFuvvK7t7S3Vs2bdq0zuUBMDVCGwCso+5+oLuf7O6nkrw/35oCuSvJqTNdTxnaAGCfhDYAWEdVddLM6iuS7L2z5A1JLqqq76yq5yc5PcnvbXR9AEzP0YsuAACmqqquS3JukhOqameStyU5t6rOTNJJdiR5fZJ0911V9ZEkX0jyRJJLu/vJBZQNwMQIbQCwRt198QrNV+2j/zuSvGN+FQFwODI9EgAAYMSENgAAgBET2gAAAEZMaAMAABgxoQ0AAGDEhDYAAIARE9oAAABGTGgDAAAYsf2Gtqq6uqoerKo7Z9reXlW7quq24XHBzLafrartVfWlqvqJeRUOAABwJDiQM23XJDlvhfb3dPeZw+PGJKmqM5JclORFwz7/sqqOWq9iAQAAjjT7DW3d/ekkDx/g612Y5EPd/c3u/sMk25OcfQj1AQAAHNEO5Zq2N1bV7cP0yeOGtpOT3DfTZ+fQ9gxVtbWqtlXVtj179hxCGQAAAIevtYa2K5K8MMmZSXYnedfBvkB3X9ndW7p7y6ZNm9ZYBgAAwOFtTaGtux/o7ie7+6kk78+3pkDuSnLqTNdThjYAAADWYE2hrapOmll9RZK9d5a8IclFVfWdVfX8JKcn+b1DKxEAAODIdfT+OlTVdUnOTXJCVe1M8rYk51bVmUk6yY4kr0+S7r6rqj6S5AtJnkhyaXc/OZfKAQAAjgD7DW3dffEKzVfto/87krzjUIoCAABgyaHcPRIAAIA5E9oAAABGTGgDAAAYMaENAABgxIQ2AACAERPaAGCNqurqqnqwqu6cafulqvpiVd1eVR+rqmOH9s1V9Y2qum14vG9hhQMwKUIbAKzdNUnOW9Z2U5Lv7+6/kOS/JvnZmW1f7u4zh8cbNqhGACZOaAOANeruTyd5eFnbJ7v7iWH1liSnbHhhABxWhDYAmJ/XJvnNmfXnV9UfVNV/rqoXL6ooAKbl6EUXAACHo6r6uSRPJPng0LQ7yWnd/VBV/cUkv15VL+ruR1fYd2uSrUly2mmnbVTJAIyUM20AsM6q6tVJfjLJ3+ruTpLu/mZ3PzQs35rky0m+d6X9u/vK7t7S3Vs2bdq0QVUDMFZCGwCso6o6L8k/TPJT3f34TPumqjpqWH5BktOTfGUxVQIwJaZHAsAaVdV1Sc5NckJV7UzytizdLfI7k9xUVUlyy3CnyJck+cdV9T+TPJXkDd398IovDAAzhDYAWKPuvniF5qtW6Xt9kuvnWxEAhyPTIwEAAEZMaAMAABgxoQ0AAGDEhDYAAIARE9oAAABGTGgDAAAYMaENAABgxIQ2AACAERPaAAAARkxoAwAAGDGhDQAAYMSENgAAgBET2gAAAEZMaAMAABgxoQ0AAGDEhDYAAIARE9oAAABGTGgDAAAYMaENAABgxIQ2AACAERPaAAAARkxoAwAAGLGjF10AAACM1ebLPrHoEsCZNgBYq6q6uqoerKo7Z9qOr6qbquqe4fm4ob2q6r1Vtb2qbq+qsxZXOQBTIrQBwNpdk+S8ZW2XJbm5u09PcvOwniTnJzl9eGxNcsUG1QjAxAltALBG3f3pJA8va74wybXD8rVJXj7T/oFeckuSY6vqpA0pFIBJE9oAYH2d2N27h+X7k5w4LJ+c5L6ZfjuHNgDYJ6ENAOakuztJH+x+VbW1qrZV1bY9e/bMoTIApsTdIwFgfT1QVSd19+5h+uODQ/uuJKfO9DtlaHuG7r4yyZVJsmXLloMOfQDzNJY7au64/GWLLmHDONMGAOvrhiSXDMuXJPn4TPurhrtInpPkkZlplACwKmfaAGCNquq6JOcmOaGqdiZ5W5LLk3ykql6X5N4krxy635jkgiTbkzye5DUbXjAAkyS0ccRxSh9YL9198SqbXrpC305y6XwrAuBwZHokAADAiAltAAAAI7bf0FZVV1fVg1V150zb8VV1U1XdMzwfN7RXVb23qrZX1e1VddY8iwcAADjcHciZtmuSnLes7bIkN3f36UluHtaT5Pwkpw+PrUmuWJ8yAQAAjkz7DW3d/ekkDy9rvjDJtcPytUlePtP+gV5yS5Jjh9+oAQAAYA3WevfIE2d+W+b+JCcOyycnuW+m386hze/QAACjuYMvzzSmfzfusAzf7pBvRDLcwrgPdr+q2lpV26pq2549ew61DAAAgMPSWkPbA3unPQ7PDw7tu5KcOtPvlKHtGbr7yu7e0t1bNm3atMYyAAAADm9rDW03JLlkWL4kycdn2l813EXynCSPzEyjBAAA4CDt95q2qrouyblJTqiqnUneluTyJB+pqtcluTfJK4fuNya5IMn2JI8nec0cagaYO9d2AABjsd/Q1t0Xr7LppSv07SSXHmpRAAAALDnkG5EAAAAwP0IbAADAiAltAAAAIya0AQAAjJjQBgAAMGJCGwAAwIgJbQAAACO2399pAwAAGJvNl31i0SU8bcflL5vr6zvTBgAAMGJCGwAAwIiZHgmManoBAADfzpk2AACAEXOmDQDWWVV9X5IPzzS9IMnPJzk2yf+ZZM/Q/tbuvnFjqwNgaoQ2gJEby/TVed8Z63DS3V9KcmaSVNVRSXYl+ViS1yR5T3f/8uKqA2BqTI8EgPl6aZIvd/e9iy4EgGkS2gBgvi5Kct3M+hur6vaqurqqjltUUQBMh9AGAHNSVc9O8lNJfm1ouiLJC7M0dXJ3knetst/WqtpWVdv27NmzUhcAjiBCGwDMz/lJPtfdDyRJdz/Q3U9291NJ3p/k7JV26u4ru3tLd2/ZtGnTBpYLwBgdNjciGcuF+omL9QF42sWZmRpZVSd19+5h9RVJ7lxIVQBMymET2gBgTKrqu5L8tSSvn2n+xao6M0kn2bFsGwCsSGgDgDno7j9O8qeXtf3MgsoBYMJc0wYAADBiQhsAAMCImR4JAIe5Md2sC4CD50wbAADAiAltAAAAIya0AQAAjJjQBgAAMGJCGwAAwIgJbQAAACMmtAEAAIyY32kDAGBU/LYgfDtn2gAAAEZMaAMAABgxoQ0AAGDEhDYAAIARE9oAAABGTGgDAAAYMaENAABgxPxO2xz4bREAAGC9ONMGAAAwYkIbAADAiAltAAAAI+aaNlgQ1z4CAHAghDYAmIOq2pHk60meTPJEd2+pquOTfDjJ5iQ7kryyu7+2qBoBmAbTIwFgfv5Kd5/Z3VuG9cuS3Nzdpye5eVgHgH0S2gBg41yY5Nph+dokL19cKQBMhdAGAPPRST5ZVbdW1dah7cTu3j0s35/kxJV2rKqtVbWtqrbt2bNnI2oFYMRc0wYA8/Gj3b2rqv5Mkpuq6ouzG7u7q6pX2rG7r0xyZZJs2bJlxT4AHDkO6UxbVe2oqjuq6raq2ja0HV9VN1XVPcPzcetTKgBMR3fvGp4fTPKxJGcneaCqTkqS4fnBxVUIwFSsx/RIF1kDwIyq+q6qes7e5SQ/nuTOJDckuWTodkmSjy+mQgCmZB7TIy9Mcu6wfG2S30nyj+ZwHAAYqxOTfKyqkqWx9t9193+sqt9P8pGqel2Se5O8coE1AjARhxra9l5k3Ul+dZiDf0AXWQPA4aq7v5LkB1ZofyjJSze+IgCm7FBD25ovsh7upLU1SU477bRDLAMAAODwdEjXtB3KRdbdfWV3b+nuLZs2bTqUMgAAAA5baw5tLrIGAACYv0OZHukiawAAgDlbc2hzkTUAAMD8rcfvtAEAADAnQhsAAMCICW0AAAAjJrQBAACMmNAGAAAwYkIbAADAiAltAAAAIya0AQAAjJjQBgAAMGJCGwAAwIgJbQAAACMmtAEAAIyY0AYAADBiQhsAAMCICW0AAAAjJrQBAACMmNAGAAAwYkIbAKyzqjq1qj5VVV+oqruq6k1D+9uraldV3TY8Llh0rQCM39GLLgAADkNPJHlLd3+uqp6T5NaqumnY9p7u/uUF1gbAxAhtALDOunt3kt3D8ter6u4kJy+2KgCmyvRIAJijqtqc5AeTfHZoemNV3V5VV1fVcYurDICpENoAYE6q6pgk1yd5c3c/muSKJC9McmaWzsS9a5X9tlbVtqratmfPno0qF4CREtoAYA6q6llZCmwf7O6PJkl3P9DdT3b3U0nen+Tslfbt7iu7e0t3b9m0adPGFQ3AKAltALDOqqqSXJXk7u5+90z7STPdXpHkzo2uDYDpcSMSAFh/P5LkZ5LcUVW3DW1vTXJxVZ2ZpJPsSPL6RRQHwLQIbQCwzrr7M0lqhU03bnQtAEyf6ZEAAAAjJrQBAACMmNAGAAAwYkIbAADAiAltAAAAIya0AQAAjJjQBgAAMGJCGwAAwIgJbQAAACMmtAEAAIyY0AYAADBiQhsAAMCICW0AAAAjJrQBAACMmNAGAAAwYkIbAADAiAltAAAAIya0AQAAjJjQBgAAMGJCGwAAwIgJbQAAACMmtAEAAIyY0AYAADBicwttVXVeVX2pqrZX1WXzOg4ATInxEYCDNZfQVlVHJfkXSc5PckaSi6vqjHkcCwCmwvgIwFrM60zb2Um2d/dXuvt/JPlQkgvndCwAmArjIwAHbV6h7eQk982s7xzaAOBIZnwE4KAdvagDV9XWJFuH1ceq6kuLqmWNTkjy1UUXcZCmWHMyzbqnWHMyzbqnWHMywbrrnetW8/PW4TUOaxMfIyf32R5Mse4p1pxMs+4p1pxMs+4p1rxeY+Sq4+O8QtuuJKfOrJ8ytD2tu69McuWcjj93VbWtu7csuo6DMcWak2nWPcWak2nWPcWak2nWPcWaR2i/42My7TFyqp+TKdY9xZqTadY9xZqTadY9xZqT+dc9r+mRv5/k9Kp6flU9O8lFSW6Y07EAYCqMjwActLmcaevuJ6rqjUl+K8lRSa7u7rvmcSwAmArjIwBrMbdr2rr7xiQ3zuv1R2CK01amWHMyzbqnWHMyzbqnWHMyzbqnWPPoGB9Ha4p1T7HmZJp1T7HmZJp1T7HmZM51V3fP8/UBAAA4BPO6pg0AAIB1ILTtQ1WdWlWfqqovVNVdVfWmFfqcW1WPVNVtw+PnF1Hrspp2VNUdQz3bVtheVfXeqtpeVbdX1VmLqHNZTd838x7eVlWPVtWbl/VZ+HtdVVdX1YNVdedM2/FVdVNV3TM8H7fKvpcMfe6pqks2rupV6/6lqvri8Bn4WFUdu8q++/w8bXDNb6+qXTOfgQtW2fe8qvrS8Bm/bKNqHo69Ut0fnql5R1Xdtsq+i3qvV/yum8Jnm8WY6viYTG+MnMr4ONQxuTFyiuPjcOzJjZFTHB+HY49jjOxuj1UeSU5Kctaw/Jwk/zXJGcv6nJvkNxZd67KadiQ5YR/bL0jym0kqyTlJPrvompfVd1SS+5M8b2zvdZKXJDkryZ0zbb+Y5LJh+bIk71xhv+OTfGV4Pm5YPm7Bdf94kqOH5XeuVPeBfJ42uOa3J/n7B/D5+XKSFyR5dpLPL//vdqPrXrb9XUl+fmTv9YrfdVP4bHss5jHV8XGoa7Jj5JjHx6GOyY2RUxwf91H3qMfIKY6Pw7FHMUY607YP3b27uz83LH89yd1JTl5sVeviwiQf6CW3JDm2qk5adFEzXprky91976ILWa67P53k4WXNFya5dli+NsnLV9j1J5Lc1N0Pd/fXktyU5Lx51bncSnV39ye7+4lh9ZYs/V7UaKzyXh+Is5Ns7+6vdPf/SPKhLP072hD7qruqKskrk1y3UfUciH18143+s81iHMbjYzLuMXK042MyzTFyiuNjMs0xcorjYzKeMVJoO0BVtTnJDyb57Aqbf7iqPl9Vv1lVL9rYylbUST5ZVbdW1dYVtp+c5L6Z9Z0Z12B7UVb/j3Zs73WSnNjdu4fl+5OcuEKfsb/nr83SX5ZXsr/P00Z74zBl5epVpiKM+b1+cZIHuvueVbYv/L1e9l13OHy2mbOJjY/JtMfIqY2PyfS/R6Y0PibTHSNHPz4mix0jhbYDUFXHJLk+yZu7+9Flmz+XpWkKP5Dknyf59Q0ubyU/2t1nJTk/yaVV9ZJFF3SgaunHZn8qya+tsHmM7/W36aVz4ZO6JWtV/VySJ5J8cJUuY/o8XZHkhUnOTLI7S1MppuTi7PuviAt9r/f1XTfFzzbzN8HxMRnXd9oBm/r4mEzve2Ri42My7TFy1ONjsvgxUmjbj6p6Vpb+BX2wuz+6fHt3P9rdjw3LNyZ5VlWdsMFlLq9p1/D8YJKPZelU+KxdSU6dWT9laBuD85N8rrsfWL5hjO/14IG9U2eG5wdX6DPK97yqXp3kJ5P8reEL5xkO4PO0Ybr7ge5+srufSvL+VWoZ63t9dJKfTvLh1fos8r1e5btusp9t5m+K4+NQy1THyCmOj8lEv0emNj4OdUxyjBz7+JiMY4wU2vZhmF97VZK7u/vdq/T5s0O/VNXZWXpPH9q4Kp9Rz3dV1XP2LmfpYto7l3W7Icmrask5SR6ZOb27aKv+pWVs7/WMG5LsvRvQJUk+vkKf30ry41V13DBd4ceHtoWpqvOS/MMkP9Xdj6/S50A+Txtm2XUlr1illt9PcnpVPX/4y/RFWfp3tGg/luSL3b1zpY2LfK/38V03yc828zfF8XGoY8pj5BTHx2SC3yNTHB+HOqY6Ro52fByOOY4xshdwF5apPJL8aJZOdd6e5LbhcUGSNyR5w9DnjUnuytLdd25J8r8tuOYXDLV8fqjr54b22Zoryb/I0t2D7kiyZdHv9VDXd2VpkPnumbZRvddZGjB3J/mfWZqX/LokfzrJzUnuSfLbSY4f+m5J8q9m9n1tku3D4zUjqHt7luZZ7/1sv2/o+z1JbtzX52mBNf+b4TN7e5a+LE9aXvOwfkGW7u705Y2sebW6h/Zr9n6WZ/qO5b1e7btu9J9tj8U89vGZGdV39gp1T3KMzATGx6GOyY2Rq9Q86vFxH3WPeoxcqeah/ZqMdHwcjj+KMbKGFwMAAGCETI8EAAAYMaENAABgxIQ2AACAERPaAAAARkxoAwAAGDGhDQAAYMSENgAAgBET2gAAAEbs/wduTuP5rPfDKgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(final_data['Empty_count'][(final_data['Empty_count'] != 0) & (final_data['label_original'] == -1)])\n",
    "axs[1].hist(final_data['Empty_count'][(final_data['Empty_count'] != 0) & (final_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_original</th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_original  has_empty\n",
       "0              -1       1191\n",
       "1               1       1191"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Final preparations for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = final_data\n",
    "\n",
    "##################################################################################\n",
    "##### Create dictionary of all characters in the NT sequence \n",
    "##################################################################################\n",
    "all_char_set = set({})\n",
    "for val in [set(val) for val in train_data['Sequence']]:\n",
    "    all_char_set = all_char_set.union(val)\n",
    "all_char_list = list(all_char_set)\n",
    "all_char_list.sort()\n",
    "all_char_dict = {}\n",
    "for i in range(len(all_char_list)):\n",
    "    all_char_dict[all_char_list[i]] = i\n",
    "    \n",
    "##################################################################################\n",
    "##### Create OHE of sequence\n",
    "##################################################################################\n",
    "train_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                        for val in train_data[\"Sequence\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Fix the labels\n",
    "##################################################################################\n",
    "train_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                 for val in train_data[\"label_original\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Extract features and labels, create folds\n",
    "##################################################################################\n",
    "\n",
    "train_features = np.array(list(train_data['OHE_Sequence']))\n",
    "train_labels = np.array(list(train_data['label']))\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], 1))\n",
    "\n",
    "input_seq_shape = train_features[0].shape\n",
    "\n",
    "folds = build_kfold(train_features, train_labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "\n",
    "## Write the k-fold dataset to file\n",
    "foldPath = os.path.join(outPath, expName, \"{}fold\".format(n_fold))\n",
    "if(not os.path.isdir(foldPath)):\n",
    "    os.makedirs(foldPath)\n",
    "pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prep for Independent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Create OHE of sequence\n",
    "##################################################################################\n",
    "indpe_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                        for val in indpe_data[\"Sequence\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Fix the labels\n",
    "##################################################################################\n",
    "indpe_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                 for val in indpe_data[\"label_original\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Extract features and labels, create folds\n",
    "##################################################################################\n",
    "\n",
    "indpe_features = np.array(list(indpe_data['OHE_Sequence']))\n",
    "indpe_labels = np.array(list(indpe_data['label']))\n",
    "indpe_labels = indpe_labels.reshape((indpe_labels.shape[0], 1))\n",
    "\n",
    "input_seq_shape = indpe_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 21)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/Test model on Fold #0.\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3736\n",
      "Epoch 1: val_loss improved from inf to 1.27744, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 5s 19ms/step - loss: 1.3736 - val_loss: 1.2774\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2160\n",
      "Epoch 2: val_loss improved from 1.27744 to 1.13120, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 1.2160 - val_loss: 1.1312\n",
      "Epoch 3/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 1.0912\n",
      "Epoch 3: val_loss improved from 1.13120 to 0.99187, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.0875 - val_loss: 0.9919\n",
      "Epoch 4/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.9862\n",
      "Epoch 4: val_loss improved from 0.99187 to 0.91057, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9853 - val_loss: 0.9106\n",
      "Epoch 5/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.9137\n",
      "Epoch 5: val_loss improved from 0.91057 to 0.85825, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9122 - val_loss: 0.8583\n",
      "Epoch 6/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.8628\n",
      "Epoch 6: val_loss improved from 0.85825 to 0.82679, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.8621 - val_loss: 0.8268\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.8185\n",
      "Epoch 7: val_loss improved from 0.82679 to 0.77321, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.8185 - val_loss: 0.7732\n",
      "Epoch 8/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.7986\n",
      "Epoch 8: val_loss improved from 0.77321 to 0.75027, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7980 - val_loss: 0.7503\n",
      "Epoch 9/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.7554\n",
      "Epoch 9: val_loss did not improve from 0.75027\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.7529 - val_loss: 0.7627\n",
      "Epoch 10/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.7357\n",
      "Epoch 10: val_loss improved from 0.75027 to 0.69872, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7334 - val_loss: 0.6987\n",
      "Epoch 11/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.7184\n",
      "Epoch 11: val_loss improved from 0.69872 to 0.68077, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7218 - val_loss: 0.6808\n",
      "Epoch 12/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.6955\n",
      "Epoch 12: val_loss improved from 0.68077 to 0.66568, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6962 - val_loss: 0.6657\n",
      "Epoch 13/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.6813\n",
      "Epoch 13: val_loss improved from 0.66568 to 0.65463, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6811 - val_loss: 0.6546\n",
      "Epoch 14/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.6648\n",
      "Epoch 14: val_loss improved from 0.65463 to 0.63640, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6624 - val_loss: 0.6364\n",
      "Epoch 15/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.6481\n",
      "Epoch 15: val_loss improved from 0.63640 to 0.62526, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6494 - val_loss: 0.6253\n",
      "Epoch 16/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.6296\n",
      "Epoch 16: val_loss improved from 0.62526 to 0.61328, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6285 - val_loss: 0.6133\n",
      "Epoch 17/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.6260\n",
      "Epoch 17: val_loss improved from 0.61328 to 0.60276, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6262 - val_loss: 0.6028\n",
      "Epoch 18/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.6179\n",
      "Epoch 18: val_loss improved from 0.60276 to 0.59651, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6157 - val_loss: 0.5965\n",
      "Epoch 19/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.6161\n",
      "Epoch 19: val_loss improved from 0.59651 to 0.59134, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6157 - val_loss: 0.5913\n",
      "Epoch 20/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5907\n",
      "Epoch 20: val_loss improved from 0.59134 to 0.58577, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5911 - val_loss: 0.5858\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5793\n",
      "Epoch 21: val_loss improved from 0.58577 to 0.56304, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5793 - val_loss: 0.5630\n",
      "Epoch 22/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5751\n",
      "Epoch 22: val_loss improved from 0.56304 to 0.56211, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5743 - val_loss: 0.5621\n",
      "Epoch 23/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5631\n",
      "Epoch 23: val_loss improved from 0.56211 to 0.55325, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5626 - val_loss: 0.5532\n",
      "Epoch 24/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5470\n",
      "Epoch 24: val_loss did not improve from 0.55325\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5476 - val_loss: 0.5543\n",
      "Epoch 25/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5443\n",
      "Epoch 25: val_loss did not improve from 0.55325\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5453 - val_loss: 0.5537\n",
      "Epoch 26/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/58 [============================>.] - ETA: 0s - loss: 0.5421\n",
      "Epoch 26: val_loss improved from 0.55325 to 0.53720, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5426 - val_loss: 0.5372\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5279\n",
      "Epoch 27: val_loss improved from 0.53720 to 0.52649, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5279 - val_loss: 0.5265\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5221\n",
      "Epoch 28: val_loss improved from 0.52649 to 0.51993, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5221 - val_loss: 0.5199\n",
      "Epoch 29/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5107\n",
      "Epoch 29: val_loss improved from 0.51993 to 0.51340, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5107 - val_loss: 0.5134\n",
      "Epoch 30/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5128\n",
      "Epoch 30: val_loss did not improve from 0.51340\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5139 - val_loss: 0.5172\n",
      "Epoch 31/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4982\n",
      "Epoch 31: val_loss improved from 0.51340 to 0.51069, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4974 - val_loss: 0.5107\n",
      "Epoch 32/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4759\n",
      "Epoch 32: val_loss improved from 0.51069 to 0.50178, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4759 - val_loss: 0.5018\n",
      "Epoch 33/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4980\n",
      "Epoch 33: val_loss improved from 0.50178 to 0.49744, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4944 - val_loss: 0.4974\n",
      "Epoch 34/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.4708\n",
      "Epoch 34: val_loss improved from 0.49744 to 0.49141, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4722 - val_loss: 0.4914\n",
      "Epoch 35/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4834\n",
      "Epoch 35: val_loss did not improve from 0.49141\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4835 - val_loss: 0.5145\n",
      "Epoch 36/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4684\n",
      "Epoch 36: val_loss did not improve from 0.49141\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4691 - val_loss: 0.4931\n",
      "Epoch 37/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4482\n",
      "Epoch 37: val_loss improved from 0.49141 to 0.48053, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4525 - val_loss: 0.4805\n",
      "Epoch 38/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4517\n",
      "Epoch 38: val_loss improved from 0.48053 to 0.47575, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4508 - val_loss: 0.4757\n",
      "Epoch 39/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4519\n",
      "Epoch 39: val_loss did not improve from 0.47575\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4510 - val_loss: 0.4758\n",
      "Epoch 40/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.4536\n",
      "Epoch 40: val_loss improved from 0.47575 to 0.47118, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4508 - val_loss: 0.4712\n",
      "Epoch 41/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4374\n",
      "Epoch 41: val_loss did not improve from 0.47118\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4403 - val_loss: 0.4845\n",
      "Epoch 42/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4306\n",
      "Epoch 42: val_loss improved from 0.47118 to 0.46243, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4311 - val_loss: 0.4624\n",
      "Epoch 43/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4306\n",
      "Epoch 43: val_loss did not improve from 0.46243\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4310 - val_loss: 0.4637\n",
      "Epoch 44/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4317\n",
      "Epoch 44: val_loss improved from 0.46243 to 0.45638, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4313 - val_loss: 0.4564\n",
      "Epoch 45/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4179\n",
      "Epoch 45: val_loss did not improve from 0.45638\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4164 - val_loss: 0.5017\n",
      "Epoch 46/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4192\n",
      "Epoch 46: val_loss improved from 0.45638 to 0.45606, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4183 - val_loss: 0.4561\n",
      "Epoch 47/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.4005\n",
      "Epoch 47: val_loss improved from 0.45606 to 0.44732, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4036 - val_loss: 0.4473\n",
      "Epoch 48/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4065\n",
      "Epoch 48: val_loss improved from 0.44732 to 0.44471, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4063 - val_loss: 0.4447\n",
      "Epoch 49/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.4079\n",
      "Epoch 49: val_loss did not improve from 0.44471\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4081 - val_loss: 0.4475\n",
      "Epoch 50/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4022\n",
      "Epoch 50: val_loss improved from 0.44471 to 0.43803, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4014 - val_loss: 0.4380\n",
      "Epoch 51/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3932\n",
      "Epoch 51: val_loss improved from 0.43803 to 0.43538, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3917 - val_loss: 0.4354\n",
      "Epoch 52/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3789\n",
      "Epoch 52: val_loss improved from 0.43538 to 0.42446, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.4245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3942\n",
      "Epoch 53: val_loss did not improve from 0.42446\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3936 - val_loss: 0.4297\n",
      "Epoch 54/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3817\n",
      "Epoch 54: val_loss did not improve from 0.42446\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3815 - val_loss: 0.4253\n",
      "Epoch 55/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3875\n",
      "Epoch 55: val_loss improved from 0.42446 to 0.42373, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3883 - val_loss: 0.4237\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3838\n",
      "Epoch 56: val_loss did not improve from 0.42373\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3838 - val_loss: 0.4374\n",
      "Epoch 57/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.3764\n",
      "Epoch 57: val_loss did not improve from 0.42373\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3811 - val_loss: 0.4325\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3826\n",
      "Epoch 58: val_loss improved from 0.42373 to 0.42206, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3826 - val_loss: 0.4221\n",
      "Epoch 59/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3589\n",
      "Epoch 59: val_loss did not improve from 0.42206\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3600 - val_loss: 0.4248\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3601\n",
      "Epoch 60: val_loss did not improve from 0.42206\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3601 - val_loss: 0.4253\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3675\n",
      "Epoch 61: val_loss did not improve from 0.42206\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3675 - val_loss: 0.4264\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3520\n",
      "Epoch 62: val_loss improved from 0.42206 to 0.42081, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3520 - val_loss: 0.4208\n",
      "Epoch 63/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3545\n",
      "Epoch 63: val_loss improved from 0.42081 to 0.40912, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3538 - val_loss: 0.4091\n",
      "Epoch 64/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3561\n",
      "Epoch 64: val_loss did not improve from 0.40912\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3563 - val_loss: 0.4111\n",
      "Epoch 65/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3622\n",
      "Epoch 65: val_loss improved from 0.40912 to 0.40847, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3615 - val_loss: 0.4085\n",
      "Epoch 66/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3421\n",
      "Epoch 66: val_loss did not improve from 0.40847\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3418 - val_loss: 0.4098\n",
      "Epoch 67/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3624\n",
      "Epoch 67: val_loss did not improve from 0.40847\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3620 - val_loss: 0.4521\n",
      "Epoch 68/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3548\n",
      "Epoch 68: val_loss did not improve from 0.40847\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3524 - val_loss: 0.4125\n",
      "Epoch 69/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.3467\n",
      "Epoch 69: val_loss did not improve from 0.40847\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3475 - val_loss: 0.4141\n",
      "Epoch 70/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3401\n",
      "Epoch 70: val_loss improved from 0.40847 to 0.40330, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3396 - val_loss: 0.4033\n",
      "Epoch 71/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3247\n",
      "Epoch 71: val_loss improved from 0.40330 to 0.40239, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3245 - val_loss: 0.4024\n",
      "Epoch 72/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3348\n",
      "Epoch 72: val_loss improved from 0.40239 to 0.39167, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3353 - val_loss: 0.3917\n",
      "Epoch 73/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3431\n",
      "Epoch 73: val_loss did not improve from 0.39167\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3425 - val_loss: 0.3960\n",
      "Epoch 74/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3294\n",
      "Epoch 74: val_loss did not improve from 0.39167\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3300 - val_loss: 0.3999\n",
      "Epoch 75/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3292\n",
      "Epoch 75: val_loss improved from 0.39167 to 0.38967, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3316 - val_loss: 0.3897\n",
      "Epoch 76/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3316\n",
      "Epoch 76: val_loss did not improve from 0.38967\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3321 - val_loss: 0.3911\n",
      "Epoch 77/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3404\n",
      "Epoch 77: val_loss did not improve from 0.38967\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3402 - val_loss: 0.3934\n",
      "Epoch 78/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3317\n",
      "Epoch 78: val_loss improved from 0.38967 to 0.38956, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3315 - val_loss: 0.3896\n",
      "Epoch 79/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3173\n",
      "Epoch 79: val_loss did not improve from 0.38956\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3215 - val_loss: 0.3991\n",
      "Epoch 80/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3332\n",
      "Epoch 80: val_loss did not improve from 0.38956\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3305 - val_loss: 0.3951\n",
      "Epoch 81/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3190\n",
      "Epoch 81: val_loss improved from 0.38956 to 0.38656, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.3189 - val_loss: 0.3866\n",
      "Epoch 82/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3223\n",
      "Epoch 82: val_loss improved from 0.38656 to 0.38637, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3212 - val_loss: 0.3864\n",
      "Epoch 83/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.3243\n",
      "Epoch 83: val_loss did not improve from 0.38637\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3243 - val_loss: 0.3899\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3114\n",
      "Epoch 84: val_loss improved from 0.38637 to 0.37806, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3127 - val_loss: 0.3781\n",
      "Epoch 85/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3218\n",
      "Epoch 85: val_loss did not improve from 0.37806\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3217 - val_loss: 0.3921\n",
      "Epoch 86/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3219\n",
      "Epoch 86: val_loss did not improve from 0.37806\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3252 - val_loss: 0.3969\n",
      "Epoch 87/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3070\n",
      "Epoch 87: val_loss did not improve from 0.37806\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3040 - val_loss: 0.3832\n",
      "Epoch 88/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3208\n",
      "Epoch 88: val_loss improved from 0.37806 to 0.37446, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3211 - val_loss: 0.3745\n",
      "Epoch 89/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3085\n",
      "Epoch 89: val_loss did not improve from 0.37446\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3083 - val_loss: 0.3809\n",
      "Epoch 90/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3186\n",
      "Epoch 90: val_loss did not improve from 0.37446\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3199 - val_loss: 0.3762\n",
      "Epoch 91/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.2957\n",
      "Epoch 91: val_loss improved from 0.37446 to 0.37383, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2949 - val_loss: 0.3738\n",
      "Epoch 92/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3010\n",
      "Epoch 92: val_loss did not improve from 0.37383\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3024 - val_loss: 0.3836\n",
      "Epoch 93/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3123\n",
      "Epoch 93: val_loss did not improve from 0.37383\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3111 - val_loss: 0.4026\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3069\n",
      "Epoch 94: val_loss did not improve from 0.37383\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3069 - val_loss: 0.4174\n",
      "Epoch 95/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3041\n",
      "Epoch 95: val_loss improved from 0.37383 to 0.36768, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3045 - val_loss: 0.3677\n",
      "Epoch 96/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3057\n",
      "Epoch 96: val_loss did not improve from 0.36768\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3055 - val_loss: 0.3781\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2897\n",
      "Epoch 97: val_loss improved from 0.36768 to 0.36625, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold0.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2897 - val_loss: 0.3663\n",
      "Epoch 98/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3167\n",
      "Epoch 98: val_loss did not improve from 0.36625\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3168 - val_loss: 0.3691\n",
      "Epoch 99/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.2968\n",
      "Epoch 99: val_loss did not improve from 0.36625\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2964 - val_loss: 0.3740\n",
      "Epoch 100/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3025\n",
      "Epoch 100: val_loss did not improve from 0.36625\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3038 - val_loss: 0.3697\n",
      "\n",
      "Train/Test model on Fold #1.\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.3721\n",
      "Epoch 1: val_loss improved from inf to 1.28545, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 3s 19ms/step - loss: 1.3721 - val_loss: 1.2854\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2235\n",
      "Epoch 2: val_loss improved from 1.28545 to 1.14146, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.2235 - val_loss: 1.1415\n",
      "Epoch 3/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 1.0881\n",
      "Epoch 3: val_loss improved from 1.14146 to 1.00622, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.0879 - val_loss: 1.0062\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.9795\n",
      "Epoch 4: val_loss improved from 1.00622 to 0.92963, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.9795 - val_loss: 0.9296\n",
      "Epoch 5/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.9096\n",
      "Epoch 5: val_loss improved from 0.92963 to 0.87764, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9101 - val_loss: 0.8776\n",
      "Epoch 6/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.8572\n",
      "Epoch 6: val_loss improved from 0.87764 to 0.83280, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8565 - val_loss: 0.8328\n",
      "Epoch 7/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.8240\n",
      "Epoch 7: val_loss improved from 0.83280 to 0.80046, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8223 - val_loss: 0.8005\n",
      "Epoch 8/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.7794\n",
      "Epoch 8: val_loss improved from 0.80046 to 0.76827, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7801 - val_loss: 0.7683\n",
      "Epoch 9/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.7530\n",
      "Epoch 9: val_loss improved from 0.76827 to 0.74749, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7536 - val_loss: 0.7475\n",
      "Epoch 10/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.7281\n",
      "Epoch 10: val_loss improved from 0.74749 to 0.72784, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7273 - val_loss: 0.7278\n",
      "Epoch 11/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.7118\n",
      "Epoch 11: val_loss improved from 0.72784 to 0.70226, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7118 - val_loss: 0.7023\n",
      "Epoch 12/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6824\n",
      "Epoch 12: val_loss improved from 0.70226 to 0.68518, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6817 - val_loss: 0.6852\n",
      "Epoch 13/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.6694\n",
      "Epoch 13: val_loss improved from 0.68518 to 0.66893, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6718 - val_loss: 0.6689\n",
      "Epoch 14/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6578\n",
      "Epoch 14: val_loss improved from 0.66893 to 0.65928, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6575 - val_loss: 0.6593\n",
      "Epoch 15/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.6326\n",
      "Epoch 15: val_loss improved from 0.65928 to 0.64412, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6324 - val_loss: 0.6441\n",
      "Epoch 16/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.6237\n",
      "Epoch 16: val_loss improved from 0.64412 to 0.63720, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6251 - val_loss: 0.6372\n",
      "Epoch 17/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.6184\n",
      "Epoch 17: val_loss improved from 0.63720 to 0.62760, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6148 - val_loss: 0.6276\n",
      "Epoch 18/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.6067\n",
      "Epoch 18: val_loss improved from 0.62760 to 0.61202, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6057 - val_loss: 0.6120\n",
      "Epoch 19/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5915\n",
      "Epoch 19: val_loss improved from 0.61202 to 0.60835, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5915 - val_loss: 0.6083\n",
      "Epoch 20/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.5784\n",
      "Epoch 20: val_loss improved from 0.60835 to 0.59884, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5760 - val_loss: 0.5988\n",
      "Epoch 21/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.5591\n",
      "Epoch 21: val_loss improved from 0.59884 to 0.58015, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5600 - val_loss: 0.5802\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5632\n",
      "Epoch 22: val_loss improved from 0.58015 to 0.57712, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.5632 - val_loss: 0.5771\n",
      "Epoch 23/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5472\n",
      "Epoch 23: val_loss improved from 0.57712 to 0.57702, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5486 - val_loss: 0.5770\n",
      "Epoch 24/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.5304\n",
      "Epoch 24: val_loss improved from 0.57702 to 0.56864, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5348 - val_loss: 0.5686\n",
      "Epoch 25/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5336\n",
      "Epoch 25: val_loss improved from 0.56864 to 0.55585, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5365 - val_loss: 0.5558\n",
      "Epoch 26/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5198\n",
      "Epoch 26: val_loss did not improve from 0.55585\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5197 - val_loss: 0.5606\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5210\n",
      "Epoch 27: val_loss improved from 0.55585 to 0.54509, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5210 - val_loss: 0.5451\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5088\n",
      "Epoch 28: val_loss improved from 0.54509 to 0.54330, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5088 - val_loss: 0.5433\n",
      "Epoch 29/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5052\n",
      "Epoch 29: val_loss improved from 0.54330 to 0.53305, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5047 - val_loss: 0.5331\n",
      "Epoch 30/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5009\n",
      "Epoch 30: val_loss improved from 0.53305 to 0.52903, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5012 - val_loss: 0.5290\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4815\n",
      "Epoch 31: val_loss improved from 0.52903 to 0.52025, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4815 - val_loss: 0.5202\n",
      "Epoch 32/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4828\n",
      "Epoch 32: val_loss improved from 0.52025 to 0.51929, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4833 - val_loss: 0.5193\n",
      "Epoch 33/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4779\n",
      "Epoch 33: val_loss improved from 0.51929 to 0.50894, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4791 - val_loss: 0.5089\n",
      "Epoch 34/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4722\n",
      "Epoch 34: val_loss improved from 0.50894 to 0.50248, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4726 - val_loss: 0.5025\n",
      "Epoch 35/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4593\n",
      "Epoch 35: val_loss improved from 0.50248 to 0.49789, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4613 - val_loss: 0.4979\n",
      "Epoch 36/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4555\n",
      "Epoch 36: val_loss improved from 0.49789 to 0.49522, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4540 - val_loss: 0.4952\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4545\n",
      "Epoch 37: val_loss did not improve from 0.49522\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4535 - val_loss: 0.5054\n",
      "Epoch 38/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4421\n",
      "Epoch 38: val_loss improved from 0.49522 to 0.48790, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4411 - val_loss: 0.4879\n",
      "Epoch 39/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4343\n",
      "Epoch 39: val_loss did not improve from 0.48790\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4342 - val_loss: 0.5021\n",
      "Epoch 40/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4379\n",
      "Epoch 40: val_loss did not improve from 0.48790\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4368 - val_loss: 0.5002\n",
      "Epoch 41/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4525\n",
      "Epoch 41: val_loss improved from 0.48790 to 0.48751, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4504 - val_loss: 0.4875\n",
      "Epoch 42/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4268\n",
      "Epoch 42: val_loss improved from 0.48751 to 0.48054, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4265 - val_loss: 0.4805\n",
      "Epoch 43/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4144\n",
      "Epoch 43: val_loss improved from 0.48054 to 0.48007, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4155 - val_loss: 0.4801\n",
      "Epoch 44/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4124\n",
      "Epoch 44: val_loss improved from 0.48007 to 0.47209, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4138 - val_loss: 0.4721\n",
      "Epoch 45/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4083\n",
      "Epoch 45: val_loss improved from 0.47209 to 0.45921, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4090 - val_loss: 0.4592\n",
      "Epoch 46/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4053\n",
      "Epoch 46: val_loss did not improve from 0.45921\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4053 - val_loss: 0.4626\n",
      "Epoch 47/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3989\n",
      "Epoch 47: val_loss did not improve from 0.45921\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3995 - val_loss: 0.4744\n",
      "Epoch 48/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3991\n",
      "Epoch 48: val_loss improved from 0.45921 to 0.45402, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4019 - val_loss: 0.4540\n",
      "Epoch 49/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3982\n",
      "Epoch 49: val_loss improved from 0.45402 to 0.45143, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4012 - val_loss: 0.4514\n",
      "Epoch 50/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.3997\n",
      "Epoch 50: val_loss did not improve from 0.45143\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3959 - val_loss: 0.4524\n",
      "Epoch 51/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3893\n",
      "Epoch 51: val_loss did not improve from 0.45143\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3873 - val_loss: 0.4566\n",
      "Epoch 52/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3865\n",
      "Epoch 52: val_loss improved from 0.45143 to 0.44059, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3880 - val_loss: 0.4406\n",
      "Epoch 53/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3709\n",
      "Epoch 53: val_loss did not improve from 0.44059\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3728 - val_loss: 0.4876\n",
      "Epoch 54/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3860\n",
      "Epoch 54: val_loss did not improve from 0.44059\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3834 - val_loss: 0.4561\n",
      "Epoch 55/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3820\n",
      "Epoch 55: val_loss improved from 0.44059 to 0.43159, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3819 - val_loss: 0.4316\n",
      "Epoch 56/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3817\n",
      "Epoch 56: val_loss did not improve from 0.43159\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3812 - val_loss: 0.4352\n",
      "Epoch 57/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3767\n",
      "Epoch 57: val_loss improved from 0.43159 to 0.42809, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3773 - val_loss: 0.4281\n",
      "Epoch 58/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3614\n",
      "Epoch 58: val_loss improved from 0.42809 to 0.42639, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3611 - val_loss: 0.4264\n",
      "Epoch 59/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3797\n",
      "Epoch 59: val_loss improved from 0.42639 to 0.41964, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3798 - val_loss: 0.4196\n",
      "Epoch 60/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3687\n",
      "Epoch 60: val_loss did not improve from 0.41964\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3684 - val_loss: 0.4302\n",
      "Epoch 61/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3669\n",
      "Epoch 61: val_loss did not improve from 0.41964\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3682 - val_loss: 0.4396\n",
      "Epoch 62/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3435\n",
      "Epoch 62: val_loss improved from 0.41964 to 0.41565, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3435 - val_loss: 0.4156\n",
      "Epoch 63/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3593\n",
      "Epoch 63: val_loss did not improve from 0.41565\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3590 - val_loss: 0.4160\n",
      "Epoch 64/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3589\n",
      "Epoch 64: val_loss did not improve from 0.41565\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3565 - val_loss: 0.4230\n",
      "Epoch 65/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3514\n",
      "Epoch 65: val_loss did not improve from 0.41565\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3512 - val_loss: 0.4261\n",
      "Epoch 66/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3629\n",
      "Epoch 66: val_loss did not improve from 0.41565\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3616 - val_loss: 0.4166\n",
      "Epoch 67/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3579\n",
      "Epoch 67: val_loss improved from 0.41565 to 0.41208, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3571 - val_loss: 0.4121\n",
      "Epoch 68/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3393\n",
      "Epoch 68: val_loss improved from 0.41208 to 0.40596, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3407 - val_loss: 0.4060\n",
      "Epoch 69/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3524\n",
      "Epoch 69: val_loss did not improve from 0.40596\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3501 - val_loss: 0.4119\n",
      "Epoch 70/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3440\n",
      "Epoch 70: val_loss improved from 0.40596 to 0.39706, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3470 - val_loss: 0.3971\n",
      "Epoch 71/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3311\n",
      "Epoch 71: val_loss did not improve from 0.39706\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3314 - val_loss: 0.3981\n",
      "Epoch 72/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3363\n",
      "Epoch 72: val_loss did not improve from 0.39706\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3378 - val_loss: 0.3998\n",
      "Epoch 73/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3418\n",
      "Epoch 73: val_loss did not improve from 0.39706\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3418 - val_loss: 0.4020\n",
      "Epoch 74/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3286\n",
      "Epoch 74: val_loss did not improve from 0.39706\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3291 - val_loss: 0.3993\n",
      "Epoch 75/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3262\n",
      "Epoch 75: val_loss improved from 0.39706 to 0.39338, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3251 - val_loss: 0.3934\n",
      "Epoch 76/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3230\n",
      "Epoch 76: val_loss did not improve from 0.39338\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3229 - val_loss: 0.3947\n",
      "Epoch 77/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3347\n",
      "Epoch 77: val_loss did not improve from 0.39338\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3328 - val_loss: 0.3992\n",
      "Epoch 78/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3266\n",
      "Epoch 78: val_loss did not improve from 0.39338\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3267 - val_loss: 0.3975\n",
      "Epoch 79/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3299\n",
      "Epoch 79: val_loss improved from 0.39338 to 0.38819, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3328 - val_loss: 0.3882\n",
      "Epoch 80/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3211\n",
      "Epoch 80: val_loss did not improve from 0.38819\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3206 - val_loss: 0.4234\n",
      "Epoch 81/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3154\n",
      "Epoch 81: val_loss improved from 0.38819 to 0.38323, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3144 - val_loss: 0.3832\n",
      "Epoch 82/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3170\n",
      "Epoch 82: val_loss did not improve from 0.38323\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3184 - val_loss: 0.3885\n",
      "Epoch 83/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3236\n",
      "Epoch 83: val_loss did not improve from 0.38323\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3224 - val_loss: 0.3889\n",
      "Epoch 84/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3014\n",
      "Epoch 84: val_loss did not improve from 0.38323\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3005 - val_loss: 0.3939\n",
      "Epoch 85/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3238\n",
      "Epoch 85: val_loss did not improve from 0.38323\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3243 - val_loss: 0.3849\n",
      "Epoch 86/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.2968\n",
      "Epoch 86: val_loss improved from 0.38323 to 0.37969, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2966 - val_loss: 0.3797\n",
      "Epoch 87/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3221\n",
      "Epoch 87: val_loss did not improve from 0.37969\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3224 - val_loss: 0.3823\n",
      "Epoch 88/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3094\n",
      "Epoch 88: val_loss did not improve from 0.37969\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3100 - val_loss: 0.4040\n",
      "Epoch 89/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.2998\n",
      "Epoch 89: val_loss did not improve from 0.37969\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2996 - val_loss: 0.4019\n",
      "Epoch 90/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3064\n",
      "Epoch 90: val_loss improved from 0.37969 to 0.37512, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3039 - val_loss: 0.3751\n",
      "Epoch 91/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3099\n",
      "Epoch 91: val_loss improved from 0.37512 to 0.37100, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3139 - val_loss: 0.3710\n",
      "Epoch 92/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3038\n",
      "Epoch 92: val_loss did not improve from 0.37100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3035 - val_loss: 0.3731\n",
      "Epoch 93/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3039\n",
      "Epoch 93: val_loss improved from 0.37100 to 0.36955, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3036 - val_loss: 0.3696\n",
      "Epoch 94/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3002\n",
      "Epoch 94: val_loss did not improve from 0.36955\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2996 - val_loss: 0.3708\n",
      "Epoch 95/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.2917\n",
      "Epoch 95: val_loss did not improve from 0.36955\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2912 - val_loss: 0.3825\n",
      "Epoch 96/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3021\n",
      "Epoch 96: val_loss did not improve from 0.36955\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3017 - val_loss: 0.3714\n",
      "Epoch 97/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3076\n",
      "Epoch 97: val_loss did not improve from 0.36955\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3064 - val_loss: 0.3811\n",
      "Epoch 98/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3002\n",
      "Epoch 98: val_loss did not improve from 0.36955\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2997 - val_loss: 0.3771\n",
      "Epoch 99/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.2977\n",
      "Epoch 99: val_loss improved from 0.36955 to 0.36840, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2997 - val_loss: 0.3684\n",
      "Epoch 100/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.2890\n",
      "Epoch 100: val_loss improved from 0.36840 to 0.36286, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold1.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2902 - val_loss: 0.3629\n",
      "\n",
      "Train/Test model on Fold #2.\n",
      "Epoch 1/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 1.3812\n",
      "Epoch 1: val_loss improved from inf to 1.29077, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 3s 18ms/step - loss: 1.3765 - val_loss: 1.2908\n",
      "Epoch 2/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 1.2340\n",
      "Epoch 2: val_loss improved from 1.29077 to 1.15051, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.2293 - val_loss: 1.1505\n",
      "Epoch 3/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 1.0967\n",
      "Epoch 3: val_loss improved from 1.15051 to 1.00445, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.0943 - val_loss: 1.0044\n",
      "Epoch 4/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.9807\n",
      "Epoch 4: val_loss improved from 1.00445 to 0.94662, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9812 - val_loss: 0.9466\n",
      "Epoch 5/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.9180\n",
      "Epoch 5: val_loss improved from 0.94662 to 0.87808, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9184 - val_loss: 0.8781\n",
      "Epoch 6/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.8576\n",
      "Epoch 6: val_loss improved from 0.87808 to 0.83777, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8577 - val_loss: 0.8378\n",
      "Epoch 7/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.8221\n",
      "Epoch 7: val_loss improved from 0.83777 to 0.80921, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8222 - val_loss: 0.8092\n",
      "Epoch 8/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.7871\n",
      "Epoch 8: val_loss improved from 0.80921 to 0.77859, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7881 - val_loss: 0.7786\n",
      "Epoch 9/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.7599\n",
      "Epoch 9: val_loss improved from 0.77859 to 0.75090, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7595 - val_loss: 0.7509\n",
      "Epoch 10/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.7410\n",
      "Epoch 10: val_loss improved from 0.75090 to 0.73001, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7417 - val_loss: 0.7300\n",
      "Epoch 11/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.7056\n",
      "Epoch 11: val_loss improved from 0.73001 to 0.71544, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7072 - val_loss: 0.7154\n",
      "Epoch 12/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.6860\n",
      "Epoch 12: val_loss improved from 0.71544 to 0.69835, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6866 - val_loss: 0.6984\n",
      "Epoch 13/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6655\n",
      "Epoch 13: val_loss improved from 0.69835 to 0.68718, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6650 - val_loss: 0.6872\n",
      "Epoch 14/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.6505\n",
      "Epoch 14: val_loss improved from 0.68718 to 0.66783, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6504 - val_loss: 0.6678\n",
      "Epoch 15/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6423\n",
      "Epoch 15: val_loss improved from 0.66783 to 0.65745, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6404 - val_loss: 0.6574\n",
      "Epoch 16/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6210\n",
      "Epoch 16: val_loss improved from 0.65745 to 0.64498, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6231 - val_loss: 0.6450\n",
      "Epoch 17/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5977\n",
      "Epoch 17: val_loss improved from 0.64498 to 0.63293, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5968 - val_loss: 0.6329\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5924\n",
      "Epoch 18: val_loss improved from 0.63293 to 0.62888, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5924 - val_loss: 0.6289\n",
      "Epoch 19/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5747\n",
      "Epoch 19: val_loss did not improve from 0.62888\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5738 - val_loss: 0.6361\n",
      "Epoch 20/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5703\n",
      "Epoch 20: val_loss improved from 0.62888 to 0.61585, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5707 - val_loss: 0.6158\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5613\n",
      "Epoch 21: val_loss improved from 0.61585 to 0.60042, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5613 - val_loss: 0.6004\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5447\n",
      "Epoch 22: val_loss improved from 0.60042 to 0.59531, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5447 - val_loss: 0.5953\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5320\n",
      "Epoch 23: val_loss did not improve from 0.59531\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5320 - val_loss: 0.6027\n",
      "Epoch 24/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5286\n",
      "Epoch 24: val_loss improved from 0.59531 to 0.57693, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5287 - val_loss: 0.5769\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5227\n",
      "Epoch 25: val_loss improved from 0.57693 to 0.57525, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5227 - val_loss: 0.5752\n",
      "Epoch 26/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5093\n",
      "Epoch 26: val_loss improved from 0.57525 to 0.56812, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5095 - val_loss: 0.5681\n",
      "Epoch 27/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5047\n",
      "Epoch 27: val_loss improved from 0.56812 to 0.56646, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5059 - val_loss: 0.5665\n",
      "Epoch 28/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4897\n",
      "Epoch 28: val_loss did not improve from 0.56646\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4862 - val_loss: 0.5685\n",
      "Epoch 29/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.4750\n",
      "Epoch 29: val_loss improved from 0.56646 to 0.56074, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.4778 - val_loss: 0.5607\n",
      "Epoch 30/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4827\n",
      "Epoch 30: val_loss did not improve from 0.56074\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4829 - val_loss: 0.5639\n",
      "Epoch 31/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4747\n",
      "Epoch 31: val_loss improved from 0.56074 to 0.54583, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4752 - val_loss: 0.5458\n",
      "Epoch 32/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4669\n",
      "Epoch 32: val_loss did not improve from 0.54583\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4639 - val_loss: 0.5523\n",
      "Epoch 33/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4485\n",
      "Epoch 33: val_loss improved from 0.54583 to 0.53817, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4505 - val_loss: 0.5382\n",
      "Epoch 34/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4559\n",
      "Epoch 34: val_loss improved from 0.53817 to 0.53477, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4562 - val_loss: 0.5348\n",
      "Epoch 35/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4463\n",
      "Epoch 35: val_loss improved from 0.53477 to 0.53041, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4490 - val_loss: 0.5304\n",
      "Epoch 36/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4383\n",
      "Epoch 36: val_loss did not improve from 0.53041\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4352 - val_loss: 0.5451\n",
      "Epoch 37/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4485\n",
      "Epoch 37: val_loss did not improve from 0.53041\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4454 - val_loss: 0.5424\n",
      "Epoch 38/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4313\n",
      "Epoch 38: val_loss improved from 0.53041 to 0.52187, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4366 - val_loss: 0.5219\n",
      "Epoch 39/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4277\n",
      "Epoch 39: val_loss did not improve from 0.52187\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4287 - val_loss: 0.5512\n",
      "Epoch 40/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4222\n",
      "Epoch 40: val_loss did not improve from 0.52187\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4219 - val_loss: 0.5374\n",
      "Epoch 41/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4194\n",
      "Epoch 41: val_loss improved from 0.52187 to 0.51323, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4188 - val_loss: 0.5132\n",
      "Epoch 42/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4208\n",
      "Epoch 42: val_loss improved from 0.51323 to 0.50783, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4188 - val_loss: 0.5078\n",
      "Epoch 43/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4001\n",
      "Epoch 43: val_loss did not improve from 0.50783\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3997 - val_loss: 0.5174\n",
      "Epoch 44/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4103\n",
      "Epoch 44: val_loss improved from 0.50783 to 0.50677, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4115 - val_loss: 0.5068\n",
      "Epoch 45/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3907\n",
      "Epoch 45: val_loss did not improve from 0.50677\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3931 - val_loss: 0.5082\n",
      "Epoch 46/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4047\n",
      "Epoch 46: val_loss did not improve from 0.50677\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4055 - val_loss: 0.5077\n",
      "Epoch 47/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3874\n",
      "Epoch 47: val_loss improved from 0.50677 to 0.50027, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.5003\n",
      "Epoch 48/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3862\n",
      "Epoch 48: val_loss improved from 0.50027 to 0.49939, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3849 - val_loss: 0.4994\n",
      "Epoch 49/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3906\n",
      "Epoch 49: val_loss did not improve from 0.49939\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3890 - val_loss: 0.5017\n",
      "Epoch 50/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3843\n",
      "Epoch 50: val_loss improved from 0.49939 to 0.49081, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3824 - val_loss: 0.4908\n",
      "Epoch 51/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3792\n",
      "Epoch 51: val_loss did not improve from 0.49081\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3827 - val_loss: 0.4970\n",
      "Epoch 52/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3804\n",
      "Epoch 52: val_loss did not improve from 0.49081\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3805 - val_loss: 0.5118\n",
      "Epoch 53/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3733\n",
      "Epoch 53: val_loss improved from 0.49081 to 0.48142, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3688 - val_loss: 0.4814\n",
      "Epoch 54/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3617\n",
      "Epoch 54: val_loss improved from 0.48142 to 0.47858, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3638 - val_loss: 0.4786\n",
      "Epoch 55/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3709\n",
      "Epoch 55: val_loss improved from 0.47858 to 0.47847, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3710 - val_loss: 0.4785\n",
      "Epoch 56/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3693\n",
      "Epoch 56: val_loss improved from 0.47847 to 0.47214, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3689 - val_loss: 0.4721\n",
      "Epoch 57/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3751\n",
      "Epoch 57: val_loss improved from 0.47214 to 0.47212, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3765 - val_loss: 0.4721\n",
      "Epoch 58/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3629\n",
      "Epoch 58: val_loss did not improve from 0.47212\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3625 - val_loss: 0.4742\n",
      "Epoch 59/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3541\n",
      "Epoch 59: val_loss did not improve from 0.47212\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3561 - val_loss: 0.4751\n",
      "Epoch 60/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3534\n",
      "Epoch 60: val_loss did not improve from 0.47212\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3533 - val_loss: 0.4729\n",
      "Epoch 61/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3478\n",
      "Epoch 61: val_loss improved from 0.47212 to 0.46936, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3466 - val_loss: 0.4694\n",
      "Epoch 62/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3442\n",
      "Epoch 62: val_loss improved from 0.46936 to 0.46553, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3422 - val_loss: 0.4655\n",
      "Epoch 63/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3383\n",
      "Epoch 63: val_loss improved from 0.46553 to 0.46464, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3390 - val_loss: 0.4646\n",
      "Epoch 64/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3446\n",
      "Epoch 64: val_loss did not improve from 0.46464\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3440 - val_loss: 0.4779\n",
      "Epoch 65/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3377\n",
      "Epoch 65: val_loss did not improve from 0.46464\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3383 - val_loss: 0.5181\n",
      "Epoch 66/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3416\n",
      "Epoch 66: val_loss improved from 0.46464 to 0.46321, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3412 - val_loss: 0.4632\n",
      "Epoch 67/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3513\n",
      "Epoch 67: val_loss did not improve from 0.46321\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3524 - val_loss: 0.4669\n",
      "Epoch 68/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3514\n",
      "Epoch 68: val_loss improved from 0.46321 to 0.45676, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3482 - val_loss: 0.4568\n",
      "Epoch 69/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3389\n",
      "Epoch 69: val_loss did not improve from 0.45676\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3392 - val_loss: 0.4638\n",
      "Epoch 70/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3285\n",
      "Epoch 70: val_loss improved from 0.45676 to 0.45443, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3282 - val_loss: 0.4544\n",
      "Epoch 71/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3220\n",
      "Epoch 71: val_loss did not improve from 0.45443\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3218 - val_loss: 0.4587\n",
      "Epoch 72/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3344\n",
      "Epoch 72: val_loss improved from 0.45443 to 0.45141, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3345 - val_loss: 0.4514\n",
      "Epoch 73/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3328\n",
      "Epoch 73: val_loss improved from 0.45141 to 0.44412, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3329 - val_loss: 0.4441\n",
      "Epoch 74/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3269\n",
      "Epoch 74: val_loss did not improve from 0.44412\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3263 - val_loss: 0.4589\n",
      "Epoch 75/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3285\n",
      "Epoch 75: val_loss did not improve from 0.44412\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3284 - val_loss: 0.4471\n",
      "Epoch 76/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3268\n",
      "Epoch 76: val_loss did not improve from 0.44412\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3271 - val_loss: 0.4483\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3129\n",
      "Epoch 77: val_loss did not improve from 0.44412\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3129 - val_loss: 0.4486\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3279\n",
      "Epoch 78: val_loss did not improve from 0.44412\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3279 - val_loss: 0.4472\n",
      "Epoch 79/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3227\n",
      "Epoch 79: val_loss did not improve from 0.44412\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3224 - val_loss: 0.4478\n",
      "Epoch 80/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3112\n",
      "Epoch 80: val_loss improved from 0.44412 to 0.44303, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3112 - val_loss: 0.4430\n",
      "Epoch 81/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3187\n",
      "Epoch 81: val_loss improved from 0.44303 to 0.44024, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3188 - val_loss: 0.4402\n",
      "Epoch 82/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3117\n",
      "Epoch 82: val_loss did not improve from 0.44024\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3112 - val_loss: 0.4463\n",
      "Epoch 83/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3102\n",
      "Epoch 83: val_loss did not improve from 0.44024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3098 - val_loss: 0.4428\n",
      "Epoch 84/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3027\n",
      "Epoch 84: val_loss did not improve from 0.44024\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3010 - val_loss: 0.4561\n",
      "Epoch 85/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3149\n",
      "Epoch 85: val_loss improved from 0.44024 to 0.43460, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3139 - val_loss: 0.4346\n",
      "Epoch 86/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3008\n",
      "Epoch 86: val_loss did not improve from 0.43460\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3039 - val_loss: 0.4378\n",
      "Epoch 87/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3061\n",
      "Epoch 87: val_loss improved from 0.43460 to 0.43172, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3068 - val_loss: 0.4317\n",
      "Epoch 88/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3035\n",
      "Epoch 88: val_loss improved from 0.43172 to 0.42721, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3043 - val_loss: 0.4272\n",
      "Epoch 89/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3048\n",
      "Epoch 89: val_loss improved from 0.42721 to 0.42319, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3062 - val_loss: 0.4232\n",
      "Epoch 90/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3022\n",
      "Epoch 90: val_loss did not improve from 0.42319\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3042 - val_loss: 0.4374\n",
      "Epoch 91/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3035\n",
      "Epoch 91: val_loss did not improve from 0.42319\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3040 - val_loss: 0.4301\n",
      "Epoch 92/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.2943\n",
      "Epoch 92: val_loss did not improve from 0.42319\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2942 - val_loss: 0.4277\n",
      "Epoch 93/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3001\n",
      "Epoch 93: val_loss improved from 0.42319 to 0.41918, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold2.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3013 - val_loss: 0.4192\n",
      "Epoch 94/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3046\n",
      "Epoch 94: val_loss did not improve from 0.41918\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3035 - val_loss: 0.4347\n",
      "Epoch 95/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.2951\n",
      "Epoch 95: val_loss did not improve from 0.41918\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2939 - val_loss: 0.4255\n",
      "Epoch 96/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.2931\n",
      "Epoch 96: val_loss did not improve from 0.41918\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2927 - val_loss: 0.4283\n",
      "Epoch 97/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.2990\n",
      "Epoch 97: val_loss did not improve from 0.41918\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2991 - val_loss: 0.4250\n",
      "Epoch 98/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3011\n",
      "Epoch 98: val_loss did not improve from 0.41918\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3011 - val_loss: 0.4285\n",
      "Epoch 99/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.2933\n",
      "Epoch 99: val_loss did not improve from 0.41918\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2910 - val_loss: 0.4340\n",
      "Epoch 100/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.2945\n",
      "Epoch 100: val_loss did not improve from 0.41918\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2943 - val_loss: 0.4449\n",
      "\n",
      "Train/Test model on Fold #3.\n",
      "Epoch 1/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 1.3840\n",
      "Epoch 1: val_loss improved from inf to 1.28834, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 3s 19ms/step - loss: 1.3777 - val_loss: 1.2883\n",
      "Epoch 2/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 1.2260\n",
      "Epoch 2: val_loss improved from 1.28834 to 1.12033, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.2215 - val_loss: 1.1203\n",
      "Epoch 3/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 1.0973\n",
      "Epoch 3: val_loss improved from 1.12033 to 1.00809, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.0943 - val_loss: 1.0081\n",
      "Epoch 4/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.9991\n",
      "Epoch 4: val_loss improved from 1.00809 to 0.92326, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9932 - val_loss: 0.9233\n",
      "Epoch 5/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.9258\n",
      "Epoch 5: val_loss improved from 0.92326 to 0.87491, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9260 - val_loss: 0.8749\n",
      "Epoch 6/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.8821\n",
      "Epoch 6: val_loss improved from 0.87491 to 0.82932, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8806 - val_loss: 0.8293\n",
      "Epoch 7/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.8438\n",
      "Epoch 7: val_loss improved from 0.82932 to 0.79033, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8434 - val_loss: 0.7903\n",
      "Epoch 8/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.8033\n",
      "Epoch 8: val_loss improved from 0.79033 to 0.76418, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8029 - val_loss: 0.7642\n",
      "Epoch 9/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.7730\n",
      "Epoch 9: val_loss improved from 0.76418 to 0.74099, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7691 - val_loss: 0.7410\n",
      "Epoch 10/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.7467\n",
      "Epoch 10: val_loss improved from 0.74099 to 0.71628, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7455 - val_loss: 0.7163\n",
      "Epoch 11/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.7234\n",
      "Epoch 11: val_loss improved from 0.71628 to 0.69368, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7242 - val_loss: 0.6937\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/58 [============================>.] - ETA: 0s - loss: 0.7014\n",
      "Epoch 12: val_loss improved from 0.69368 to 0.68491, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7022 - val_loss: 0.6849\n",
      "Epoch 13/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.6871\n",
      "Epoch 13: val_loss improved from 0.68491 to 0.66321, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6879 - val_loss: 0.6632\n",
      "Epoch 14/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6696\n",
      "Epoch 14: val_loss improved from 0.66321 to 0.64699, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6669 - val_loss: 0.6470\n",
      "Epoch 15/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.6499\n",
      "Epoch 15: val_loss improved from 0.64699 to 0.63578, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6489 - val_loss: 0.6358\n",
      "Epoch 16/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6314\n",
      "Epoch 16: val_loss improved from 0.63578 to 0.62242, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6304 - val_loss: 0.6224\n",
      "Epoch 17/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6172\n",
      "Epoch 17: val_loss did not improve from 0.62242\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.6196 - val_loss: 0.6483\n",
      "Epoch 18/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.6110\n",
      "Epoch 18: val_loss improved from 0.62242 to 0.60283, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6114 - val_loss: 0.6028\n",
      "Epoch 19/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.6187\n",
      "Epoch 19: val_loss improved from 0.60283 to 0.59959, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6173 - val_loss: 0.5996\n",
      "Epoch 20/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5974\n",
      "Epoch 20: val_loss improved from 0.59959 to 0.58906, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5989 - val_loss: 0.5891\n",
      "Epoch 21/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5775\n",
      "Epoch 21: val_loss improved from 0.58906 to 0.57542, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5764 - val_loss: 0.5754\n",
      "Epoch 22/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.5715\n",
      "Epoch 22: val_loss improved from 0.57542 to 0.57259, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5706 - val_loss: 0.5726\n",
      "Epoch 23/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5580\n",
      "Epoch 23: val_loss improved from 0.57259 to 0.55927, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5562 - val_loss: 0.5593\n",
      "Epoch 24/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5539\n",
      "Epoch 24: val_loss improved from 0.55927 to 0.55409, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5527 - val_loss: 0.5541\n",
      "Epoch 25/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5339\n",
      "Epoch 25: val_loss improved from 0.55409 to 0.55064, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5373 - val_loss: 0.5506\n",
      "Epoch 26/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5341\n",
      "Epoch 26: val_loss improved from 0.55064 to 0.54356, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5337 - val_loss: 0.5436\n",
      "Epoch 27/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5479\n",
      "Epoch 27: val_loss did not improve from 0.54356\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5470 - val_loss: 0.5440\n",
      "Epoch 28/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5118\n",
      "Epoch 28: val_loss improved from 0.54356 to 0.53639, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5117 - val_loss: 0.5364\n",
      "Epoch 29/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5042\n",
      "Epoch 29: val_loss improved from 0.53639 to 0.52513, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5039 - val_loss: 0.5251\n",
      "Epoch 30/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4973\n",
      "Epoch 30: val_loss improved from 0.52513 to 0.52181, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4966 - val_loss: 0.5218\n",
      "Epoch 31/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4908\n",
      "Epoch 31: val_loss improved from 0.52181 to 0.51557, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4916 - val_loss: 0.5156\n",
      "Epoch 32/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4911\n",
      "Epoch 32: val_loss improved from 0.51557 to 0.51022, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4932 - val_loss: 0.5102\n",
      "Epoch 33/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4795\n",
      "Epoch 33: val_loss improved from 0.51022 to 0.50576, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4793 - val_loss: 0.5058\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4816\n",
      "Epoch 34: val_loss improved from 0.50576 to 0.50220, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4816 - val_loss: 0.5022\n",
      "Epoch 35/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4671\n",
      "Epoch 35: val_loss improved from 0.50220 to 0.50096, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4711 - val_loss: 0.5010\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4703\n",
      "Epoch 36: val_loss improved from 0.50096 to 0.50090, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 11ms/step - loss: 0.4703 - val_loss: 0.5009\n",
      "Epoch 37/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4552\n",
      "Epoch 37: val_loss improved from 0.50090 to 0.48865, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4554 - val_loss: 0.4886\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4574\n",
      "Epoch 38: val_loss did not improve from 0.48865\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4574 - val_loss: 0.4999\n",
      "Epoch 39/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.4575\n",
      "Epoch 39: val_loss improved from 0.48865 to 0.48060, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4542 - val_loss: 0.4806\n",
      "Epoch 40/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.4356\n",
      "Epoch 40: val_loss improved from 0.48060 to 0.47254, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4361 - val_loss: 0.4725\n",
      "Epoch 41/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4349\n",
      "Epoch 41: val_loss did not improve from 0.47254\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4347 - val_loss: 0.4766\n",
      "Epoch 42/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4360\n",
      "Epoch 42: val_loss did not improve from 0.47254\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4334 - val_loss: 0.4756\n",
      "Epoch 43/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4158\n",
      "Epoch 43: val_loss did not improve from 0.47254\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4156 - val_loss: 0.5033\n",
      "Epoch 44/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4256\n",
      "Epoch 44: val_loss improved from 0.47254 to 0.47124, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4238 - val_loss: 0.4712\n",
      "Epoch 45/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.4219\n",
      "Epoch 45: val_loss improved from 0.47124 to 0.46134, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4211 - val_loss: 0.4613\n",
      "Epoch 46/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4097\n",
      "Epoch 46: val_loss improved from 0.46134 to 0.45901, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4094 - val_loss: 0.4590\n",
      "Epoch 47/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3947\n",
      "Epoch 47: val_loss did not improve from 0.45901\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3954 - val_loss: 0.4623\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4006\n",
      "Epoch 48: val_loss improved from 0.45901 to 0.45233, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.4006 - val_loss: 0.4523\n",
      "Epoch 49/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3973\n",
      "Epoch 49: val_loss improved from 0.45233 to 0.44697, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3973 - val_loss: 0.4470\n",
      "Epoch 50/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4033\n",
      "Epoch 50: val_loss improved from 0.44697 to 0.44206, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4025 - val_loss: 0.4421\n",
      "Epoch 51/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3988\n",
      "Epoch 51: val_loss did not improve from 0.44206\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3973 - val_loss: 0.4504\n",
      "Epoch 52/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3832\n",
      "Epoch 52: val_loss did not improve from 0.44206\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3825 - val_loss: 0.4449\n",
      "Epoch 53/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3876\n",
      "Epoch 53: val_loss did not improve from 0.44206\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3865 - val_loss: 0.4622\n",
      "Epoch 54/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3805\n",
      "Epoch 54: val_loss improved from 0.44206 to 0.43500, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3836 - val_loss: 0.4350\n",
      "Epoch 55/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3824\n",
      "Epoch 55: val_loss did not improve from 0.43500\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3809 - val_loss: 0.4412\n",
      "Epoch 56/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3956\n",
      "Epoch 56: val_loss did not improve from 0.43500\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3976 - val_loss: 0.4762\n",
      "Epoch 57/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3868\n",
      "Epoch 57: val_loss improved from 0.43500 to 0.43438, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3852 - val_loss: 0.4344\n",
      "Epoch 58/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3827\n",
      "Epoch 58: val_loss did not improve from 0.43438\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3838 - val_loss: 0.4352\n",
      "Epoch 59/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3786\n",
      "Epoch 59: val_loss did not improve from 0.43438\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3809 - val_loss: 0.4347\n",
      "Epoch 60/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3657\n",
      "Epoch 60: val_loss improved from 0.43438 to 0.43180, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3697 - val_loss: 0.4318\n",
      "Epoch 61/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3623\n",
      "Epoch 61: val_loss improved from 0.43180 to 0.42311, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3616 - val_loss: 0.4231\n",
      "Epoch 62/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3488\n",
      "Epoch 62: val_loss did not improve from 0.42311\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3489 - val_loss: 0.4308\n",
      "Epoch 63/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3606\n",
      "Epoch 63: val_loss did not improve from 0.42311\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3623 - val_loss: 0.4247\n",
      "Epoch 64/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3526\n",
      "Epoch 64: val_loss improved from 0.42311 to 0.41768, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3544 - val_loss: 0.4177\n",
      "Epoch 65/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3589\n",
      "Epoch 65: val_loss improved from 0.41768 to 0.41454, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3570 - val_loss: 0.4145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3482\n",
      "Epoch 66: val_loss improved from 0.41454 to 0.41262, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3481 - val_loss: 0.4126\n",
      "Epoch 67/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3514\n",
      "Epoch 67: val_loss improved from 0.41262 to 0.41173, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3530 - val_loss: 0.4117\n",
      "Epoch 68/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3551\n",
      "Epoch 68: val_loss did not improve from 0.41173\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3548 - val_loss: 0.4192\n",
      "Epoch 69/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3353\n",
      "Epoch 69: val_loss did not improve from 0.41173\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3385 - val_loss: 0.4146\n",
      "Epoch 70/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3322\n",
      "Epoch 70: val_loss did not improve from 0.41173\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3327 - val_loss: 0.4142\n",
      "Epoch 71/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3559\n",
      "Epoch 71: val_loss improved from 0.41173 to 0.40739, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3540 - val_loss: 0.4074\n",
      "Epoch 72/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3442\n",
      "Epoch 72: val_loss did not improve from 0.40739\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3410 - val_loss: 0.4152\n",
      "Epoch 73/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3400\n",
      "Epoch 73: val_loss improved from 0.40739 to 0.40290, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3395 - val_loss: 0.4029\n",
      "Epoch 74/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3206\n",
      "Epoch 74: val_loss did not improve from 0.40290\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3207 - val_loss: 0.4077\n",
      "Epoch 75/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3288\n",
      "Epoch 75: val_loss did not improve from 0.40290\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3302 - val_loss: 0.4160\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3376\n",
      "Epoch 76: val_loss did not improve from 0.40290\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3376 - val_loss: 0.4063\n",
      "Epoch 77/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3282\n",
      "Epoch 77: val_loss improved from 0.40290 to 0.40177, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3270 - val_loss: 0.4018\n",
      "Epoch 78/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3183\n",
      "Epoch 78: val_loss improved from 0.40177 to 0.39787, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3210 - val_loss: 0.3979\n",
      "Epoch 79/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3266\n",
      "Epoch 79: val_loss improved from 0.39787 to 0.39398, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3250 - val_loss: 0.3940\n",
      "Epoch 80/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3314\n",
      "Epoch 80: val_loss did not improve from 0.39398\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3306 - val_loss: 0.3955\n",
      "Epoch 81/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3292\n",
      "Epoch 81: val_loss improved from 0.39398 to 0.38973, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3282 - val_loss: 0.3897\n",
      "Epoch 82/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3205\n",
      "Epoch 82: val_loss did not improve from 0.38973\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3230 - val_loss: 0.3922\n",
      "Epoch 83/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3262\n",
      "Epoch 83: val_loss did not improve from 0.38973\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3256 - val_loss: 0.3963\n",
      "Epoch 84/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3195\n",
      "Epoch 84: val_loss did not improve from 0.38973\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3205 - val_loss: 0.3897\n",
      "Epoch 85/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3198\n",
      "Epoch 85: val_loss did not improve from 0.38973\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3197 - val_loss: 0.4024\n",
      "Epoch 86/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3085\n",
      "Epoch 86: val_loss improved from 0.38973 to 0.38776, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3102 - val_loss: 0.3878\n",
      "Epoch 87/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3073\n",
      "Epoch 87: val_loss improved from 0.38776 to 0.38274, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3054 - val_loss: 0.3827\n",
      "Epoch 88/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3172\n",
      "Epoch 88: val_loss did not improve from 0.38274\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3153 - val_loss: 0.3899\n",
      "Epoch 89/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3066\n",
      "Epoch 89: val_loss did not improve from 0.38274\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3075 - val_loss: 0.3828\n",
      "Epoch 90/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3141\n",
      "Epoch 90: val_loss improved from 0.38274 to 0.38140, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3156 - val_loss: 0.3814\n",
      "Epoch 91/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3078\n",
      "Epoch 91: val_loss did not improve from 0.38140\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3068 - val_loss: 0.3843\n",
      "Epoch 92/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3091\n",
      "Epoch 92: val_loss did not improve from 0.38140\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3094 - val_loss: 0.3835\n",
      "Epoch 93/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3211\n",
      "Epoch 93: val_loss did not improve from 0.38140\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3209 - val_loss: 0.3834\n",
      "Epoch 94/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3036\n",
      "Epoch 94: val_loss improved from 0.38140 to 0.37781, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3076 - val_loss: 0.3778\n",
      "Epoch 95/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3105\n",
      "Epoch 95: val_loss did not improve from 0.37781\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3101 - val_loss: 0.3792\n",
      "Epoch 96/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3012\n",
      "Epoch 96: val_loss did not improve from 0.37781\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2998 - val_loss: 0.3812\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2994\n",
      "Epoch 97: val_loss improved from 0.37781 to 0.37485, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2992 - val_loss: 0.3749\n",
      "Epoch 98/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2927\n",
      "Epoch 98: val_loss did not improve from 0.37485\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2924 - val_loss: 0.3775\n",
      "Epoch 99/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3061\n",
      "Epoch 99: val_loss did not improve from 0.37485\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3040 - val_loss: 0.3777\n",
      "Epoch 100/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.2959\n",
      "Epoch 100: val_loss improved from 0.37485 to 0.37430, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold3.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2958 - val_loss: 0.3743\n",
      "\n",
      "Train/Test model on Fold #4.\n",
      "Epoch 1/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 1.3790\n",
      "Epoch 1: val_loss improved from inf to 1.29351, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 3s 19ms/step - loss: 1.3771 - val_loss: 1.2935\n",
      "Epoch 2/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 1.2317\n",
      "Epoch 2: val_loss improved from 1.29351 to 1.13659, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.2280 - val_loss: 1.1366\n",
      "Epoch 3/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 1.0727\n",
      "Epoch 3: val_loss improved from 1.13659 to 1.01457, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 1.0715 - val_loss: 1.0146\n",
      "Epoch 4/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.9822\n",
      "Epoch 4: val_loss improved from 1.01457 to 0.93434, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9803 - val_loss: 0.9343\n",
      "Epoch 5/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.9270\n",
      "Epoch 5: val_loss improved from 0.93434 to 0.87639, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.9248 - val_loss: 0.8764\n",
      "Epoch 6/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.8669\n",
      "Epoch 6: val_loss improved from 0.87639 to 0.83099, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8644 - val_loss: 0.8310\n",
      "Epoch 7/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.8256\n",
      "Epoch 7: val_loss improved from 0.83099 to 0.79632, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.8281 - val_loss: 0.7963\n",
      "Epoch 8/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.7923\n",
      "Epoch 8: val_loss improved from 0.79632 to 0.76963, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7913 - val_loss: 0.7696\n",
      "Epoch 9/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.7633\n",
      "Epoch 9: val_loss improved from 0.76963 to 0.74772, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7631 - val_loss: 0.7477\n",
      "Epoch 10/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.7337\n",
      "Epoch 10: val_loss improved from 0.74772 to 0.72447, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7324 - val_loss: 0.7245\n",
      "Epoch 11/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.7121\n",
      "Epoch 11: val_loss improved from 0.72447 to 0.70058, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.7122 - val_loss: 0.7006\n",
      "Epoch 12/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.6911\n",
      "Epoch 12: val_loss improved from 0.70058 to 0.68484, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6915 - val_loss: 0.6848\n",
      "Epoch 13/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.6759\n",
      "Epoch 13: val_loss improved from 0.68484 to 0.66570, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6756 - val_loss: 0.6657\n",
      "Epoch 14/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.6500\n",
      "Epoch 14: val_loss improved from 0.66570 to 0.65172, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6502 - val_loss: 0.6517\n",
      "Epoch 15/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.6349\n",
      "Epoch 15: val_loss improved from 0.65172 to 0.63994, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.6403 - val_loss: 0.6399\n",
      "Epoch 16/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.6240\n",
      "Epoch 16: val_loss improved from 0.63994 to 0.63134, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6234 - val_loss: 0.6313\n",
      "Epoch 17/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.6108\n",
      "Epoch 17: val_loss improved from 0.63134 to 0.61445, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6113 - val_loss: 0.6144\n",
      "Epoch 18/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.6014\n",
      "Epoch 18: val_loss improved from 0.61445 to 0.60642, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.6008 - val_loss: 0.6064\n",
      "Epoch 19/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5870\n",
      "Epoch 19: val_loss improved from 0.60642 to 0.59459, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5867 - val_loss: 0.5946\n",
      "Epoch 20/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.5820\n",
      "Epoch 20: val_loss improved from 0.59459 to 0.58543, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5776 - val_loss: 0.5854\n",
      "Epoch 21/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.5623\n",
      "Epoch 21: val_loss did not improve from 0.58543\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5628 - val_loss: 0.5870\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/58 [===========================>..] - ETA: 0s - loss: 0.5720\n",
      "Epoch 22: val_loss improved from 0.58543 to 0.56859, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5699 - val_loss: 0.5686\n",
      "Epoch 23/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.5471\n",
      "Epoch 23: val_loss improved from 0.56859 to 0.56341, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5454 - val_loss: 0.5634\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5425\n",
      "Epoch 24: val_loss improved from 0.56341 to 0.55667, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5425 - val_loss: 0.5567\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5435\n",
      "Epoch 25: val_loss did not improve from 0.55667\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 0.5435 - val_loss: 0.5615\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5177\n",
      "Epoch 26: val_loss improved from 0.55667 to 0.54137, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5177 - val_loss: 0.5414\n",
      "Epoch 27/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5127\n",
      "Epoch 27: val_loss improved from 0.54137 to 0.53382, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.5132 - val_loss: 0.5338\n",
      "Epoch 28/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.5085\n",
      "Epoch 28: val_loss did not improve from 0.53382\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.5078 - val_loss: 0.5388\n",
      "Epoch 29/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4835\n",
      "Epoch 29: val_loss did not improve from 0.53382\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.4848 - val_loss: 0.5526\n",
      "Epoch 30/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4850\n",
      "Epoch 30: val_loss improved from 0.53382 to 0.53191, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4842 - val_loss: 0.5319\n",
      "Epoch 31/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.4904\n",
      "Epoch 31: val_loss improved from 0.53191 to 0.51143, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4887 - val_loss: 0.5114\n",
      "Epoch 32/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4733\n",
      "Epoch 32: val_loss improved from 0.51143 to 0.50506, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4748 - val_loss: 0.5051\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4719\n",
      "Epoch 33: val_loss did not improve from 0.50506\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.4719 - val_loss: 0.5072\n",
      "Epoch 34/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4643\n",
      "Epoch 34: val_loss did not improve from 0.50506\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4647 - val_loss: 0.5054\n",
      "Epoch 35/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4577\n",
      "Epoch 35: val_loss improved from 0.50506 to 0.49353, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4573 - val_loss: 0.4935\n",
      "Epoch 36/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4551\n",
      "Epoch 36: val_loss improved from 0.49353 to 0.48644, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4556 - val_loss: 0.4864\n",
      "Epoch 37/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4435\n",
      "Epoch 37: val_loss did not improve from 0.48644\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4442 - val_loss: 0.4889\n",
      "Epoch 38/100\n",
      "51/58 [=========================>....] - ETA: 0s - loss: 0.4326\n",
      "Epoch 38: val_loss improved from 0.48644 to 0.48436, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4336 - val_loss: 0.4844\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4361\n",
      "Epoch 39: val_loss improved from 0.48436 to 0.47177, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4361 - val_loss: 0.4718\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4208\n",
      "Epoch 40: val_loss improved from 0.47177 to 0.46474, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4208 - val_loss: 0.4647\n",
      "Epoch 41/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4294\n",
      "Epoch 41: val_loss improved from 0.46474 to 0.46402, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4287 - val_loss: 0.4640\n",
      "Epoch 42/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.4198\n",
      "Epoch 42: val_loss improved from 0.46402 to 0.46393, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4201 - val_loss: 0.4639\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4198\n",
      "Epoch 43: val_loss improved from 0.46393 to 0.45834, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4198 - val_loss: 0.4583\n",
      "Epoch 44/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.4130\n",
      "Epoch 44: val_loss improved from 0.45834 to 0.45514, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4183 - val_loss: 0.4551\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4057\n",
      "Epoch 45: val_loss improved from 0.45514 to 0.45442, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.4057 - val_loss: 0.4544\n",
      "Epoch 46/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.4104\n",
      "Epoch 46: val_loss did not improve from 0.45442\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.4108 - val_loss: 0.4576\n",
      "Epoch 47/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3928\n",
      "Epoch 47: val_loss improved from 0.45442 to 0.44381, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3923 - val_loss: 0.4438\n",
      "Epoch 48/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3965\n",
      "Epoch 48: val_loss improved from 0.44381 to 0.44100, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3936 - val_loss: 0.4410\n",
      "Epoch 49/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3868\n",
      "Epoch 49: val_loss did not improve from 0.44100\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3873 - val_loss: 0.4451\n",
      "Epoch 50/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3908\n",
      "Epoch 50: val_loss improved from 0.44100 to 0.43889, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3911 - val_loss: 0.4389\n",
      "Epoch 51/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3926\n",
      "Epoch 51: val_loss improved from 0.43889 to 0.43666, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3938 - val_loss: 0.4367\n",
      "Epoch 52/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3955\n",
      "Epoch 52: val_loss improved from 0.43666 to 0.43260, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3955 - val_loss: 0.4326\n",
      "Epoch 53/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3995\n",
      "Epoch 53: val_loss did not improve from 0.43260\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.4000 - val_loss: 0.4334\n",
      "Epoch 54/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.3703\n",
      "Epoch 54: val_loss improved from 0.43260 to 0.42782, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3728 - val_loss: 0.4278\n",
      "Epoch 55/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3716\n",
      "Epoch 55: val_loss did not improve from 0.42782\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3712 - val_loss: 0.4373\n",
      "Epoch 56/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3657\n",
      "Epoch 56: val_loss did not improve from 0.42782\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3671 - val_loss: 0.4290\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3668\n",
      "Epoch 57: val_loss improved from 0.42782 to 0.42652, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3668 - val_loss: 0.4265\n",
      "Epoch 58/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3702\n",
      "Epoch 58: val_loss improved from 0.42652 to 0.42565, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3731 - val_loss: 0.4256\n",
      "Epoch 59/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3695\n",
      "Epoch 59: val_loss did not improve from 0.42565\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3706 - val_loss: 0.4348\n",
      "Epoch 60/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3479\n",
      "Epoch 60: val_loss improved from 0.42565 to 0.41434, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3465 - val_loss: 0.4143\n",
      "Epoch 61/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3568\n",
      "Epoch 61: val_loss improved from 0.41434 to 0.41367, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3580 - val_loss: 0.4137\n",
      "Epoch 62/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3592\n",
      "Epoch 62: val_loss did not improve from 0.41367\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3582 - val_loss: 0.4242\n",
      "Epoch 63/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3489\n",
      "Epoch 63: val_loss improved from 0.41367 to 0.41338, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3472 - val_loss: 0.4134\n",
      "Epoch 64/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3439\n",
      "Epoch 64: val_loss improved from 0.41338 to 0.40802, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3424 - val_loss: 0.4080\n",
      "Epoch 65/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3573\n",
      "Epoch 65: val_loss did not improve from 0.40802\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3568 - val_loss: 0.4105\n",
      "Epoch 66/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3472\n",
      "Epoch 66: val_loss did not improve from 0.40802\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3461 - val_loss: 0.4094\n",
      "Epoch 67/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3402\n",
      "Epoch 67: val_loss did not improve from 0.40802\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3404 - val_loss: 0.4259\n",
      "Epoch 68/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3378\n",
      "Epoch 68: val_loss improved from 0.40802 to 0.40487, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3382 - val_loss: 0.4049\n",
      "Epoch 69/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3360\n",
      "Epoch 69: val_loss improved from 0.40487 to 0.40149, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3355 - val_loss: 0.4015\n",
      "Epoch 70/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3361\n",
      "Epoch 70: val_loss improved from 0.40149 to 0.39957, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3371 - val_loss: 0.3996\n",
      "Epoch 71/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3356\n",
      "Epoch 71: val_loss did not improve from 0.39957\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3361 - val_loss: 0.4033\n",
      "Epoch 72/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3269\n",
      "Epoch 72: val_loss did not improve from 0.39957\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3288 - val_loss: 0.4044\n",
      "Epoch 73/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3305\n",
      "Epoch 73: val_loss did not improve from 0.39957\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3306 - val_loss: 0.4146\n",
      "Epoch 74/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3340\n",
      "Epoch 74: val_loss did not improve from 0.39957\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3372 - val_loss: 0.4109\n",
      "Epoch 75/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3251\n",
      "Epoch 75: val_loss did not improve from 0.39957\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3271 - val_loss: 0.4118\n",
      "Epoch 76/100\n",
      "53/58 [==========================>...] - ETA: 0s - loss: 0.3121\n",
      "Epoch 76: val_loss did not improve from 0.39957\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3120 - val_loss: 0.4119\n",
      "Epoch 77/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3086\n",
      "Epoch 77: val_loss did not improve from 0.39957\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3079 - val_loss: 0.4044\n",
      "Epoch 78/100\n",
      "55/58 [===========================>..] - ETA: 0s - loss: 0.3194\n",
      "Epoch 78: val_loss did not improve from 0.39957\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3165 - val_loss: 0.4047\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3245\n",
      "Epoch 79: val_loss improved from 0.39957 to 0.39766, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3245 - val_loss: 0.3977\n",
      "Epoch 80/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3163\n",
      "Epoch 80: val_loss did not improve from 0.39766\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3153 - val_loss: 0.4221\n",
      "Epoch 81/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3191\n",
      "Epoch 81: val_loss improved from 0.39766 to 0.39551, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 11ms/step - loss: 0.3191 - val_loss: 0.3955\n",
      "Epoch 82/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3058\n",
      "Epoch 82: val_loss improved from 0.39551 to 0.39401, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3056 - val_loss: 0.3940\n",
      "Epoch 83/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.3162\n",
      "Epoch 83: val_loss did not improve from 0.39401\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3160 - val_loss: 0.3973\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3252\n",
      "Epoch 84: val_loss did not improve from 0.39401\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3252 - val_loss: 0.3957\n",
      "Epoch 85/100\n",
      "51/58 [=========================>....] - ETA: 0s - loss: 0.2997\n",
      "Epoch 85: val_loss improved from 0.39401 to 0.39356, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3047 - val_loss: 0.3936\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3181\n",
      "Epoch 86: val_loss improved from 0.39356 to 0.39086, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3181 - val_loss: 0.3909\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3096\n",
      "Epoch 87: val_loss improved from 0.39086 to 0.38675, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3096 - val_loss: 0.3867\n",
      "Epoch 88/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3089\n",
      "Epoch 88: val_loss did not improve from 0.38675\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3093 - val_loss: 0.4040\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3083\n",
      "Epoch 89: val_loss did not improve from 0.38675\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3083 - val_loss: 0.3920\n",
      "Epoch 90/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.2997\n",
      "Epoch 90: val_loss did not improve from 0.38675\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3000 - val_loss: 0.3938\n",
      "Epoch 91/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.3151\n",
      "Epoch 91: val_loss did not improve from 0.38675\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.3099 - val_loss: 0.3975\n",
      "Epoch 92/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.3026\n",
      "Epoch 92: val_loss improved from 0.38675 to 0.38185, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\bestModel-fold4.hdf5\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.3029 - val_loss: 0.3819\n",
      "Epoch 93/100\n",
      "54/58 [==========================>...] - ETA: 0s - loss: 0.3032\n",
      "Epoch 93: val_loss did not improve from 0.38185\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2996 - val_loss: 0.3890\n",
      "Epoch 94/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2994\n",
      "Epoch 94: val_loss did not improve from 0.38185\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.3011 - val_loss: 0.3896\n",
      "Epoch 95/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2889\n",
      "Epoch 95: val_loss did not improve from 0.38185\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2886 - val_loss: 0.4100\n",
      "Epoch 96/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2960\n",
      "Epoch 96: val_loss did not improve from 0.38185\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2967 - val_loss: 0.3823\n",
      "Epoch 97/100\n",
      "56/58 [===========================>..] - ETA: 0s - loss: 0.2958\n",
      "Epoch 97: val_loss did not improve from 0.38185\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.2956 - val_loss: 0.3828\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.2951\n",
      "Epoch 98: val_loss did not improve from 0.38185\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2951 - val_loss: 0.4190\n",
      "Epoch 99/100\n",
      "57/58 [============================>.] - ETA: 0s - loss: 0.2908\n",
      "Epoch 99: val_loss did not improve from 0.38185\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2904 - val_loss: 0.3983\n",
      "Epoch 100/100\n",
      "52/58 [=========================>....] - ETA: 0s - loss: 0.2853\n",
      "Epoch 100: val_loss did not improve from 0.38185\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.2867 - val_loss: 0.3860\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Train/Test model on all folds, generate evaluations\n",
    "##################################################################################\n",
    "\n",
    "## Create and set directory to save model\n",
    "modelPath = os.path.join(outPath, expName, \"{}fold\".format(n_fold), \"models\")\n",
    "if(not os.path.isdir(modelPath)):\n",
    "    os.makedirs(modelPath)\n",
    "\n",
    "i = -1\n",
    "for fold in folds:\n",
    "    i += 1\n",
    "    \n",
    "    print(\"\\nTrain/Test model on Fold #\"+str(i)+\".\")\n",
    "    \n",
    "    model = DLNN_CORENup(input_seq_shape = input_seq_shape)\n",
    "    \n",
    "    ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    modelCallbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
    "                                           monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                           save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "    ]\n",
    "    \n",
    "    # adding random shuffling of the dataset for training purpose\n",
    "    index_arr = np.arange(fold[\"X_train\"].shape[0])\n",
    "    index_arr = np.random.permutation(index_arr)\n",
    "    \n",
    "    model.fit(x = fold[\"X_train\"][index_arr], y = fold[\"y_train\"][index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "              callbacks = modelCallbacks, validation_data = (fold[\"X_test\"], fold[\"y_test\"]))\n",
    "    \n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Prediction and metrics for TRAIN dataset\n",
    "    ##################################################################################\n",
    "\n",
    "    y_pred = model.predict(fold[\"X_train\"])\n",
    "    label_pred = pred2label(y_pred)\n",
    "    \n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "    prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "    mcc = matthews_corrcoef(fold[\"y_train\"], label_pred)\n",
    "\n",
    "    conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "    auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "    \n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Train\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Prediction and metrics for TEST dataset\n",
    "    ##################################################################################\n",
    "\n",
    "    y_pred = model.predict(fold[\"X_test\"])\n",
    "    label_pred = pred2label(y_pred)\n",
    "    \n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "    prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "    mcc = matthews_corrcoef(fold[\"y_test\"], label_pred)\n",
    "\n",
    "    conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "    auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "    \n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Test\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.889545</td>\n",
       "      <td>0.882330</td>\n",
       "      <td>0.950555</td>\n",
       "      <td>0.893221</td>\n",
       "      <td>0.886045</td>\n",
       "      <td>0.779274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.989958</td>\n",
       "      <td>0.990168</td>\n",
       "      <td>0.999280</td>\n",
       "      <td>0.989276</td>\n",
       "      <td>0.990610</td>\n",
       "      <td>0.979915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Test        0.889545   0.882330  0.950555     0.893221     0.886045  0.779274\n",
       "Train       0.989958   0.990168  0.999280     0.989276     0.990610  0.979915"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>[0.0, 0.002232142857142857, 0.3147321428571428...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0021321961620469083, 0.00213...</td>\n",
       "      <td>[1.9999897, 0.99998975, 0.97669035, 0.9766491,...</td>\n",
       "      <td>0.955498</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.895522</td>\n",
       "      <td>0.770882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.898472</td>\n",
       "      <td>0.889868</td>\n",
       "      <td>[0.0, 0.0022371364653243847, 0.091722595078299...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.002132196162046908...</td>\n",
       "      <td>[1.9997444, 0.99974436, 0.9911056, 0.990776, 0...</td>\n",
       "      <td>0.956538</td>\n",
       "      <td>0.903803</td>\n",
       "      <td>0.893390</td>\n",
       "      <td>0.796994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.887555</td>\n",
       "      <td>0.865957</td>\n",
       "      <td>[0.0, 0.0022371364653243847, 0.156599552572706...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0021321961620469083, 0.00213...</td>\n",
       "      <td>[1.9998047, 0.9998048, 0.98289615, 0.9827208, ...</td>\n",
       "      <td>0.938119</td>\n",
       "      <td>0.910515</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.776229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.883188</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>[0.0, 0.002232142857142857, 0.2455357142857142...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.002136752136752137, 0.002136...</td>\n",
       "      <td>[1.9999865, 0.9999865, 0.9845069, 0.9840381, 0...</td>\n",
       "      <td>0.952495</td>\n",
       "      <td>0.883929</td>\n",
       "      <td>0.882479</td>\n",
       "      <td>0.766314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.893013</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>[0.0, 0.002232142857142857, 0.1428571428571428...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.002136752136752137, 0.002136...</td>\n",
       "      <td>[1.998975, 0.99897504, 0.98446345, 0.9840373, ...</td>\n",
       "      <td>0.950125</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.785952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold Train_Test  Accuracy  Precision  \\\n",
       "1     0       Test  0.885496   0.888889   \n",
       "3     1       Test  0.898472   0.889868   \n",
       "5     2       Test  0.887555   0.865957   \n",
       "7     3       Test  0.883188   0.878049   \n",
       "9     4       Test  0.893013   0.888889   \n",
       "\n",
       "                                                 TPR  \\\n",
       "1  [0.0, 0.002232142857142857, 0.3147321428571428...   \n",
       "3  [0.0, 0.0022371364653243847, 0.091722595078299...   \n",
       "5  [0.0, 0.0022371364653243847, 0.156599552572706...   \n",
       "7  [0.0, 0.002232142857142857, 0.2455357142857142...   \n",
       "9  [0.0, 0.002232142857142857, 0.1428571428571428...   \n",
       "\n",
       "                                                 FPR  \\\n",
       "1  [0.0, 0.0, 0.0, 0.0021321961620469083, 0.00213...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.002132196162046908...   \n",
       "5  [0.0, 0.0, 0.0, 0.0021321961620469083, 0.00213...   \n",
       "7  [0.0, 0.0, 0.0, 0.002136752136752137, 0.002136...   \n",
       "9  [0.0, 0.0, 0.0, 0.002136752136752137, 0.002136...   \n",
       "\n",
       "                                  TPR_FPR_Thresholds       AUC  Sensitivity  \\\n",
       "1  [1.9999897, 0.99998975, 0.97669035, 0.9766491,...  0.955498     0.875000   \n",
       "3  [1.9997444, 0.99974436, 0.9911056, 0.990776, 0...  0.956538     0.903803   \n",
       "5  [1.9998047, 0.9998048, 0.98289615, 0.9827208, ...  0.938119     0.910515   \n",
       "7  [1.9999865, 0.9999865, 0.9845069, 0.9840381, 0...  0.952495     0.883929   \n",
       "9  [1.998975, 0.99897504, 0.98446345, 0.9840373, ...  0.950125     0.892857   \n",
       "\n",
       "   Specificity       MCC  \n",
       "1     0.895522  0.770882  \n",
       "3     0.893390  0.796994  \n",
       "5     0.865672  0.776229  \n",
       "7     0.882479  0.766314  \n",
       "9     0.893162  0.785952  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df[evaluations_df[\"Train_Test\"] == \"Test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent data testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-fold Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of each k-fold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.658122</td>\n",
       "      <td>0.26352</td>\n",
       "      <td>0.667463</td>\n",
       "      <td>0.590148</td>\n",
       "      <td>0.671624</td>\n",
       "      <td>0.201645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.658122    0.26352  0.667463     0.590148     0.671624  0.201645"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    label_pred = pred2label(y_pred)\n",
    "\n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(indpe_labels, label_pred)\n",
    "    prec = precision_score(indpe_labels,label_pred)\n",
    "    mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "    conf = confusion_matrix(indpe_labels, label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(indpe_labels, y_pred)\n",
    "    auc = roc_auc_score(indpe_labels, y_pred)\n",
    "\n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.678367</td>\n",
       "      <td>0.275294</td>\n",
       "      <td>[0.0, 0.0, 0.014778325123152709, 0.01477832512...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.000978473581213...</td>\n",
       "      <td>[1.9999896, 0.9999896, 0.999828, 0.99949074, 0...</td>\n",
       "      <td>0.665333</td>\n",
       "      <td>0.576355</td>\n",
       "      <td>0.698630</td>\n",
       "      <td>0.214804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.672653</td>\n",
       "      <td>0.280973</td>\n",
       "      <td>[0.0, 0.0, 0.009852216748768473, 0.00985221674...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.000978473581213...</td>\n",
       "      <td>[1.9996637, 0.9996637, 0.99943346, 0.999353, 0...</td>\n",
       "      <td>0.689120</td>\n",
       "      <td>0.625616</td>\n",
       "      <td>0.681996</td>\n",
       "      <td>0.237038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.648980</td>\n",
       "      <td>0.252723</td>\n",
       "      <td>[0.0, 0.0, 0.0049261083743842365, 0.0049261083...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.000978473581213...</td>\n",
       "      <td>[1.9996042, 0.9996043, 0.9995111, 0.998442, 0....</td>\n",
       "      <td>0.660291</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>0.181142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.659592</td>\n",
       "      <td>0.266376</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.014778325123152709, 0.014778...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.001956947162426...</td>\n",
       "      <td>[1.9999964, 0.9999964, 0.9998448, 0.9994086, 0...</td>\n",
       "      <td>0.675137</td>\n",
       "      <td>0.600985</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.209199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.631020</td>\n",
       "      <td>0.242236</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.014778325123152709, 0.014778...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.001956947162426...</td>\n",
       "      <td>[1.9993458, 0.99934584, 0.99909437, 0.9975636,...</td>\n",
       "      <td>0.647431</td>\n",
       "      <td>0.576355</td>\n",
       "      <td>0.641879</td>\n",
       "      <td>0.166042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold   Train_Test  Accuracy  Precision  \\\n",
       "0     0  Independent  0.678367   0.275294   \n",
       "1     1  Independent  0.672653   0.280973   \n",
       "2     2  Independent  0.648980   0.252723   \n",
       "3     3  Independent  0.659592   0.266376   \n",
       "4     4  Independent  0.631020   0.242236   \n",
       "\n",
       "                                                 TPR  \\\n",
       "0  [0.0, 0.0, 0.014778325123152709, 0.01477832512...   \n",
       "1  [0.0, 0.0, 0.009852216748768473, 0.00985221674...   \n",
       "2  [0.0, 0.0, 0.0049261083743842365, 0.0049261083...   \n",
       "3  [0.0, 0.0, 0.0, 0.014778325123152709, 0.014778...   \n",
       "4  [0.0, 0.0, 0.0, 0.014778325123152709, 0.014778...   \n",
       "\n",
       "                                                 FPR  \\\n",
       "0  [0.0, 0.0009784735812133072, 0.000978473581213...   \n",
       "1  [0.0, 0.0009784735812133072, 0.000978473581213...   \n",
       "2  [0.0, 0.0009784735812133072, 0.000978473581213...   \n",
       "3  [0.0, 0.0009784735812133072, 0.001956947162426...   \n",
       "4  [0.0, 0.0009784735812133072, 0.001956947162426...   \n",
       "\n",
       "                                  TPR_FPR_Thresholds       AUC  Sensitivity  \\\n",
       "0  [1.9999896, 0.9999896, 0.999828, 0.99949074, 0...  0.665333     0.576355   \n",
       "1  [1.9996637, 0.9996637, 0.99943346, 0.999353, 0...  0.689120     0.625616   \n",
       "2  [1.9996042, 0.9996043, 0.9995111, 0.998442, 0....  0.660291     0.571429   \n",
       "3  [1.9999964, 0.9999964, 0.9998448, 0.9994086, 0...  0.675137     0.600985   \n",
       "4  [1.9993458, 0.99934584, 0.99909437, 0.9975636,...  0.647431     0.576355   \n",
       "\n",
       "   Specificity       MCC  \n",
       "0     0.698630  0.214804  \n",
       "1     0.681996  0.237038  \n",
       "2     0.664384  0.181142  \n",
       "3     0.671233  0.209199  \n",
       "4     0.641879  0.166042  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean score with k-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.669388</td>\n",
       "      <td>0.272523</td>\n",
       "      <td>0.675513</td>\n",
       "      <td>0.596059</td>\n",
       "      <td>0.683953</td>\n",
       "      <td>0.216587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.669388   0.272523  0.675513     0.596059     0.683953  0.216587"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "total_pred = np.zeros(indpe_labels.shape)\n",
    "all_preds = []\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    total_pred += y_pred\n",
    "    all_preds.append(y_pred)\n",
    "    \n",
    "total_pred = total_pred / n_fold\n",
    "label_pred = pred2label(total_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, total_pred)\n",
    "auc = roc_auc_score(indpe_labels, total_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting score with k-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.672653</td>\n",
       "      <td>0.276018</td>\n",
       "      <td>0.661904</td>\n",
       "      <td>0.600985</td>\n",
       "      <td>0.686888</td>\n",
       "      <td>0.222886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.672653   0.276018  0.661904     0.600985     0.686888  0.222886"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "total_pred = np.zeros(indpe_labels.shape)\n",
    "all_preds = []\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    vote_pred = pred2label(y_pred)\n",
    "    total_pred += vote_pred\n",
    "    all_preds.append(vote_pred)\n",
    "    \n",
    "total_pred = total_pred / n_fold\n",
    "label_pred = pred2label(total_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, total_pred)\n",
    "auc = roc_auc_score(indpe_labels, total_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using New Model\n",
    "\n",
    "Train model on full data from training. Predict and evaluate on Independent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 1.3509 - accuracy: 0.5357\n",
      "Epoch 1: val_loss improved from inf to 1.20870, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 4s 21ms/step - loss: 1.3509 - accuracy: 0.5357 - val_loss: 1.2087 - val_accuracy: 0.7673\n",
      "Epoch 2/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 1.1599 - accuracy: 0.6250\n",
      "Epoch 2: val_loss improved from 1.20870 to 1.13297, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.1562 - accuracy: 0.6265 - val_loss: 1.1330 - val_accuracy: 0.6163\n",
      "Epoch 3/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 1.0105 - accuracy: 0.6898\n",
      "Epoch 3: val_loss improved from 1.13297 to 1.07713, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 1.0087 - accuracy: 0.6896 - val_loss: 1.0771 - val_accuracy: 0.6237\n",
      "Epoch 4/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.9205 - accuracy: 0.7002\n",
      "Epoch 4: val_loss improved from 1.07713 to 1.02960, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.9199 - accuracy: 0.7012 - val_loss: 1.0296 - val_accuracy: 0.6024\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.8647 - accuracy: 0.7086\n",
      "Epoch 5: val_loss improved from 1.02960 to 1.00822, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.8647 - accuracy: 0.7086 - val_loss: 1.0082 - val_accuracy: 0.5837\n",
      "Epoch 6/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.8099 - accuracy: 0.7225\n",
      "Epoch 6: val_loss did not improve from 1.00822\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.8092 - accuracy: 0.7228 - val_loss: 1.0533 - val_accuracy: 0.5429\n",
      "Epoch 7/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.7719 - accuracy: 0.7299\n",
      "Epoch 7: val_loss did not improve from 1.00822\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.7725 - accuracy: 0.7278 - val_loss: 1.0236 - val_accuracy: 0.5543\n",
      "Epoch 8/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.7457 - accuracy: 0.7341\n",
      "Epoch 8: val_loss improved from 1.00822 to 0.94298, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.7448 - accuracy: 0.7337 - val_loss: 0.9430 - val_accuracy: 0.5698\n",
      "Epoch 9/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.7113 - accuracy: 0.7390\n",
      "Epoch 9: val_loss did not improve from 0.94298\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.7098 - accuracy: 0.7396 - val_loss: 0.9463 - val_accuracy: 0.5682\n",
      "Epoch 10/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.6974 - accuracy: 0.7320\n",
      "Epoch 10: val_loss improved from 0.94298 to 0.93391, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.6957 - accuracy: 0.7315 - val_loss: 0.9339 - val_accuracy: 0.5690\n",
      "Epoch 11/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.6655 - accuracy: 0.7502\n",
      "Epoch 11: val_loss improved from 0.93391 to 0.87704, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 11ms/step - loss: 0.6661 - accuracy: 0.7501 - val_loss: 0.8770 - val_accuracy: 0.5853\n",
      "Epoch 12/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.6536 - accuracy: 0.7493\n",
      "Epoch 12: val_loss did not improve from 0.87704\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6537 - accuracy: 0.7485 - val_loss: 0.8931 - val_accuracy: 0.5853\n",
      "Epoch 13/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.6381 - accuracy: 0.7531\n",
      "Epoch 13: val_loss did not improve from 0.87704\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6372 - accuracy: 0.7538 - val_loss: 0.8809 - val_accuracy: 0.5682\n",
      "Epoch 14/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.6238 - accuracy: 0.7572\n",
      "Epoch 14: val_loss improved from 0.87704 to 0.86805, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6264 - accuracy: 0.7551 - val_loss: 0.8681 - val_accuracy: 0.5820\n",
      "Epoch 15/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.6076 - accuracy: 0.7641\n",
      "Epoch 15: val_loss improved from 0.86805 to 0.78749, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6084 - accuracy: 0.7636 - val_loss: 0.7875 - val_accuracy: 0.6653\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.6011 - accuracy: 0.7690\n",
      "Epoch 16: val_loss did not improve from 0.78749\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.6011 - accuracy: 0.7690 - val_loss: 0.8488 - val_accuracy: 0.6163\n",
      "Epoch 17/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.5895 - accuracy: 0.7702\n",
      "Epoch 17: val_loss did not improve from 0.78749\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.5911 - accuracy: 0.7697 - val_loss: 0.8848 - val_accuracy: 0.5551\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.5826 - accuracy: 0.7673\n",
      "Epoch 18: val_loss improved from 0.78749 to 0.75300, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.5826 - accuracy: 0.7673 - val_loss: 0.7530 - val_accuracy: 0.6784\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.5699 - accuracy: 0.7769\n",
      "Epoch 19: val_loss improved from 0.75300 to 0.73502, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.5699 - accuracy: 0.7769 - val_loss: 0.7350 - val_accuracy: 0.6865\n",
      "Epoch 20/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.5539 - accuracy: 0.7875\n",
      "Epoch 20: val_loss did not improve from 0.73502\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5536 - accuracy: 0.7876 - val_loss: 0.9030 - val_accuracy: 0.5624\n",
      "Epoch 21/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.5501 - accuracy: 0.7857\n",
      "Epoch 21: val_loss did not improve from 0.73502\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5501 - accuracy: 0.7848 - val_loss: 0.7460 - val_accuracy: 0.6890\n",
      "Epoch 22/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.5360 - accuracy: 0.7978\n",
      "Epoch 22: val_loss did not improve from 0.73502\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5365 - accuracy: 0.7983 - val_loss: 0.8534 - val_accuracy: 0.6082\n",
      "Epoch 23/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.5340 - accuracy: 0.7964\n",
      "Epoch 23: val_loss did not improve from 0.73502\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5330 - accuracy: 0.7974 - val_loss: 0.8331 - val_accuracy: 0.6310\n",
      "Epoch 24/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.5242 - accuracy: 0.7986\n",
      "Epoch 24: val_loss did not improve from 0.73502\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5240 - accuracy: 0.7987 - val_loss: 0.8216 - val_accuracy: 0.6522\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.5227 - accuracy: 0.8031\n",
      "Epoch 25: val_loss did not improve from 0.73502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5227 - accuracy: 0.8031 - val_loss: 0.8603 - val_accuracy: 0.6237\n",
      "Epoch 26/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.5135 - accuracy: 0.8083\n",
      "Epoch 26: val_loss did not improve from 0.73502\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.5130 - accuracy: 0.8081 - val_loss: 0.9464 - val_accuracy: 0.5665\n",
      "Epoch 27/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.8178\n",
      "Epoch 27: val_loss improved from 0.73502 to 0.73260, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2_mySOTA\\5fold\\models\\_fullModel.hdf5\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4983 - accuracy: 0.8173 - val_loss: 0.7326 - val_accuracy: 0.7020\n",
      "Epoch 28/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.5001 - accuracy: 0.8136\n",
      "Epoch 28: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4992 - accuracy: 0.8138 - val_loss: 0.7616 - val_accuracy: 0.6816\n",
      "Epoch 29/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.4920 - accuracy: 0.8148\n",
      "Epoch 29: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4923 - accuracy: 0.8145 - val_loss: 0.7597 - val_accuracy: 0.6686\n",
      "Epoch 30/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.4886 - accuracy: 0.8169\n",
      "Epoch 30: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4875 - accuracy: 0.8177 - val_loss: 0.9447 - val_accuracy: 0.5902\n",
      "Epoch 31/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.4807 - accuracy: 0.8225\n",
      "Epoch 31: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4800 - accuracy: 0.8223 - val_loss: 0.8424 - val_accuracy: 0.6376\n",
      "Epoch 32/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.4713 - accuracy: 0.8270\n",
      "Epoch 32: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4702 - accuracy: 0.8275 - val_loss: 0.7824 - val_accuracy: 0.6735\n",
      "Epoch 33/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.4559 - accuracy: 0.8426\n",
      "Epoch 33: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4534 - accuracy: 0.8439 - val_loss: 0.7985 - val_accuracy: 0.6514\n",
      "Epoch 34/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.4691 - accuracy: 0.8383\n",
      "Epoch 34: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4702 - accuracy: 0.8356 - val_loss: 0.7808 - val_accuracy: 0.6833\n",
      "Epoch 35/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.4644 - accuracy: 0.8314\n",
      "Epoch 35: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4630 - accuracy: 0.8324 - val_loss: 0.9613 - val_accuracy: 0.5788\n",
      "Epoch 36/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.4396 - accuracy: 0.8431\n",
      "Epoch 36: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4410 - accuracy: 0.8422 - val_loss: 0.7982 - val_accuracy: 0.6637\n",
      "Epoch 37/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.4383 - accuracy: 0.8493\n",
      "Epoch 37: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4391 - accuracy: 0.8487 - val_loss: 0.9090 - val_accuracy: 0.6171\n",
      "Epoch 38/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.4447 - accuracy: 0.8435\n",
      "Epoch 38: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4411 - accuracy: 0.8465 - val_loss: 0.7423 - val_accuracy: 0.6963\n",
      "Epoch 39/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.4330 - accuracy: 0.8527\n",
      "Epoch 39: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4336 - accuracy: 0.8533 - val_loss: 0.8142 - val_accuracy: 0.6580\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.4230 - accuracy: 0.8492\n",
      "Epoch 40: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4230 - accuracy: 0.8492 - val_loss: 0.8567 - val_accuracy: 0.6490\n",
      "Epoch 41/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.4139 - accuracy: 0.8628\n",
      "Epoch 41: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.4157 - accuracy: 0.8609 - val_loss: 0.8062 - val_accuracy: 0.6710\n",
      "Epoch 42/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.4110 - accuracy: 0.8591\n",
      "Epoch 42: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4124 - accuracy: 0.8585 - val_loss: 0.9856 - val_accuracy: 0.5673\n",
      "Epoch 43/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.4060 - accuracy: 0.8625\n",
      "Epoch 43: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4102 - accuracy: 0.8607 - val_loss: 0.7770 - val_accuracy: 0.6816\n",
      "Epoch 44/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3986 - accuracy: 0.8616\n",
      "Epoch 44: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4001 - accuracy: 0.8603 - val_loss: 0.8838 - val_accuracy: 0.6457\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.4112 - accuracy: 0.8644\n",
      "Epoch 45: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4112 - accuracy: 0.8644 - val_loss: 0.8100 - val_accuracy: 0.6710\n",
      "Epoch 46/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.4041 - accuracy: 0.8635\n",
      "Epoch 46: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.4046 - accuracy: 0.8636 - val_loss: 0.8261 - val_accuracy: 0.6514\n",
      "Epoch 47/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.3910 - accuracy: 0.8757\n",
      "Epoch 47: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3906 - accuracy: 0.8762 - val_loss: 0.9553 - val_accuracy: 0.6441\n",
      "Epoch 48/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3898 - accuracy: 0.8714\n",
      "Epoch 48: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3901 - accuracy: 0.8714 - val_loss: 0.8846 - val_accuracy: 0.6416\n",
      "Epoch 49/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3837 - accuracy: 0.8737\n",
      "Epoch 49: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3842 - accuracy: 0.8732 - val_loss: 0.9900 - val_accuracy: 0.6090\n",
      "Epoch 50/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3751 - accuracy: 0.8775\n",
      "Epoch 50: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3767 - accuracy: 0.8769 - val_loss: 1.0883 - val_accuracy: 0.5943\n",
      "Epoch 51/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3740 - accuracy: 0.8775\n",
      "Epoch 51: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3754 - accuracy: 0.8769 - val_loss: 0.8991 - val_accuracy: 0.6563\n",
      "Epoch 52/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3950 - accuracy: 0.8650\n",
      "Epoch 52: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3932 - accuracy: 0.8662 - val_loss: 0.9529 - val_accuracy: 0.6343\n",
      "Epoch 53/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3756 - accuracy: 0.8842\n",
      "Epoch 53: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3749 - accuracy: 0.8845 - val_loss: 0.8727 - val_accuracy: 0.6661\n",
      "Epoch 54/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.3730 - accuracy: 0.8789\n",
      "Epoch 54: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3727 - accuracy: 0.8791 - val_loss: 0.9611 - val_accuracy: 0.6424\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/72 [===========================>..] - ETA: 0s - loss: 0.3778 - accuracy: 0.8791\n",
      "Epoch 55: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3759 - accuracy: 0.8806 - val_loss: 0.8344 - val_accuracy: 0.6857\n",
      "Epoch 56/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3874 - accuracy: 0.8794\n",
      "Epoch 56: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3883 - accuracy: 0.8791 - val_loss: 0.8917 - val_accuracy: 0.6718\n",
      "Epoch 57/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3650 - accuracy: 0.8897\n",
      "Epoch 57: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3664 - accuracy: 0.8895 - val_loss: 0.8795 - val_accuracy: 0.6678\n",
      "Epoch 58/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3545 - accuracy: 0.8862\n",
      "Epoch 58: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3554 - accuracy: 0.8856 - val_loss: 0.9222 - val_accuracy: 0.6661\n",
      "Epoch 59/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.3627 - accuracy: 0.8897\n",
      "Epoch 59: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3608 - accuracy: 0.8902 - val_loss: 0.9197 - val_accuracy: 0.6661\n",
      "Epoch 60/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3527 - accuracy: 0.8911\n",
      "Epoch 60: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3523 - accuracy: 0.8913 - val_loss: 0.8795 - val_accuracy: 0.6735\n",
      "Epoch 61/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.3658 - accuracy: 0.8920\n",
      "Epoch 61: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3649 - accuracy: 0.8917 - val_loss: 0.8491 - val_accuracy: 0.6800\n",
      "Epoch 62/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3594 - accuracy: 0.8930\n",
      "Epoch 62: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3587 - accuracy: 0.8937 - val_loss: 0.9259 - val_accuracy: 0.6441\n",
      "Epoch 63/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.3589 - accuracy: 0.8895\n",
      "Epoch 63: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3596 - accuracy: 0.8891 - val_loss: 0.8619 - val_accuracy: 0.6841\n",
      "Epoch 64/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3586 - accuracy: 0.8871\n",
      "Epoch 64: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3549 - accuracy: 0.8898 - val_loss: 0.8330 - val_accuracy: 0.6947\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.8937\n",
      "Epoch 65: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3481 - accuracy: 0.8937 - val_loss: 0.8768 - val_accuracy: 0.6710\n",
      "Epoch 66/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8908\n",
      "Epoch 66: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3478 - accuracy: 0.8928 - val_loss: 0.8914 - val_accuracy: 0.6849\n",
      "Epoch 67/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3480 - accuracy: 0.8917\n",
      "Epoch 67: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3474 - accuracy: 0.8926 - val_loss: 0.8599 - val_accuracy: 0.6808\n",
      "Epoch 68/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3345 - accuracy: 0.9025\n",
      "Epoch 68: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3346 - accuracy: 0.9024 - val_loss: 0.9360 - val_accuracy: 0.6629\n",
      "Epoch 69/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.3355 - accuracy: 0.9013\n",
      "Epoch 69: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3362 - accuracy: 0.9033 - val_loss: 0.9768 - val_accuracy: 0.6392\n",
      "Epoch 70/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3487 - accuracy: 0.8939\n",
      "Epoch 70: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3472 - accuracy: 0.8948 - val_loss: 0.8856 - val_accuracy: 0.6759\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.9048\n",
      "Epoch 71: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3297 - accuracy: 0.9048 - val_loss: 1.0340 - val_accuracy: 0.6327\n",
      "Epoch 72/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3346 - accuracy: 0.8992\n",
      "Epoch 72: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3363 - accuracy: 0.8989 - val_loss: 0.9296 - val_accuracy: 0.6612\n",
      "Epoch 73/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3379 - accuracy: 0.8999\n",
      "Epoch 73: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3375 - accuracy: 0.8998 - val_loss: 1.1480 - val_accuracy: 0.6082\n",
      "Epoch 74/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3321 - accuracy: 0.8988\n",
      "Epoch 74: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3370 - accuracy: 0.8965 - val_loss: 0.9006 - val_accuracy: 0.6784\n",
      "Epoch 75/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.8984\n",
      "Epoch 75: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3347 - accuracy: 0.8989 - val_loss: 0.9517 - val_accuracy: 0.6678\n",
      "Epoch 76/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3347 - accuracy: 0.9014\n",
      "Epoch 76: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3331 - accuracy: 0.9015 - val_loss: 1.0054 - val_accuracy: 0.6498\n",
      "Epoch 77/100\n",
      "70/72 [============================>.] - ETA: 0s - loss: 0.3339 - accuracy: 0.9022\n",
      "Epoch 77: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3346 - accuracy: 0.9022 - val_loss: 0.9742 - val_accuracy: 0.6637\n",
      "Epoch 78/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3210 - accuracy: 0.9109\n",
      "Epoch 78: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3191 - accuracy: 0.9118 - val_loss: 1.0485 - val_accuracy: 0.6416\n",
      "Epoch 79/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.3185 - accuracy: 0.9111\n",
      "Epoch 79: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3196 - accuracy: 0.9103 - val_loss: 0.9979 - val_accuracy: 0.6612\n",
      "Epoch 80/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3202 - accuracy: 0.9065\n",
      "Epoch 80: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3201 - accuracy: 0.9055 - val_loss: 0.9955 - val_accuracy: 0.6645\n",
      "Epoch 81/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3185 - accuracy: 0.9137\n",
      "Epoch 81: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3192 - accuracy: 0.9133 - val_loss: 1.0207 - val_accuracy: 0.6571\n",
      "Epoch 82/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3236 - accuracy: 0.9049\n",
      "Epoch 82: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3232 - accuracy: 0.9044 - val_loss: 0.9799 - val_accuracy: 0.6629\n",
      "Epoch 83/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.9107\n",
      "Epoch 83: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3137 - accuracy: 0.9103 - val_loss: 1.0777 - val_accuracy: 0.6376\n",
      "Epoch 84/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.3195 - accuracy: 0.9067\n",
      "Epoch 84: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3204 - accuracy: 0.9068 - val_loss: 1.0142 - val_accuracy: 0.6653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3159 - accuracy: 0.9083\n",
      "Epoch 85: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 9ms/step - loss: 0.3177 - accuracy: 0.9070 - val_loss: 0.8665 - val_accuracy: 0.6922\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.9155\n",
      "Epoch 86: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3100 - accuracy: 0.9155 - val_loss: 1.0129 - val_accuracy: 0.6629\n",
      "Epoch 87/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3100 - accuracy: 0.9126\n",
      "Epoch 87: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3094 - accuracy: 0.9131 - val_loss: 0.9797 - val_accuracy: 0.6792\n",
      "Epoch 88/100\n",
      "68/72 [===========================>..] - ETA: 0s - loss: 0.3112 - accuracy: 0.9150\n",
      "Epoch 88: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3113 - accuracy: 0.9151 - val_loss: 0.8727 - val_accuracy: 0.7053\n",
      "Epoch 89/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3024 - accuracy: 0.9169\n",
      "Epoch 89: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3026 - accuracy: 0.9168 - val_loss: 1.0421 - val_accuracy: 0.6416\n",
      "Epoch 90/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.2985 - accuracy: 0.9164\n",
      "Epoch 90: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.2973 - accuracy: 0.9173 - val_loss: 0.9254 - val_accuracy: 0.6906\n",
      "Epoch 91/100\n",
      "66/72 [==========================>...] - ETA: 0s - loss: 0.3049 - accuracy: 0.9176\n",
      "Epoch 91: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3049 - accuracy: 0.9168 - val_loss: 0.9081 - val_accuracy: 0.6849\n",
      "Epoch 92/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3012 - accuracy: 0.9149\n",
      "Epoch 92: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3041 - accuracy: 0.9133 - val_loss: 0.9067 - val_accuracy: 0.6922\n",
      "Epoch 93/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3078 - accuracy: 0.9158\n",
      "Epoch 93: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3065 - accuracy: 0.9164 - val_loss: 1.0973 - val_accuracy: 0.6449\n",
      "Epoch 94/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3126 - accuracy: 0.9125\n",
      "Epoch 94: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3120 - accuracy: 0.9118 - val_loss: 1.1044 - val_accuracy: 0.6604\n",
      "Epoch 95/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3004 - accuracy: 0.9179\n",
      "Epoch 95: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3013 - accuracy: 0.9170 - val_loss: 1.0752 - val_accuracy: 0.6620\n",
      "Epoch 96/100\n",
      "71/72 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.9173\n",
      "Epoch 96: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3031 - accuracy: 0.9173 - val_loss: 0.9462 - val_accuracy: 0.6824\n",
      "Epoch 97/100\n",
      "69/72 [===========================>..] - ETA: 0s - loss: 0.3103 - accuracy: 0.9187\n",
      "Epoch 97: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3081 - accuracy: 0.9197 - val_loss: 0.9653 - val_accuracy: 0.6718\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.9101\n",
      "Epoch 98: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3171 - accuracy: 0.9101 - val_loss: 1.0056 - val_accuracy: 0.6653\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.9105\n",
      "Epoch 99: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3141 - accuracy: 0.9105 - val_loss: 0.9281 - val_accuracy: 0.6947\n",
      "Epoch 100/100\n",
      "67/72 [==========================>...] - ETA: 0s - loss: 0.3085 - accuracy: 0.9170\n",
      "Epoch 100: val_loss did not improve from 0.73260\n",
      "72/72 [==============================] - 1s 10ms/step - loss: 0.3096 - accuracy: 0.9164 - val_loss: 1.0689 - val_accuracy: 0.6506\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "model = DLNN_CORENup(input_seq_shape = input_seq_shape,\n",
    "                     metrics='accuracy')\n",
    "    \n",
    "## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "current_model_path = os.path.join(modelPath, \"_fullModel.hdf5\")\n",
    "modelCallbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
    "                                       monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                       save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "]\n",
    "\n",
    "# adding random shuffling of the dataset for training purpose\n",
    "index_arr = np.arange(train_features.shape[0])\n",
    "index_arr = np.random.permutation(index_arr)\n",
    "\n",
    "model.fit(x = train_features[index_arr], y = train_labels[index_arr], \n",
    "          batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "          callbacks = modelCallbacks, validation_data = (indpe_features, indpe_labels))\n",
    "# model.fit(x = train_features[index_arr], y = train_labels[index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "#           callbacks = modelCallbacks, validation_split = 0.2)\n",
    "\n",
    "model = tf.keras.models.load_model(current_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.702041</td>\n",
       "      <td>0.279891</td>\n",
       "      <td>0.672354</td>\n",
       "      <td>0.507389</td>\n",
       "      <td>0.740705</td>\n",
       "      <td>0.201222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.702041   0.279891  0.672354     0.507389     0.740705  0.201222"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "y_pred = model.predict(indpe_features)\n",
    "label_pred = pred2label(y_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, y_pred)\n",
    "auc = roc_auc_score(indpe_labels, y_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after balancing the augmentation - large n/w - factor 3\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.68898\t0.265789\t0.680907\t0.497537\t0.727006\t0.18049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before balancing - large n/w - factor 3\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.733878\t0.273063\t0.666128\t0.364532\t0.807241\t0.153875\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.715918\t0.280967\t0.673619\t0.458128\t0.767123\t0.188607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.74      0.81      1022\n",
      "           1       0.28      0.51      0.36       203\n",
      "\n",
      "    accuracy                           0.70      1225\n",
      "   macro avg       0.58      0.62      0.58      1225\n",
      "weighted avg       0.78      0.70      0.73      1225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(indpe_labels, np.round(y_pred).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after balancing the augmentation - large n/w - factor 3\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.73      0.80      1022\n",
    "#            1       0.27      0.50      0.35       203\n",
    "\n",
    "#     accuracy                           0.69      1225\n",
    "#    macro avg       0.57      0.61      0.57      1225\n",
    "# weighted avg       0.78      0.69      0.72      1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before balancing - large n/w - factor 3\n",
    "\n",
    "# precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.75      0.81      1022\n",
    "#            1       0.29      0.50      0.37       203\n",
    "\n",
    "#     accuracy                           0.71      1225\n",
    "#    macro avg       0.59      0.63      0.59      1225\n",
    "# weighted avg       0.79      0.71      0.74      1225\n",
    "\n",
    "# precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.86      0.81      0.84      1022\n",
    "#            1       0.27      0.36      0.31       203\n",
    "\n",
    "#     accuracy                           0.73      1225\n",
    "#    macro avg       0.57      0.59      0.57      1225\n",
    "# weighted avg       0.77      0.73      0.75      1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 368)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(indpe_labels), np.sum(np.round(y_pred).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (203, 331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
