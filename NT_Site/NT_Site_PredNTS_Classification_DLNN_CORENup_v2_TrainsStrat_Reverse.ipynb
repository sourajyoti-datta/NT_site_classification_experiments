{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 5\n",
    "expName = \"NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\"\n",
    "outPath = \"Results\"\n",
    "foldName = \"folds.pickle\"\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "seed = None\n",
    "\n",
    "input_data_folder = \"Data\"\n",
    "training_data_file = \"Training-datasets-PredNTS.txt\"\n",
    "independent_data_file = \"independent dataset-PredNTS.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.is_gpu_available(cuda_only=True))\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define all CUSTOM functions\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_encode_nt(sequence, char_dict):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),len(char_dict)))\n",
    "    \n",
    "    i = 0\n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in char_dict.keys()):\n",
    "            seq_encoded[i][char_dict[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            raise ValueError('Incorrect character in NT sequence: '+sequence)\n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Build k-fold functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(y_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to customize the DLNN architecture with parameters\n",
    "##################################################################################\n",
    "\n",
    "def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "                 conv_filters_per_layer_1 = 10, kernel_length_1 = 10, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "                 max_pool_width_1 = 3, max_pool_stride_1 = 3, ## 1st Maxpool layer parameters\n",
    "                 lstm_decode_units = 25, ## LSTM layer parameters\n",
    "                 conv_filters_per_layer_2 = 10,  kernel_length_2 = 5, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "                 max_pool_width_2 = 3, max_pool_stride_2 = 3, ## 2nd Maxpool layer parameters\n",
    "                 dense_decode_units = 128, ## Dense layer parameters\n",
    "                 prob = 0.5, learn_rate = 0.0005, \n",
    "                 loss = 'binary_crossentropy', metrics = 'accuracy'):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  SEQUENCE  ##################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "                                strides = conv_strides_1, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = \"same\")(input1)\n",
    "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "    x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "    ## Conv Path\n",
    "\n",
    "    x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, strides = conv_strides_2, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), padding = 'same')(x1)\n",
    "    x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "    x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "    x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  Classifier  ################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    y = tf.keras.layers.Dense(dense_decode_units, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'relu'\n",
    "                             )(x4)\n",
    "    \n",
    "    y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(1, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'sigmoid')(y)\n",
    "\n",
    "    ## Generate Model from input and output\n",
    "    model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 41, 21)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 41, 10)       2110        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 41, 10)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 13, 10)       0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 13, 10)       0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 13, 10)       510         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 13, 10)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 13, 25)       3600        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 4, 10)       0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 13, 25)       0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4, 10)        0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 325)          0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 40)           0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 365)          0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          46848       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 53,197\n",
      "Trainable params: 53,197\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DLNN_CORENup().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### read training file\n",
    "##################################################################################\n",
    "train_file_path = os.path.join(input_data_folder, training_data_file)\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t', header=None)\n",
    "train_data.columns = ['Sequence', 'name', 'id', 'flag', 'label_original', 'type']\n",
    "train_data.head()\n",
    "\n",
    "##################################################################################\n",
    "##### Create dictionary of all characters in the NT sequence \n",
    "##################################################################################\n",
    "all_char_set = set({})\n",
    "for val in [set(val) for val in train_data['Sequence']]:\n",
    "    all_char_set = all_char_set.union(val)\n",
    "all_char_list = list(all_char_set)\n",
    "all_char_list.sort()\n",
    "all_char_dict = {}\n",
    "for i in range(len(all_char_list)):\n",
    "    all_char_dict[all_char_list[i]] = i\n",
    "    \n",
    "##################################################################################\n",
    "##### Create OHE of sequence\n",
    "##################################################################################\n",
    "train_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                        for val in train_data[\"Sequence\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Fix the labels\n",
    "##################################################################################\n",
    "train_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                 for val in train_data[\"label_original\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Extract features and labels, create folds\n",
    "##################################################################################\n",
    "\n",
    "features = np.array(list(train_data['OHE_Sequence']))\n",
    "labels = np.array(list(train_data['label']))\n",
    "labels = labels.reshape((labels.shape[0], 1))\n",
    "\n",
    "input_seq_shape = features[0].shape\n",
    "\n",
    "folds = build_kfold(features, labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "\n",
    "## Write the k-fold dataset to file\n",
    "foldPath = os.path.join(outPath, expName, \"{}fold\".format(n_fold))\n",
    "if(not os.path.isdir(foldPath)):\n",
    "    os.makedirs(foldPath)\n",
    "pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/Test model on Fold #0.\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.8946\n",
      "Epoch 1: val_loss improved from inf to 0.84374, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 5s 12ms/step - loss: 0.8946 - val_loss: 0.8437\n",
      "Epoch 2/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.8095\n",
      "Epoch 2: val_loss improved from 0.84374 to 0.71685, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.8066 - val_loss: 0.7169\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.7195\n",
      "Epoch 3: val_loss improved from 0.71685 to 0.65912, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.7195 - val_loss: 0.6591\n",
      "Epoch 4/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.6661\n",
      "Epoch 4: val_loss improved from 0.65912 to 0.61529, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6657 - val_loss: 0.6153\n",
      "Epoch 5/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.6497\n",
      "Epoch 5: val_loss improved from 0.61529 to 0.60027, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6479 - val_loss: 0.6003\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6123\n",
      "Epoch 6: val_loss improved from 0.60027 to 0.59099, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6123 - val_loss: 0.5910\n",
      "Epoch 7/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.5958\n",
      "Epoch 7: val_loss improved from 0.59099 to 0.57377, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5960 - val_loss: 0.5738\n",
      "Epoch 8/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.5912\n",
      "Epoch 8: val_loss improved from 0.57377 to 0.56420, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5905 - val_loss: 0.5642\n",
      "Epoch 9/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5789\n",
      "Epoch 9: val_loss improved from 0.56420 to 0.55992, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5788 - val_loss: 0.5599\n",
      "Epoch 10/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5695\n",
      "Epoch 10: val_loss improved from 0.55992 to 0.55001, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5695 - val_loss: 0.5500\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5605\n",
      "Epoch 11: val_loss improved from 0.55001 to 0.54341, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5605 - val_loss: 0.5434\n",
      "Epoch 12/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5528\n",
      "Epoch 12: val_loss did not improve from 0.54341\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5530 - val_loss: 0.5447\n",
      "Epoch 13/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.5509\n",
      "Epoch 13: val_loss improved from 0.54341 to 0.53932, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5506 - val_loss: 0.5393\n",
      "Epoch 14/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5450\n",
      "Epoch 14: val_loss improved from 0.53932 to 0.53820, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5436 - val_loss: 0.5382\n",
      "Epoch 15/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.5269\n",
      "Epoch 15: val_loss improved from 0.53820 to 0.53211, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5273 - val_loss: 0.5321\n",
      "Epoch 16/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.5275\n",
      "Epoch 16: val_loss improved from 0.53211 to 0.52734, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5279 - val_loss: 0.5273\n",
      "Epoch 17/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5220\n",
      "Epoch 17: val_loss improved from 0.52734 to 0.52426, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5247 - val_loss: 0.5243\n",
      "Epoch 18/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.5220\n",
      "Epoch 18: val_loss improved from 0.52426 to 0.52171, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5221 - val_loss: 0.5217\n",
      "Epoch 19/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5121\n",
      "Epoch 19: val_loss did not improve from 0.52171\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5146 - val_loss: 0.5265\n",
      "Epoch 20/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5131\n",
      "Epoch 20: val_loss did not improve from 0.52171\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.5145 - val_loss: 0.5225\n",
      "Epoch 21/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5018\n",
      "Epoch 21: val_loss improved from 0.52171 to 0.51983, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5018 - val_loss: 0.5198\n",
      "Epoch 22/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5000\n",
      "Epoch 22: val_loss improved from 0.51983 to 0.51380, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4996 - val_loss: 0.5138\n",
      "Epoch 23/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4899\n",
      "Epoch 23: val_loss did not improve from 0.51380\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4895 - val_loss: 0.5140\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4951\n",
      "Epoch 24: val_loss improved from 0.51380 to 0.51275, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4951 - val_loss: 0.5128\n",
      "Epoch 25/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4815\n",
      "Epoch 25: val_loss improved from 0.51275 to 0.50575, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4801 - val_loss: 0.5058\n",
      "Epoch 26/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4802\n",
      "Epoch 26: val_loss improved from 0.50575 to 0.50528, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4794 - val_loss: 0.5053\n",
      "Epoch 27/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4651\n",
      "Epoch 27: val_loss did not improve from 0.50528\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4649 - val_loss: 0.5127\n",
      "Epoch 28/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4749\n",
      "Epoch 28: val_loss did not improve from 0.50528\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4751 - val_loss: 0.5100\n",
      "Epoch 29/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4708\n",
      "Epoch 29: val_loss did not improve from 0.50528\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4707 - val_loss: 0.5107\n",
      "Epoch 30/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4667\n",
      "Epoch 30: val_loss improved from 0.50528 to 0.50310, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4651 - val_loss: 0.5031\n",
      "Epoch 31/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4615\n",
      "Epoch 31: val_loss did not improve from 0.50310\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4628 - val_loss: 0.5107\n",
      "Epoch 32/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4640\n",
      "Epoch 32: val_loss did not improve from 0.50310\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4620 - val_loss: 0.5033\n",
      "Epoch 33/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4539\n",
      "Epoch 33: val_loss improved from 0.50310 to 0.49862, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4516 - val_loss: 0.4986\n",
      "Epoch 34/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4477\n",
      "Epoch 34: val_loss improved from 0.49862 to 0.49561, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4472 - val_loss: 0.4956\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4451\n",
      "Epoch 35: val_loss improved from 0.49561 to 0.49427, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4451 - val_loss: 0.4943\n",
      "Epoch 36/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4507\n",
      "Epoch 36: val_loss improved from 0.49427 to 0.48831, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4508 - val_loss: 0.4883\n",
      "Epoch 37/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4483\n",
      "Epoch 37: val_loss did not improve from 0.48831\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4476 - val_loss: 0.5007\n",
      "Epoch 38/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4265\n",
      "Epoch 38: val_loss improved from 0.48831 to 0.48471, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.4266 - val_loss: 0.4847\n",
      "Epoch 39/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4381\n",
      "Epoch 39: val_loss did not improve from 0.48471\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4392 - val_loss: 0.4897\n",
      "Epoch 40/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4334\n",
      "Epoch 40: val_loss did not improve from 0.48471\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4332 - val_loss: 0.4853\n",
      "Epoch 41/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4358\n",
      "Epoch 41: val_loss did not improve from 0.48471\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4353 - val_loss: 0.4897\n",
      "Epoch 42/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4256\n",
      "Epoch 42: val_loss did not improve from 0.48471\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4266 - val_loss: 0.4960\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4269\n",
      "Epoch 43: val_loss did not improve from 0.48471\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4269 - val_loss: 0.5065\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4241\n",
      "Epoch 44: val_loss did not improve from 0.48471\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4241 - val_loss: 0.4862\n",
      "Epoch 45/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4274\n",
      "Epoch 45: val_loss improved from 0.48471 to 0.48455, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4272 - val_loss: 0.4845\n",
      "Epoch 46/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4217\n",
      "Epoch 46: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4215 - val_loss: 0.4887\n",
      "Epoch 47/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4174\n",
      "Epoch 47: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4188 - val_loss: 0.5164\n",
      "Epoch 48/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4263\n",
      "Epoch 48: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4285 - val_loss: 0.4859\n",
      "Epoch 49/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3981\n",
      "Epoch 49: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4013 - val_loss: 0.4882\n",
      "Epoch 50/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4242\n",
      "Epoch 50: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4240 - val_loss: 0.4985\n",
      "Epoch 51/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4125\n",
      "Epoch 51: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4102 - val_loss: 0.4989\n",
      "Epoch 52/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4131\n",
      "Epoch 52: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4142 - val_loss: 0.4908\n",
      "Epoch 53/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4083\n",
      "Epoch 53: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4095 - val_loss: 0.4867\n",
      "Epoch 54/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4124\n",
      "Epoch 54: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4125 - val_loss: 0.4958\n",
      "Epoch 55/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4157\n",
      "Epoch 55: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4157 - val_loss: 0.4907\n",
      "Epoch 56/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4061\n",
      "Epoch 56: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4070 - val_loss: 0.4927\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4169\n",
      "Epoch 57: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4169 - val_loss: 0.4903\n",
      "Epoch 58/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.3915\n",
      "Epoch 58: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3955 - val_loss: 0.4942\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117/120 [============================>.] - ETA: 0s - loss: 0.4095\n",
      "Epoch 59: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4101 - val_loss: 0.4930\n",
      "Epoch 60/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4034\n",
      "Epoch 60: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4042 - val_loss: 0.4876\n",
      "Epoch 61/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.3974\n",
      "Epoch 61: val_loss did not improve from 0.48455\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3952 - val_loss: 0.4881\n",
      "Epoch 62/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4077\n",
      "Epoch 62: val_loss improved from 0.48455 to 0.48384, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold0.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4083 - val_loss: 0.4838\n",
      "Epoch 63/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4037\n",
      "Epoch 63: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4026 - val_loss: 0.5037\n",
      "Epoch 64/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4014\n",
      "Epoch 64: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4031 - val_loss: 0.4923\n",
      "Epoch 65/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3995\n",
      "Epoch 65: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3990 - val_loss: 0.4917\n",
      "Epoch 66/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4055\n",
      "Epoch 66: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4064 - val_loss: 0.4904\n",
      "Epoch 67/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4108\n",
      "Epoch 67: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4116 - val_loss: 0.4902\n",
      "Epoch 68/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4002\n",
      "Epoch 68: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3977 - val_loss: 0.4925\n",
      "Epoch 69/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4090\n",
      "Epoch 69: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4081 - val_loss: 0.4981\n",
      "Epoch 70/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.3983\n",
      "Epoch 70: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3970 - val_loss: 0.5010\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3876\n",
      "Epoch 71: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3876 - val_loss: 0.5002\n",
      "Epoch 72/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.3925\n",
      "Epoch 72: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3953 - val_loss: 0.5126\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3880\n",
      "Epoch 73: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3880 - val_loss: 0.5093\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3927\n",
      "Epoch 74: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3927 - val_loss: 0.5117\n",
      "Epoch 75/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3911\n",
      "Epoch 75: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3886 - val_loss: 0.4898\n",
      "Epoch 76/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3921\n",
      "Epoch 76: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3924 - val_loss: 0.4949\n",
      "Epoch 77/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3877\n",
      "Epoch 77: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3860 - val_loss: 0.5006\n",
      "Epoch 78/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3826\n",
      "Epoch 78: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3826 - val_loss: 0.4934\n",
      "Epoch 79/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.3835\n",
      "Epoch 79: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3826 - val_loss: 0.5092\n",
      "Epoch 80/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3840\n",
      "Epoch 80: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3845 - val_loss: 0.5017\n",
      "Epoch 81/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3984\n",
      "Epoch 81: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3994 - val_loss: 0.4971\n",
      "Epoch 82/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3900\n",
      "Epoch 82: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3913 - val_loss: 0.4934\n",
      "Epoch 83/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3935\n",
      "Epoch 83: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3943 - val_loss: 0.4987\n",
      "Epoch 84/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3710\n",
      "Epoch 84: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3718 - val_loss: 0.4972\n",
      "Epoch 85/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3834\n",
      "Epoch 85: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3844 - val_loss: 0.5033\n",
      "Epoch 86/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3748\n",
      "Epoch 86: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3796 - val_loss: 0.5012\n",
      "Epoch 87/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.3848\n",
      "Epoch 87: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3874 - val_loss: 0.5069\n",
      "Epoch 88/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3786\n",
      "Epoch 88: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3791 - val_loss: 0.4972\n",
      "Epoch 89/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3817\n",
      "Epoch 89: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3796 - val_loss: 0.4966\n",
      "Epoch 90/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3747\n",
      "Epoch 90: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3749 - val_loss: 0.4905\n",
      "Epoch 91/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3773\n",
      "Epoch 91: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3771 - val_loss: 0.4929\n",
      "Epoch 92/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3882\n",
      "Epoch 92: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3888 - val_loss: 0.4968\n",
      "Epoch 93/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3736\n",
      "Epoch 93: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3735 - val_loss: 0.5149\n",
      "Epoch 94/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3847\n",
      "Epoch 94: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3853 - val_loss: 0.4974\n",
      "Epoch 95/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3765\n",
      "Epoch 95: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3752 - val_loss: 0.4992\n",
      "Epoch 96/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3706\n",
      "Epoch 96: val_loss did not improve from 0.48384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3708 - val_loss: 0.5182\n",
      "Epoch 97/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3747\n",
      "Epoch 97: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3740 - val_loss: 0.5086\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3786\n",
      "Epoch 98: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3786 - val_loss: 0.5034\n",
      "Epoch 99/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3777\n",
      "Epoch 99: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3775 - val_loss: 0.5169\n",
      "Epoch 100/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3698\n",
      "Epoch 100: val_loss did not improve from 0.48384\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3716 - val_loss: 0.5117\n",
      "\n",
      "Train/Test model on Fold #1.\n",
      "Epoch 1/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.8966\n",
      "Epoch 1: val_loss improved from inf to 0.84214, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 3s 11ms/step - loss: 0.8942 - val_loss: 0.8421\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.7916\n",
      "Epoch 2: val_loss improved from 0.84214 to 0.71866, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.7916 - val_loss: 0.7187\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.7174\n",
      "Epoch 3: val_loss improved from 0.71866 to 0.67509, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.7174 - val_loss: 0.6751\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6716\n",
      "Epoch 4: val_loss improved from 0.67509 to 0.65740, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6716 - val_loss: 0.6574\n",
      "Epoch 5/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.6450\n",
      "Epoch 5: val_loss improved from 0.65740 to 0.63648, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6444 - val_loss: 0.6365\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6186\n",
      "Epoch 6: val_loss improved from 0.63648 to 0.62326, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6186 - val_loss: 0.6233\n",
      "Epoch 7/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.6037\n",
      "Epoch 7: val_loss improved from 0.62326 to 0.61359, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6017 - val_loss: 0.6136\n",
      "Epoch 8/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.5977\n",
      "Epoch 8: val_loss improved from 0.61359 to 0.60701, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5965 - val_loss: 0.6070\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5652\n",
      "Epoch 9: val_loss improved from 0.60701 to 0.59839, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5652 - val_loss: 0.5984\n",
      "Epoch 10/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5661\n",
      "Epoch 10: val_loss improved from 0.59839 to 0.59671, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5659 - val_loss: 0.5967\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5563\n",
      "Epoch 11: val_loss improved from 0.59671 to 0.58580, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5563 - val_loss: 0.5858\n",
      "Epoch 12/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5471\n",
      "Epoch 12: val_loss improved from 0.58580 to 0.58580, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5459 - val_loss: 0.5858\n",
      "Epoch 13/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.5356\n",
      "Epoch 13: val_loss improved from 0.58580 to 0.58063, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5356 - val_loss: 0.5806\n",
      "Epoch 14/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5299\n",
      "Epoch 14: val_loss improved from 0.58063 to 0.57482, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5298 - val_loss: 0.5748\n",
      "Epoch 15/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5214\n",
      "Epoch 15: val_loss improved from 0.57482 to 0.56961, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5213 - val_loss: 0.5696\n",
      "Epoch 16/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5237\n",
      "Epoch 16: val_loss improved from 0.56961 to 0.56807, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5237 - val_loss: 0.5681\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5177\n",
      "Epoch 17: val_loss improved from 0.56807 to 0.56260, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5177 - val_loss: 0.5626\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5015\n",
      "Epoch 18: val_loss did not improve from 0.56260\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.5015 - val_loss: 0.5630\n",
      "Epoch 19/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.5043\n",
      "Epoch 19: val_loss improved from 0.56260 to 0.55892, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5032 - val_loss: 0.5589\n",
      "Epoch 20/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5022\n",
      "Epoch 20: val_loss improved from 0.55892 to 0.55305, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5021 - val_loss: 0.5531\n",
      "Epoch 21/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4907\n",
      "Epoch 21: val_loss improved from 0.55305 to 0.55141, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4922 - val_loss: 0.5514\n",
      "Epoch 22/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4872\n",
      "Epoch 22: val_loss did not improve from 0.55141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4870 - val_loss: 0.5516\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4824\n",
      "Epoch 23: val_loss improved from 0.55141 to 0.54601, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4824 - val_loss: 0.5460\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4790\n",
      "Epoch 24: val_loss did not improve from 0.54601\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4790 - val_loss: 0.5510\n",
      "Epoch 25/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4710\n",
      "Epoch 25: val_loss did not improve from 0.54601\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4759 - val_loss: 0.5605\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4684\n",
      "Epoch 26: val_loss improved from 0.54601 to 0.53681, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4684 - val_loss: 0.5368\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4719\n",
      "Epoch 27: val_loss improved from 0.53681 to 0.53513, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4719 - val_loss: 0.5351\n",
      "Epoch 28/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4658\n",
      "Epoch 28: val_loss did not improve from 0.53513\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4660 - val_loss: 0.5466\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4701\n",
      "Epoch 29: val_loss did not improve from 0.53513\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4701 - val_loss: 0.5365\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4623\n",
      "Epoch 30: val_loss improved from 0.53513 to 0.52532, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4623 - val_loss: 0.5253\n",
      "Epoch 31/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4503\n",
      "Epoch 31: val_loss did not improve from 0.52532\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4501 - val_loss: 0.5295\n",
      "Epoch 32/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4500\n",
      "Epoch 32: val_loss did not improve from 0.52532\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4497 - val_loss: 0.5342\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4489\n",
      "Epoch 33: val_loss did not improve from 0.52532\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4489 - val_loss: 0.5482\n",
      "Epoch 34/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4516\n",
      "Epoch 34: val_loss did not improve from 0.52532\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4516 - val_loss: 0.5343\n",
      "Epoch 35/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4456\n",
      "Epoch 35: val_loss did not improve from 0.52532\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4447 - val_loss: 0.5377\n",
      "Epoch 36/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4410\n",
      "Epoch 36: val_loss did not improve from 0.52532\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4409 - val_loss: 0.5325\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4447\n",
      "Epoch 37: val_loss improved from 0.52532 to 0.52077, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4447 - val_loss: 0.5208\n",
      "Epoch 38/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4373\n",
      "Epoch 38: val_loss improved from 0.52077 to 0.52027, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4377 - val_loss: 0.5203\n",
      "Epoch 39/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4473\n",
      "Epoch 39: val_loss did not improve from 0.52027\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4472 - val_loss: 0.5211\n",
      "Epoch 40/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4331\n",
      "Epoch 40: val_loss did not improve from 0.52027\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4337 - val_loss: 0.5215\n",
      "Epoch 41/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4372\n",
      "Epoch 41: val_loss did not improve from 0.52027\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4367 - val_loss: 0.5203\n",
      "Epoch 42/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4262\n",
      "Epoch 42: val_loss did not improve from 0.52027\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4261 - val_loss: 0.5235\n",
      "Epoch 43/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4200\n",
      "Epoch 43: val_loss did not improve from 0.52027\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4195 - val_loss: 0.5285\n",
      "Epoch 44/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4292\n",
      "Epoch 44: val_loss improved from 0.52027 to 0.51805, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4288 - val_loss: 0.5180\n",
      "Epoch 45/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4279\n",
      "Epoch 45: val_loss did not improve from 0.51805\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4267 - val_loss: 0.5290\n",
      "Epoch 46/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4276\n",
      "Epoch 46: val_loss did not improve from 0.51805\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4277 - val_loss: 0.5264\n",
      "Epoch 47/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4106\n",
      "Epoch 47: val_loss did not improve from 0.51805\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4105 - val_loss: 0.5244\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4231\n",
      "Epoch 48: val_loss did not improve from 0.51805\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4231 - val_loss: 0.5184\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4284\n",
      "Epoch 49: val_loss did not improve from 0.51805\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4284 - val_loss: 0.5248\n",
      "Epoch 50/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4130\n",
      "Epoch 50: val_loss did not improve from 0.51805\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4136 - val_loss: 0.5208\n",
      "Epoch 51/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4054\n",
      "Epoch 51: val_loss improved from 0.51805 to 0.51612, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4055 - val_loss: 0.5161\n",
      "Epoch 52/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4143\n",
      "Epoch 52: val_loss improved from 0.51612 to 0.51128, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4144 - val_loss: 0.5113\n",
      "Epoch 53/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4100\n",
      "Epoch 53: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4091 - val_loss: 0.5184\n",
      "Epoch 54/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4144\n",
      "Epoch 54: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4143 - val_loss: 0.5210\n",
      "Epoch 55/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4134\n",
      "Epoch 55: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4134 - val_loss: 0.5135\n",
      "Epoch 56/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4084\n",
      "Epoch 56: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4086 - val_loss: 0.5224\n",
      "Epoch 57/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4212\n",
      "Epoch 57: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4211 - val_loss: 0.5167\n",
      "Epoch 58/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4141\n",
      "Epoch 58: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4146 - val_loss: 0.5205\n",
      "Epoch 59/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4081\n",
      "Epoch 59: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4085 - val_loss: 0.5177\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3934\n",
      "Epoch 60: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3934 - val_loss: 0.5306\n",
      "Epoch 61/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4142\n",
      "Epoch 61: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4142 - val_loss: 0.5163\n",
      "Epoch 62/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4071\n",
      "Epoch 62: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4069 - val_loss: 0.5244\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4001\n",
      "Epoch 63: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4001 - val_loss: 0.5302\n",
      "Epoch 64/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4053\n",
      "Epoch 64: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4078 - val_loss: 0.5261\n",
      "Epoch 65/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3973\n",
      "Epoch 65: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3982 - val_loss: 0.5155\n",
      "Epoch 66/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3924\n",
      "Epoch 66: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3938 - val_loss: 0.5296\n",
      "Epoch 67/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4097\n",
      "Epoch 67: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4076 - val_loss: 0.5195\n",
      "Epoch 68/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4017\n",
      "Epoch 68: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4015 - val_loss: 0.5287\n",
      "Epoch 69/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4031\n",
      "Epoch 69: val_loss did not improve from 0.51128\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4023 - val_loss: 0.5238\n",
      "Epoch 70/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4081\n",
      "Epoch 70: val_loss improved from 0.51128 to 0.51086, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4072 - val_loss: 0.5109\n",
      "Epoch 71/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3804\n",
      "Epoch 71: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3816 - val_loss: 0.5165\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4104\n",
      "Epoch 72: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4104 - val_loss: 0.5151\n",
      "Epoch 73/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4027\n",
      "Epoch 73: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4013 - val_loss: 0.5141\n",
      "Epoch 74/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4005\n",
      "Epoch 74: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4008 - val_loss: 0.5165\n",
      "Epoch 75/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3927\n",
      "Epoch 75: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3918 - val_loss: 0.5155\n",
      "Epoch 76/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4051\n",
      "Epoch 76: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4042 - val_loss: 0.5283\n",
      "Epoch 77/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3907\n",
      "Epoch 77: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3919 - val_loss: 0.5242\n",
      "Epoch 78/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4037\n",
      "Epoch 78: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4029 - val_loss: 0.5164\n",
      "Epoch 79/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3946\n",
      "Epoch 79: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3942 - val_loss: 0.5192\n",
      "Epoch 80/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3823\n",
      "Epoch 80: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3823 - val_loss: 0.5127\n",
      "Epoch 81/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3894\n",
      "Epoch 81: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3883 - val_loss: 0.5155\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3939\n",
      "Epoch 82: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3939 - val_loss: 0.5363\n",
      "Epoch 83/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3899\n",
      "Epoch 83: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3917 - val_loss: 0.5147\n",
      "Epoch 84/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3895\n",
      "Epoch 84: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3915 - val_loss: 0.5191\n",
      "Epoch 85/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3805\n",
      "Epoch 85: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3798 - val_loss: 0.5201\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3940\n",
      "Epoch 86: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3940 - val_loss: 0.5154\n",
      "Epoch 87/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3917\n",
      "Epoch 87: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3915 - val_loss: 0.5190\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3953\n",
      "Epoch 88: val_loss did not improve from 0.51086\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3953 - val_loss: 0.5120\n",
      "Epoch 89/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3734\n",
      "Epoch 89: val_loss improved from 0.51086 to 0.50779, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3735 - val_loss: 0.5078\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4024\n",
      "Epoch 90: val_loss did not improve from 0.50779\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4024 - val_loss: 0.5193\n",
      "Epoch 91/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3797\n",
      "Epoch 91: val_loss did not improve from 0.50779\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3793 - val_loss: 0.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3840\n",
      "Epoch 92: val_loss did not improve from 0.50779\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3825 - val_loss: 0.5305\n",
      "Epoch 93/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3860\n",
      "Epoch 93: val_loss did not improve from 0.50779\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3854 - val_loss: 0.5198\n",
      "Epoch 94/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3840\n",
      "Epoch 94: val_loss did not improve from 0.50779\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3840 - val_loss: 0.5203\n",
      "Epoch 95/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3815\n",
      "Epoch 95: val_loss did not improve from 0.50779\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3813 - val_loss: 0.5106\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3762\n",
      "Epoch 96: val_loss did not improve from 0.50779\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3762 - val_loss: 0.5242\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3738\n",
      "Epoch 97: val_loss improved from 0.50779 to 0.50764, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold1.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3738 - val_loss: 0.5076\n",
      "Epoch 98/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3781\n",
      "Epoch 98: val_loss did not improve from 0.50764\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3766 - val_loss: 0.5095\n",
      "Epoch 99/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3729\n",
      "Epoch 99: val_loss did not improve from 0.50764\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3732 - val_loss: 0.5184\n",
      "Epoch 100/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3751\n",
      "Epoch 100: val_loss did not improve from 0.50764\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3750 - val_loss: 0.5166\n",
      "\n",
      "Train/Test model on Fold #2.\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.8948\n",
      "Epoch 1: val_loss improved from inf to 0.84754, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 3s 12ms/step - loss: 0.8948 - val_loss: 0.8475\n",
      "Epoch 2/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.8276\n",
      "Epoch 2: val_loss improved from 0.84754 to 0.77766, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.8275 - val_loss: 0.7777\n",
      "Epoch 3/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.7330\n",
      "Epoch 3: val_loss improved from 0.77766 to 0.65023, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.7333 - val_loss: 0.6502\n",
      "Epoch 4/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.6694\n",
      "Epoch 4: val_loss improved from 0.65023 to 0.61877, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6695 - val_loss: 0.6188\n",
      "Epoch 5/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.6386\n",
      "Epoch 5: val_loss improved from 0.61877 to 0.60272, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6413 - val_loss: 0.6027\n",
      "Epoch 6/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.6133\n",
      "Epoch 6: val_loss improved from 0.60272 to 0.58257, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6137 - val_loss: 0.5826\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6019\n",
      "Epoch 7: val_loss improved from 0.58257 to 0.57286, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6019 - val_loss: 0.5729\n",
      "Epoch 8/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.5887\n",
      "Epoch 8: val_loss improved from 0.57286 to 0.56403, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5870 - val_loss: 0.5640\n",
      "Epoch 9/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.5737\n",
      "Epoch 9: val_loss improved from 0.56403 to 0.55999, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5772 - val_loss: 0.5600\n",
      "Epoch 10/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.5707\n",
      "Epoch 10: val_loss improved from 0.55999 to 0.55123, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5696 - val_loss: 0.5512\n",
      "Epoch 11/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5508\n",
      "Epoch 11: val_loss improved from 0.55123 to 0.54920, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5513 - val_loss: 0.5492\n",
      "Epoch 12/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5463\n",
      "Epoch 12: val_loss improved from 0.54920 to 0.54613, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5455 - val_loss: 0.5461\n",
      "Epoch 13/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.5362\n",
      "Epoch 13: val_loss improved from 0.54613 to 0.54250, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5359 - val_loss: 0.5425\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5357\n",
      "Epoch 14: val_loss improved from 0.54250 to 0.53613, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5357 - val_loss: 0.5361\n",
      "Epoch 15/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.5269\n",
      "Epoch 15: val_loss improved from 0.53613 to 0.53326, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5265 - val_loss: 0.5333\n",
      "Epoch 16/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.5155\n",
      "Epoch 16: val_loss improved from 0.53326 to 0.52816, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5141 - val_loss: 0.5282\n",
      "Epoch 17/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.5226\n",
      "Epoch 17: val_loss did not improve from 0.52816\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5220 - val_loss: 0.5296\n",
      "Epoch 18/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5130\n",
      "Epoch 18: val_loss improved from 0.52816 to 0.52376, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5126 - val_loss: 0.5238\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/120 [============================>.] - ETA: 0s - loss: 0.5109\n",
      "Epoch 19: val_loss did not improve from 0.52376\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.5110 - val_loss: 0.5292\n",
      "Epoch 20/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5067\n",
      "Epoch 20: val_loss improved from 0.52376 to 0.51900, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5062 - val_loss: 0.5190\n",
      "Epoch 21/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.5018\n",
      "Epoch 21: val_loss improved from 0.51900 to 0.51823, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5030 - val_loss: 0.5182\n",
      "Epoch 22/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4854\n",
      "Epoch 22: val_loss improved from 0.51823 to 0.51363, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4853 - val_loss: 0.5136\n",
      "Epoch 23/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4849\n",
      "Epoch 23: val_loss did not improve from 0.51363\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4850 - val_loss: 0.5242\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4833\n",
      "Epoch 24: val_loss improved from 0.51363 to 0.51313, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4833 - val_loss: 0.5131\n",
      "Epoch 25/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4844\n",
      "Epoch 25: val_loss improved from 0.51313 to 0.50172, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4845 - val_loss: 0.5017\n",
      "Epoch 26/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4761\n",
      "Epoch 26: val_loss did not improve from 0.50172\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4754 - val_loss: 0.5019\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4624\n",
      "Epoch 27: val_loss did not improve from 0.50172\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4624 - val_loss: 0.5051\n",
      "Epoch 28/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4615\n",
      "Epoch 28: val_loss improved from 0.50172 to 0.49992, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4615 - val_loss: 0.4999\n",
      "Epoch 29/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4654\n",
      "Epoch 29: val_loss improved from 0.49992 to 0.49841, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4645 - val_loss: 0.4984\n",
      "Epoch 30/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4514\n",
      "Epoch 30: val_loss improved from 0.49841 to 0.49467, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4512 - val_loss: 0.4947\n",
      "Epoch 31/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4542\n",
      "Epoch 31: val_loss improved from 0.49467 to 0.49050, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4538 - val_loss: 0.4905\n",
      "Epoch 32/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4528\n",
      "Epoch 32: val_loss did not improve from 0.49050\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4527 - val_loss: 0.4990\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4477\n",
      "Epoch 33: val_loss did not improve from 0.49050\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4477 - val_loss: 0.4916\n",
      "Epoch 34/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4417\n",
      "Epoch 34: val_loss did not improve from 0.49050\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4420 - val_loss: 0.4923\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4406\n",
      "Epoch 35: val_loss did not improve from 0.49050\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4406 - val_loss: 0.5001\n",
      "Epoch 36/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4437\n",
      "Epoch 36: val_loss improved from 0.49050 to 0.48782, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4441 - val_loss: 0.4878\n",
      "Epoch 37/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4453\n",
      "Epoch 37: val_loss did not improve from 0.48782\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4443 - val_loss: 0.4965\n",
      "Epoch 38/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4370\n",
      "Epoch 38: val_loss did not improve from 0.48782\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4356 - val_loss: 0.4894\n",
      "Epoch 39/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4294\n",
      "Epoch 39: val_loss improved from 0.48782 to 0.47963, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4294 - val_loss: 0.4796\n",
      "Epoch 40/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4389\n",
      "Epoch 40: val_loss did not improve from 0.47963\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4383 - val_loss: 0.5070\n",
      "Epoch 41/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4271\n",
      "Epoch 41: val_loss did not improve from 0.47963\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4286 - val_loss: 0.4797\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4273\n",
      "Epoch 42: val_loss improved from 0.47963 to 0.47958, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4273 - val_loss: 0.4796\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4256\n",
      "Epoch 43: val_loss did not improve from 0.47958\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4256 - val_loss: 0.4950\n",
      "Epoch 44/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4144\n",
      "Epoch 44: val_loss did not improve from 0.47958\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4144 - val_loss: 0.4841\n",
      "Epoch 45/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4188\n",
      "Epoch 45: val_loss did not improve from 0.47958\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4188 - val_loss: 0.4810\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4229\n",
      "Epoch 46: val_loss did not improve from 0.47958\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4229 - val_loss: 0.4810\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4151\n",
      "Epoch 47: val_loss did not improve from 0.47958\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4151 - val_loss: 0.4821\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4140\n",
      "Epoch 48: val_loss did not improve from 0.47958\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4140 - val_loss: 0.4842\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4131\n",
      "Epoch 49: val_loss improved from 0.47958 to 0.47868, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4131 - val_loss: 0.4787\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4157\n",
      "Epoch 50: val_loss did not improve from 0.47868\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4157 - val_loss: 0.4880\n",
      "Epoch 51/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4028\n",
      "Epoch 51: val_loss improved from 0.47868 to 0.47582, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold2.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4030 - val_loss: 0.4758\n",
      "Epoch 52/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4096\n",
      "Epoch 52: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4096 - val_loss: 0.4989\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4148\n",
      "Epoch 53: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4148 - val_loss: 0.4772\n",
      "Epoch 54/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4108\n",
      "Epoch 54: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4110 - val_loss: 0.4770\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4060\n",
      "Epoch 55: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4060 - val_loss: 0.4925\n",
      "Epoch 56/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4115\n",
      "Epoch 56: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4127 - val_loss: 0.4881\n",
      "Epoch 57/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4006\n",
      "Epoch 57: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4027 - val_loss: 0.4817\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4009\n",
      "Epoch 58: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4009 - val_loss: 0.5040\n",
      "Epoch 59/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4104\n",
      "Epoch 59: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4105 - val_loss: 0.4839\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3924\n",
      "Epoch 60: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3924 - val_loss: 0.4820\n",
      "Epoch 61/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3926\n",
      "Epoch 61: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3925 - val_loss: 0.4934\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3910\n",
      "Epoch 62: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3910 - val_loss: 0.4879\n",
      "Epoch 63/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3915\n",
      "Epoch 63: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3914 - val_loss: 0.4844\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3926\n",
      "Epoch 64: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3926 - val_loss: 0.4830\n",
      "Epoch 65/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3971\n",
      "Epoch 65: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3989 - val_loss: 0.4956\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3889\n",
      "Epoch 66: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3889 - val_loss: 0.4951\n",
      "Epoch 67/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3919\n",
      "Epoch 67: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3902 - val_loss: 0.4841\n",
      "Epoch 68/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3959\n",
      "Epoch 68: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3970 - val_loss: 0.4797\n",
      "Epoch 69/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3813\n",
      "Epoch 69: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3834 - val_loss: 0.4927\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3911\n",
      "Epoch 70: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3911 - val_loss: 0.4948\n",
      "Epoch 71/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3926\n",
      "Epoch 71: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3918 - val_loss: 0.4889\n",
      "Epoch 72/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3903\n",
      "Epoch 72: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3869 - val_loss: 0.5066\n",
      "Epoch 73/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3763\n",
      "Epoch 73: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3750 - val_loss: 0.5111\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3805\n",
      "Epoch 74: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3805 - val_loss: 0.4933\n",
      "Epoch 75/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3882\n",
      "Epoch 75: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3865 - val_loss: 0.5031\n",
      "Epoch 76/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3946\n",
      "Epoch 76: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3944 - val_loss: 0.4898\n",
      "Epoch 77/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4014\n",
      "Epoch 77: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3995 - val_loss: 0.4899\n",
      "Epoch 78/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3917\n",
      "Epoch 78: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3915 - val_loss: 0.5121\n",
      "Epoch 79/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3942\n",
      "Epoch 79: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3942 - val_loss: 0.4820\n",
      "Epoch 80/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3888\n",
      "Epoch 80: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3896 - val_loss: 0.4869\n",
      "Epoch 81/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3827\n",
      "Epoch 81: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3832 - val_loss: 0.4851\n",
      "Epoch 82/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3688\n",
      "Epoch 82: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3681 - val_loss: 0.5090\n",
      "Epoch 83/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3871\n",
      "Epoch 83: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3879 - val_loss: 0.4877\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3811\n",
      "Epoch 84: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3811 - val_loss: 0.5251\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3795\n",
      "Epoch 85: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3795 - val_loss: 0.4931\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3987\n",
      "Epoch 86: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3987 - val_loss: 0.4893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3718\n",
      "Epoch 87: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3718 - val_loss: 0.4781\n",
      "Epoch 88/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3766\n",
      "Epoch 88: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3763 - val_loss: 0.5273\n",
      "Epoch 89/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3749\n",
      "Epoch 89: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3746 - val_loss: 0.5056\n",
      "Epoch 90/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3882\n",
      "Epoch 90: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3873 - val_loss: 0.4949\n",
      "Epoch 91/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3697\n",
      "Epoch 91: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3694 - val_loss: 0.5113\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3757\n",
      "Epoch 92: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3757 - val_loss: 0.4901\n",
      "Epoch 93/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3783\n",
      "Epoch 93: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3779 - val_loss: 0.4906\n",
      "Epoch 94/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3710\n",
      "Epoch 94: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3687 - val_loss: 0.4883\n",
      "Epoch 95/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3861\n",
      "Epoch 95: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3860 - val_loss: 0.4874\n",
      "Epoch 96/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3786\n",
      "Epoch 96: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3776 - val_loss: 0.4897\n",
      "Epoch 97/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3700\n",
      "Epoch 97: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3709 - val_loss: 0.4911\n",
      "Epoch 98/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3784\n",
      "Epoch 98: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3817 - val_loss: 0.4811\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3638\n",
      "Epoch 99: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3638 - val_loss: 0.5063\n",
      "Epoch 100/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3828\n",
      "Epoch 100: val_loss did not improve from 0.47582\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3806 - val_loss: 0.5045\n",
      "\n",
      "Train/Test model on Fold #3.\n",
      "Epoch 1/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.8894\n",
      "Epoch 1: val_loss improved from inf to 0.83320, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 3s 11ms/step - loss: 0.8882 - val_loss: 0.8332\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.7828\n",
      "Epoch 2: val_loss improved from 0.83320 to 0.69736, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.7828 - val_loss: 0.6974\n",
      "Epoch 3/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.6911\n",
      "Epoch 3: val_loss improved from 0.69736 to 0.66164, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6914 - val_loss: 0.6616\n",
      "Epoch 4/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.6678\n",
      "Epoch 4: val_loss improved from 0.66164 to 0.64323, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6679 - val_loss: 0.6432\n",
      "Epoch 5/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.6372\n",
      "Epoch 5: val_loss improved from 0.64323 to 0.62815, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6363 - val_loss: 0.6282\n",
      "Epoch 6/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.6098\n",
      "Epoch 6: val_loss improved from 0.62815 to 0.62129, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6116 - val_loss: 0.6213\n",
      "Epoch 7/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5945\n",
      "Epoch 7: val_loss improved from 0.62129 to 0.60547, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5944 - val_loss: 0.6055\n",
      "Epoch 8/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5816\n",
      "Epoch 8: val_loss improved from 0.60547 to 0.59357, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5809 - val_loss: 0.5936\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5655\n",
      "Epoch 9: val_loss improved from 0.59357 to 0.58949, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5655 - val_loss: 0.5895\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5529\n",
      "Epoch 10: val_loss improved from 0.58949 to 0.58590, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5529 - val_loss: 0.5859\n",
      "Epoch 11/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.5517\n",
      "Epoch 11: val_loss improved from 0.58590 to 0.58026, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5502 - val_loss: 0.5803\n",
      "Epoch 12/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5396\n",
      "Epoch 12: val_loss improved from 0.58026 to 0.57717, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5392 - val_loss: 0.5772\n",
      "Epoch 13/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.5397\n",
      "Epoch 13: val_loss improved from 0.57717 to 0.57476, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5383 - val_loss: 0.5748\n",
      "Epoch 14/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5305\n",
      "Epoch 14: val_loss improved from 0.57476 to 0.57268, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5298 - val_loss: 0.5727\n",
      "Epoch 15/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5207\n",
      "Epoch 15: val_loss improved from 0.57268 to 0.56833, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5207 - val_loss: 0.5683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.5220\n",
      "Epoch 16: val_loss did not improve from 0.56833\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5228 - val_loss: 0.5705\n",
      "Epoch 17/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5070\n",
      "Epoch 17: val_loss improved from 0.56833 to 0.56696, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5077 - val_loss: 0.5670\n",
      "Epoch 18/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5013\n",
      "Epoch 18: val_loss improved from 0.56696 to 0.55953, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5035 - val_loss: 0.5595\n",
      "Epoch 19/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.5072\n",
      "Epoch 19: val_loss did not improve from 0.55953\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5066 - val_loss: 0.5596\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4892\n",
      "Epoch 20: val_loss improved from 0.55953 to 0.55943, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4892 - val_loss: 0.5594\n",
      "Epoch 21/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4875\n",
      "Epoch 21: val_loss improved from 0.55943 to 0.55702, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4871 - val_loss: 0.5570\n",
      "Epoch 22/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4921\n",
      "Epoch 22: val_loss improved from 0.55702 to 0.55482, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4920 - val_loss: 0.5548\n",
      "Epoch 23/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4827\n",
      "Epoch 23: val_loss did not improve from 0.55482\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4829 - val_loss: 0.5553\n",
      "Epoch 24/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4770\n",
      "Epoch 24: val_loss improved from 0.55482 to 0.55095, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4778 - val_loss: 0.5510\n",
      "Epoch 25/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4670\n",
      "Epoch 25: val_loss did not improve from 0.55095\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4667 - val_loss: 0.5578\n",
      "Epoch 26/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4798\n",
      "Epoch 26: val_loss improved from 0.55095 to 0.55087, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4788 - val_loss: 0.5509\n",
      "Epoch 27/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4639\n",
      "Epoch 27: val_loss improved from 0.55087 to 0.54659, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4631 - val_loss: 0.5466\n",
      "Epoch 28/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4643\n",
      "Epoch 28: val_loss improved from 0.54659 to 0.54255, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4657 - val_loss: 0.5425\n",
      "Epoch 29/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4691\n",
      "Epoch 29: val_loss improved from 0.54255 to 0.54179, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4689 - val_loss: 0.5418\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4610\n",
      "Epoch 30: val_loss improved from 0.54179 to 0.53972, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4610 - val_loss: 0.5397\n",
      "Epoch 31/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4495\n",
      "Epoch 31: val_loss did not improve from 0.53972\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4519 - val_loss: 0.5409\n",
      "Epoch 32/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4491\n",
      "Epoch 32: val_loss did not improve from 0.53972\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4489 - val_loss: 0.5445\n",
      "Epoch 33/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4494\n",
      "Epoch 33: val_loss did not improve from 0.53972\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4486 - val_loss: 0.5450\n",
      "Epoch 34/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4494\n",
      "Epoch 34: val_loss did not improve from 0.53972\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4511 - val_loss: 0.5450\n",
      "Epoch 35/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4463\n",
      "Epoch 35: val_loss improved from 0.53972 to 0.53827, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4462 - val_loss: 0.5383\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4360\n",
      "Epoch 36: val_loss did not improve from 0.53827\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4360 - val_loss: 0.5800\n",
      "Epoch 37/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4485\n",
      "Epoch 37: val_loss did not improve from 0.53827\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4490 - val_loss: 0.5498\n",
      "Epoch 38/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4412\n",
      "Epoch 38: val_loss did not improve from 0.53827\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4406 - val_loss: 0.5417\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4417\n",
      "Epoch 39: val_loss did not improve from 0.53827\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4417 - val_loss: 0.5445\n",
      "Epoch 40/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4358\n",
      "Epoch 40: val_loss improved from 0.53827 to 0.53252, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4370 - val_loss: 0.5325\n",
      "Epoch 41/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4249\n",
      "Epoch 41: val_loss did not improve from 0.53252\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4238 - val_loss: 0.5340\n",
      "Epoch 42/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4267\n",
      "Epoch 42: val_loss improved from 0.53252 to 0.53239, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4249 - val_loss: 0.5324\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4307\n",
      "Epoch 43: val_loss improved from 0.53239 to 0.53109, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold3.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4307 - val_loss: 0.5311\n",
      "Epoch 44/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4282\n",
      "Epoch 44: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4297 - val_loss: 0.5311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4181\n",
      "Epoch 45: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4181 - val_loss: 0.5404\n",
      "Epoch 46/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4253\n",
      "Epoch 46: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4250 - val_loss: 0.5388\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4214\n",
      "Epoch 47: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4214 - val_loss: 0.5391\n",
      "Epoch 48/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4225\n",
      "Epoch 48: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4225 - val_loss: 0.5469\n",
      "Epoch 49/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4211\n",
      "Epoch 49: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4206 - val_loss: 0.5381\n",
      "Epoch 50/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4159\n",
      "Epoch 50: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4153 - val_loss: 0.5423\n",
      "Epoch 51/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4145\n",
      "Epoch 51: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4151 - val_loss: 0.5507\n",
      "Epoch 52/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4068\n",
      "Epoch 52: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4072 - val_loss: 0.5394\n",
      "Epoch 53/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4073\n",
      "Epoch 53: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4080 - val_loss: 0.5461\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4109\n",
      "Epoch 54: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4109 - val_loss: 0.5556\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3991\n",
      "Epoch 55: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3991 - val_loss: 0.5417\n",
      "Epoch 56/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4078\n",
      "Epoch 56: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4072 - val_loss: 0.5398\n",
      "Epoch 57/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4109\n",
      "Epoch 57: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4107 - val_loss: 0.5320\n",
      "Epoch 58/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4010\n",
      "Epoch 58: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4020 - val_loss: 0.5440\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4053\n",
      "Epoch 59: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4053 - val_loss: 0.5417\n",
      "Epoch 60/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4045\n",
      "Epoch 60: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4043 - val_loss: 0.5448\n",
      "Epoch 61/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4047\n",
      "Epoch 61: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4052 - val_loss: 0.5321\n",
      "Epoch 62/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3944\n",
      "Epoch 62: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3925 - val_loss: 0.5536\n",
      "Epoch 63/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4024\n",
      "Epoch 63: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4023 - val_loss: 0.5406\n",
      "Epoch 64/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4051\n",
      "Epoch 64: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4053 - val_loss: 0.5429\n",
      "Epoch 65/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3971\n",
      "Epoch 65: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3959 - val_loss: 0.5448\n",
      "Epoch 66/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3954\n",
      "Epoch 66: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3922 - val_loss: 0.5575\n",
      "Epoch 67/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3901\n",
      "Epoch 67: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3891 - val_loss: 0.5543\n",
      "Epoch 68/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4055\n",
      "Epoch 68: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4040 - val_loss: 0.5559\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3872\n",
      "Epoch 69: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3872 - val_loss: 0.5432\n",
      "Epoch 70/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3944\n",
      "Epoch 70: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3943 - val_loss: 0.5493\n",
      "Epoch 71/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3767\n",
      "Epoch 71: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3771 - val_loss: 0.5505\n",
      "Epoch 72/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3809\n",
      "Epoch 72: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3798 - val_loss: 0.5421\n",
      "Epoch 73/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3844\n",
      "Epoch 73: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3850 - val_loss: 0.5445\n",
      "Epoch 74/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3921\n",
      "Epoch 74: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3922 - val_loss: 0.5819\n",
      "Epoch 75/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3871\n",
      "Epoch 75: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3858 - val_loss: 0.5512\n",
      "Epoch 76/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3919\n",
      "Epoch 76: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3941 - val_loss: 0.5528\n",
      "Epoch 77/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3792\n",
      "Epoch 77: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3807 - val_loss: 0.5580\n",
      "Epoch 78/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3753\n",
      "Epoch 78: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3763 - val_loss: 0.5581\n",
      "Epoch 79/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3870\n",
      "Epoch 79: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3868 - val_loss: 0.5527\n",
      "Epoch 80/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3817\n",
      "Epoch 80: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3815 - val_loss: 0.5510\n",
      "Epoch 81/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3760\n",
      "Epoch 81: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3746 - val_loss: 0.5459\n",
      "Epoch 82/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3900\n",
      "Epoch 82: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3899 - val_loss: 0.5453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3740\n",
      "Epoch 83: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3732 - val_loss: 0.5624\n",
      "Epoch 84/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3737\n",
      "Epoch 84: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3720 - val_loss: 0.5662\n",
      "Epoch 85/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3742\n",
      "Epoch 85: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3751 - val_loss: 0.5561\n",
      "Epoch 86/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3655\n",
      "Epoch 86: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3655 - val_loss: 0.5463\n",
      "Epoch 87/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3861\n",
      "Epoch 87: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3866 - val_loss: 0.5532\n",
      "Epoch 88/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3683\n",
      "Epoch 88: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3681 - val_loss: 0.5587\n",
      "Epoch 89/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3644\n",
      "Epoch 89: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3691 - val_loss: 0.5668\n",
      "Epoch 90/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3927\n",
      "Epoch 90: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3919 - val_loss: 0.5518\n",
      "Epoch 91/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3788\n",
      "Epoch 91: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3783 - val_loss: 0.5616\n",
      "Epoch 92/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3762\n",
      "Epoch 92: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3730 - val_loss: 0.5550\n",
      "Epoch 93/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3698\n",
      "Epoch 93: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3701 - val_loss: 0.5839\n",
      "Epoch 94/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3833\n",
      "Epoch 94: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3836 - val_loss: 0.5442\n",
      "Epoch 95/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.3690\n",
      "Epoch 95: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3671 - val_loss: 0.5657\n",
      "Epoch 96/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3670\n",
      "Epoch 96: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3670 - val_loss: 0.5885\n",
      "Epoch 97/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3642\n",
      "Epoch 97: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3640 - val_loss: 0.5613\n",
      "Epoch 98/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3770\n",
      "Epoch 98: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3774 - val_loss: 0.5379\n",
      "Epoch 99/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3688\n",
      "Epoch 99: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3689 - val_loss: 0.5585\n",
      "Epoch 100/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3567\n",
      "Epoch 100: val_loss did not improve from 0.53109\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3564 - val_loss: 0.5659\n",
      "\n",
      "Train/Test model on Fold #4.\n",
      "Epoch 1/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.8886\n",
      "Epoch 1: val_loss improved from inf to 0.84199, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 3s 11ms/step - loss: 0.8881 - val_loss: 0.8420\n",
      "Epoch 2/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.8103\n",
      "Epoch 2: val_loss improved from 0.84199 to 0.74055, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.8089 - val_loss: 0.7406\n",
      "Epoch 3/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.7194\n",
      "Epoch 3: val_loss improved from 0.74055 to 0.64932, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.7189 - val_loss: 0.6493\n",
      "Epoch 4/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.6776\n",
      "Epoch 4: val_loss improved from 0.64932 to 0.61606, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6762 - val_loss: 0.6161\n",
      "Epoch 5/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.6423\n",
      "Epoch 5: val_loss improved from 0.61606 to 0.59888, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6412 - val_loss: 0.5989\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6194\n",
      "Epoch 6: val_loss improved from 0.59888 to 0.58641, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.6194 - val_loss: 0.5864\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6048\n",
      "Epoch 7: val_loss did not improve from 0.58641\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.6048 - val_loss: 0.5875\n",
      "Epoch 8/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5845\n",
      "Epoch 8: val_loss improved from 0.58641 to 0.55852, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5847 - val_loss: 0.5585\n",
      "Epoch 9/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5720\n",
      "Epoch 9: val_loss improved from 0.55852 to 0.55390, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5718 - val_loss: 0.5539\n",
      "Epoch 10/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.5711\n",
      "Epoch 10: val_loss did not improve from 0.55390\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5703 - val_loss: 0.5669\n",
      "Epoch 11/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.5521\n",
      "Epoch 11: val_loss improved from 0.55390 to 0.55155, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5541 - val_loss: 0.5516\n",
      "Epoch 12/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5409\n",
      "Epoch 12: val_loss improved from 0.55155 to 0.53788, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5411 - val_loss: 0.5379\n",
      "Epoch 13/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5332\n",
      "Epoch 13: val_loss did not improve from 0.53788\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5346 - val_loss: 0.5398\n",
      "Epoch 14/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.5273\n",
      "Epoch 14: val_loss improved from 0.53788 to 0.53008, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5273 - val_loss: 0.5301\n",
      "Epoch 15/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.5281\n",
      "Epoch 15: val_loss did not improve from 0.53008\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.5297 - val_loss: 0.5321\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5221\n",
      "Epoch 16: val_loss did not improve from 0.53008\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.5221 - val_loss: 0.5314\n",
      "Epoch 17/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.5168\n",
      "Epoch 17: val_loss did not improve from 0.53008\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5209 - val_loss: 0.5373\n",
      "Epoch 18/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.5077\n",
      "Epoch 18: val_loss did not improve from 0.53008\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5074 - val_loss: 0.5342\n",
      "Epoch 19/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.5035\n",
      "Epoch 19: val_loss improved from 0.53008 to 0.52098, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.5035 - val_loss: 0.5210\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4999\n",
      "Epoch 20: val_loss did not improve from 0.52098\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4999 - val_loss: 0.5220\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.5069\n",
      "Epoch 21: val_loss did not improve from 0.52098\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.5069 - val_loss: 0.5239\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4907\n",
      "Epoch 22: val_loss improved from 0.52098 to 0.51871, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4907 - val_loss: 0.5187\n",
      "Epoch 23/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4827\n",
      "Epoch 23: val_loss did not improve from 0.51871\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4832 - val_loss: 0.5201\n",
      "Epoch 24/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4795\n",
      "Epoch 24: val_loss did not improve from 0.51871\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4794 - val_loss: 0.5214\n",
      "Epoch 25/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4830\n",
      "Epoch 25: val_loss did not improve from 0.51871\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4832 - val_loss: 0.5347\n",
      "Epoch 26/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4883\n",
      "Epoch 26: val_loss improved from 0.51871 to 0.51650, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4890 - val_loss: 0.5165\n",
      "Epoch 27/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4795\n",
      "Epoch 27: val_loss did not improve from 0.51650\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4796 - val_loss: 0.5178\n",
      "Epoch 28/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4595\n",
      "Epoch 28: val_loss did not improve from 0.51650\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4606 - val_loss: 0.5346\n",
      "Epoch 29/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4729\n",
      "Epoch 29: val_loss did not improve from 0.51650\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4725 - val_loss: 0.5348\n",
      "Epoch 30/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4648\n",
      "Epoch 30: val_loss did not improve from 0.51650\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4653 - val_loss: 0.5377\n",
      "Epoch 31/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4464\n",
      "Epoch 31: val_loss did not improve from 0.51650\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4484 - val_loss: 0.5173\n",
      "Epoch 32/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4646\n",
      "Epoch 32: val_loss did not improve from 0.51650\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4642 - val_loss: 0.5204\n",
      "Epoch 33/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4512\n",
      "Epoch 33: val_loss did not improve from 0.51650\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4526 - val_loss: 0.5197\n",
      "Epoch 34/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4537\n",
      "Epoch 34: val_loss improved from 0.51650 to 0.51375, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4534 - val_loss: 0.5137\n",
      "Epoch 35/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4593\n",
      "Epoch 35: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4578 - val_loss: 0.5163\n",
      "Epoch 36/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4538\n",
      "Epoch 36: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4517 - val_loss: 0.5174\n",
      "Epoch 37/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4384\n",
      "Epoch 37: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4383 - val_loss: 0.5462\n",
      "Epoch 38/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4356\n",
      "Epoch 38: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.4355 - val_loss: 0.5165\n",
      "Epoch 39/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4491\n",
      "Epoch 39: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4468 - val_loss: 0.5249\n",
      "Epoch 40/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4375\n",
      "Epoch 40: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4381 - val_loss: 0.5271\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4291\n",
      "Epoch 41: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4291 - val_loss: 0.5247\n",
      "Epoch 42/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4344\n",
      "Epoch 42: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4344 - val_loss: 0.5143\n",
      "Epoch 43/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4359\n",
      "Epoch 43: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4336 - val_loss: 0.5165\n",
      "Epoch 44/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4240\n",
      "Epoch 44: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4225 - val_loss: 0.5238\n",
      "Epoch 45/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4351\n",
      "Epoch 45: val_loss did not improve from 0.51375\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4339 - val_loss: 0.5305\n",
      "Epoch 46/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4183\n",
      "Epoch 46: val_loss improved from 0.51375 to 0.51358, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4183 - val_loss: 0.5136\n",
      "Epoch 47/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4298\n",
      "Epoch 47: val_loss improved from 0.51358 to 0.50982, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4295 - val_loss: 0.5098\n",
      "Epoch 48/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4248\n",
      "Epoch 48: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4238 - val_loss: 0.5363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4204\n",
      "Epoch 49: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4231 - val_loss: 0.5262\n",
      "Epoch 50/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4271\n",
      "Epoch 50: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4283 - val_loss: 0.5207\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4152\n",
      "Epoch 51: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4152 - val_loss: 0.5482\n",
      "Epoch 52/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4195\n",
      "Epoch 52: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4231 - val_loss: 0.5165\n",
      "Epoch 53/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4190\n",
      "Epoch 53: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4209 - val_loss: 0.5169\n",
      "Epoch 54/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4055\n",
      "Epoch 54: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4077 - val_loss: 0.5449\n",
      "Epoch 55/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4261\n",
      "Epoch 55: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4253 - val_loss: 0.5182\n",
      "Epoch 56/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4199\n",
      "Epoch 56: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4213 - val_loss: 0.5147\n",
      "Epoch 57/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4152\n",
      "Epoch 57: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4151 - val_loss: 0.5178\n",
      "Epoch 58/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4013\n",
      "Epoch 58: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4018 - val_loss: 0.5199\n",
      "Epoch 59/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.4035\n",
      "Epoch 59: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4082 - val_loss: 0.5175\n",
      "Epoch 60/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.4134\n",
      "Epoch 60: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4139 - val_loss: 0.5215\n",
      "Epoch 61/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4126\n",
      "Epoch 61: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4165 - val_loss: 0.5111\n",
      "Epoch 62/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4032\n",
      "Epoch 62: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4023 - val_loss: 0.5254\n",
      "Epoch 63/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.4051\n",
      "Epoch 63: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4060 - val_loss: 0.5241\n",
      "Epoch 64/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.4052\n",
      "Epoch 64: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4056 - val_loss: 0.5529\n",
      "Epoch 65/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3963\n",
      "Epoch 65: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3964 - val_loss: 0.5237\n",
      "Epoch 66/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4069\n",
      "Epoch 66: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4069 - val_loss: 0.5187\n",
      "Epoch 67/100\n",
      "115/120 [===========================>..] - ETA: 0s - loss: 0.4137\n",
      "Epoch 67: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4158 - val_loss: 0.5271\n",
      "Epoch 68/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4062\n",
      "Epoch 68: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4052 - val_loss: 0.5194\n",
      "Epoch 69/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.4032\n",
      "Epoch 69: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4032 - val_loss: 0.5231\n",
      "Epoch 70/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3897\n",
      "Epoch 70: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3907 - val_loss: 0.5292\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3888\n",
      "Epoch 71: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3888 - val_loss: 0.5207\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.4014\n",
      "Epoch 72: val_loss did not improve from 0.50982\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.4014 - val_loss: 0.5145\n",
      "Epoch 73/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.4017\n",
      "Epoch 73: val_loss improved from 0.50982 to 0.50653, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\bestModel-fold4.hdf5\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.4004 - val_loss: 0.5065\n",
      "Epoch 74/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3916\n",
      "Epoch 74: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3919 - val_loss: 0.5173\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3946\n",
      "Epoch 75: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3946 - val_loss: 0.5171\n",
      "Epoch 76/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3891\n",
      "Epoch 76: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3929 - val_loss: 0.5319\n",
      "Epoch 77/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3942\n",
      "Epoch 77: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3940 - val_loss: 0.5329\n",
      "Epoch 78/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3831\n",
      "Epoch 78: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3839 - val_loss: 0.5258\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3949\n",
      "Epoch 79: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3949 - val_loss: 0.5217\n",
      "Epoch 80/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3837\n",
      "Epoch 80: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3858 - val_loss: 0.5223\n",
      "Epoch 81/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3971\n",
      "Epoch 81: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3998 - val_loss: 0.5173\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3870\n",
      "Epoch 82: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3870 - val_loss: 0.5284\n",
      "Epoch 83/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3911\n",
      "Epoch 83: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3939 - val_loss: 0.5271\n",
      "Epoch 84/100\n",
      "113/120 [===========================>..] - ETA: 0s - loss: 0.3861\n",
      "Epoch 84: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3877 - val_loss: 0.5231\n",
      "Epoch 85/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3979\n",
      "Epoch 85: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3976 - val_loss: 0.5229\n",
      "Epoch 86/100\n",
      "114/120 [===========================>..] - ETA: 0s - loss: 0.3881\n",
      "Epoch 86: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3870 - val_loss: 0.5198\n",
      "Epoch 87/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3867\n",
      "Epoch 87: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3888 - val_loss: 0.5244\n",
      "Epoch 88/100\n",
      "118/120 [============================>.] - ETA: 0s - loss: 0.3763\n",
      "Epoch 88: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3766 - val_loss: 0.5390\n",
      "Epoch 89/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3851\n",
      "Epoch 89: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3848 - val_loss: 0.5185\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3772\n",
      "Epoch 90: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3772 - val_loss: 0.5294\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3786\n",
      "Epoch 91: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3786 - val_loss: 0.5253\n",
      "Epoch 92/100\n",
      "116/120 [============================>.] - ETA: 0s - loss: 0.3754\n",
      "Epoch 92: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3769 - val_loss: 0.5333\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3722\n",
      "Epoch 93: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3722 - val_loss: 0.5283\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3714\n",
      "Epoch 94: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3714 - val_loss: 0.5215\n",
      "Epoch 95/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3917\n",
      "Epoch 95: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3916 - val_loss: 0.5230\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3777\n",
      "Epoch 96: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3777 - val_loss: 0.5475\n",
      "Epoch 97/100\n",
      "119/120 [============================>.] - ETA: 0s - loss: 0.3869\n",
      "Epoch 97: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3870 - val_loss: 0.5297\n",
      "Epoch 98/100\n",
      "117/120 [============================>.] - ETA: 0s - loss: 0.3781\n",
      "Epoch 98: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 8ms/step - loss: 0.3792 - val_loss: 0.5301\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3750\n",
      "Epoch 99: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3750 - val_loss: 0.5267\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3710\n",
      "Epoch 100: val_loss did not improve from 0.50653\n",
      "120/120 [==============================] - 1s 7ms/step - loss: 0.3710 - val_loss: 0.5280\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Train/Test model on all folds, generate evaluations\n",
    "##################################################################################\n",
    "\n",
    "## Create and set directory to save model\n",
    "modelPath = os.path.join(outPath, expName, \"{}fold\".format(n_fold), \"models\")\n",
    "if(not os.path.isdir(modelPath)):\n",
    "    os.makedirs(modelPath)\n",
    "\n",
    "i = -1\n",
    "for fold in folds:\n",
    "    i += 1\n",
    "    \n",
    "    print(\"\\nTrain/Test model on Fold #\"+str(i)+\".\")\n",
    "    \n",
    "    model = DLNN_CORENup(input_seq_shape = input_seq_shape)\n",
    "    \n",
    "    ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    modelCallbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
    "                                           monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                           save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "    ]\n",
    "    \n",
    "    X_features = np.concatenate((fold[\"X_train\"], \n",
    "                                 np.flip(fold[\"X_train\"], axis=1)),\n",
    "                                axis = 0\n",
    "                               )\n",
    "    Y_labels = np.concatenate((fold[\"y_train\"], \n",
    "                               fold[\"y_train\"]), \n",
    "                              axis = 0)\n",
    "    \n",
    "    # adding random shuffling of the dataset for training purpose\n",
    "    index_arr = np.arange(X_features.shape[0])\n",
    "    index_arr = np.random.permutation(index_arr)\n",
    "    \n",
    "    model.fit(x = X_features[index_arr], y = Y_labels[index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "              callbacks = modelCallbacks, validation_data = (fold[\"X_test\"], fold[\"y_test\"]))\n",
    "    \n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Prediction and metrics for TRAIN dataset\n",
    "    ##################################################################################\n",
    "\n",
    "    y_pred = model.predict(fold[\"X_train\"])\n",
    "    label_pred = pred2label(y_pred)\n",
    "    \n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "    prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "    mcc = matthews_corrcoef(fold[\"y_train\"], label_pred)\n",
    "\n",
    "    conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "    auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "    \n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Train\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Prediction and metrics for TEST dataset\n",
    "    ##################################################################################\n",
    "\n",
    "    y_pred = model.predict(fold[\"X_test\"])\n",
    "    label_pred = pred2label(y_pred)\n",
    "    \n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "    prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "    mcc = matthews_corrcoef(fold[\"y_test\"], label_pred)\n",
    "\n",
    "    conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "    auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "    \n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Test\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.785468</td>\n",
       "      <td>0.772456</td>\n",
       "      <td>0.862004</td>\n",
       "      <td>0.810246</td>\n",
       "      <td>0.760691</td>\n",
       "      <td>0.572232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.904495</td>\n",
       "      <td>0.888980</td>\n",
       "      <td>0.961596</td>\n",
       "      <td>0.925276</td>\n",
       "      <td>0.883718</td>\n",
       "      <td>0.810167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Test        0.785468   0.772456  0.862004     0.810246     0.760691  0.572232\n",
       "Train       0.904495   0.888980  0.961596     0.925276     0.883718  0.810167"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.787755</td>\n",
       "      <td>[0.0, 0.0041841004184100415, 0.012552301255230...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.004201680672268907, 0.004201...</td>\n",
       "      <td>[1.9944923, 0.9944923, 0.98615485, 0.9853139, ...</td>\n",
       "      <td>0.872077</td>\n",
       "      <td>0.807531</td>\n",
       "      <td>0.781513</td>\n",
       "      <td>0.589262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.792453</td>\n",
       "      <td>0.783673</td>\n",
       "      <td>[0.0, 0.004201680672268907, 0.0042016806722689...</td>\n",
       "      <td>[0.0, 0.0, 0.0041841004184100415, 0.0041841004...</td>\n",
       "      <td>[1.9888046, 0.98880464, 0.98633575, 0.958263, ...</td>\n",
       "      <td>0.859420</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.778243</td>\n",
       "      <td>0.585181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.787815</td>\n",
       "      <td>0.754647</td>\n",
       "      <td>[0.0, 0.004201680672268907, 0.1302521008403361...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.004201680672268907, 0.004201...</td>\n",
       "      <td>[1.9838555, 0.98385555, 0.947379, 0.94682926, ...</td>\n",
       "      <td>0.877613</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.580576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.777311</td>\n",
       "      <td>0.777311</td>\n",
       "      <td>[0.0, 0.004201680672268907, 0.0504201680672268...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.004201680672268907, 0.004201...</td>\n",
       "      <td>[1.9833007, 0.9833007, 0.9524279, 0.95138997, ...</td>\n",
       "      <td>0.842278</td>\n",
       "      <td>0.777311</td>\n",
       "      <td>0.777311</td>\n",
       "      <td>0.554622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.775210</td>\n",
       "      <td>0.758893</td>\n",
       "      <td>[0.0, 0.004201680672268907, 0.0756302521008403...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.008403361344537815, 0.008403...</td>\n",
       "      <td>[1.991195, 0.99119496, 0.95751673, 0.9549976, ...</td>\n",
       "      <td>0.858635</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.743697</td>\n",
       "      <td>0.551517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold Train_Test  Accuracy  Precision  \\\n",
       "1     0       Test  0.794549   0.787755   \n",
       "3     1       Test  0.792453   0.783673   \n",
       "5     2       Test  0.787815   0.754647   \n",
       "7     3       Test  0.777311   0.777311   \n",
       "9     4       Test  0.775210   0.758893   \n",
       "\n",
       "                                                 TPR  \\\n",
       "1  [0.0, 0.0041841004184100415, 0.012552301255230...   \n",
       "3  [0.0, 0.004201680672268907, 0.0042016806722689...   \n",
       "5  [0.0, 0.004201680672268907, 0.1302521008403361...   \n",
       "7  [0.0, 0.004201680672268907, 0.0504201680672268...   \n",
       "9  [0.0, 0.004201680672268907, 0.0756302521008403...   \n",
       "\n",
       "                                                 FPR  \\\n",
       "1  [0.0, 0.0, 0.0, 0.004201680672268907, 0.004201...   \n",
       "3  [0.0, 0.0, 0.0041841004184100415, 0.0041841004...   \n",
       "5  [0.0, 0.0, 0.0, 0.004201680672268907, 0.004201...   \n",
       "7  [0.0, 0.0, 0.0, 0.004201680672268907, 0.004201...   \n",
       "9  [0.0, 0.0, 0.0, 0.008403361344537815, 0.008403...   \n",
       "\n",
       "                                  TPR_FPR_Thresholds       AUC  Sensitivity  \\\n",
       "1  [1.9944923, 0.9944923, 0.98615485, 0.9853139, ...  0.872077     0.807531   \n",
       "3  [1.9888046, 0.98880464, 0.98633575, 0.958263, ...  0.859420     0.806723   \n",
       "5  [1.9838555, 0.98385555, 0.947379, 0.94682926, ...  0.877613     0.852941   \n",
       "7  [1.9833007, 0.9833007, 0.9524279, 0.95138997, ...  0.842278     0.777311   \n",
       "9  [1.991195, 0.99119496, 0.95751673, 0.9549976, ...  0.858635     0.806723   \n",
       "\n",
       "   Specificity       MCC  \n",
       "1     0.781513  0.589262  \n",
       "3     0.778243  0.585181  \n",
       "5     0.722689  0.580576  \n",
       "7     0.777311  0.554622  \n",
       "9     0.743697  0.551517  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df[evaluations_df[\"Train_Test\"] == \"Test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = features\n",
    "train_labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### read independent data file\n",
    "##################################################################################\n",
    "indpe_file_path = os.path.join(input_data_folder, independent_data_file)\n",
    "indpe_data = pd.read_csv(indpe_file_path, sep='\\t', header=None)\n",
    "indpe_data.columns = ['Sequence', 'name', 'id', 'flag', 'label_original', 'type']\n",
    "indpe_data.head()\n",
    "    \n",
    "##################################################################################\n",
    "##### Create OHE of sequence\n",
    "##################################################################################\n",
    "indpe_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                        for val in indpe_data[\"Sequence\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Fix the labels\n",
    "##################################################################################\n",
    "indpe_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                 for val in indpe_data[\"label_original\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Extract features and labels, create folds\n",
    "##################################################################################\n",
    "\n",
    "indpe_features = np.array(list(indpe_data['OHE_Sequence']))\n",
    "indpe_labels = np.array(list(indpe_data['label']))\n",
    "indpe_labels = indpe_labels.reshape((indpe_labels.shape[0], 1))\n",
    "\n",
    "input_seq_shape = indpe_features[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-fold Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of each k-fold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.594776</td>\n",
       "      <td>0.242527</td>\n",
       "      <td>0.671541</td>\n",
       "      <td>0.678818</td>\n",
       "      <td>0.578082</td>\n",
       "      <td>0.191901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.594776   0.242527  0.671541     0.678818     0.578082  0.191901"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    label_pred = pred2label(y_pred)\n",
    "\n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(indpe_labels, label_pred)\n",
    "    prec = precision_score(indpe_labels,label_pred)\n",
    "    mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "    conf = confusion_matrix(indpe_labels, label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(indpe_labels, y_pred)\n",
    "    auc = roc_auc_score(indpe_labels, y_pred)\n",
    "\n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.597551</td>\n",
       "      <td>0.249135</td>\n",
       "      <td>[0.0, 0.0049261083743842365, 0.004926108374384...</td>\n",
       "      <td>[0.0, 0.0, 0.0019569471624266144, 0.0019569471...</td>\n",
       "      <td>[1.9954306, 0.9954306, 0.9887334, 0.9879417, 0...</td>\n",
       "      <td>0.681172</td>\n",
       "      <td>0.709360</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>0.212055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.255396</td>\n",
       "      <td>[0.0, 0.0049261083743842365, 0.009852216748768...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0029354207436399216, 0.00293...</td>\n",
       "      <td>[1.9897283, 0.98972833, 0.9892013, 0.98633575,...</td>\n",
       "      <td>0.673633</td>\n",
       "      <td>0.699507</td>\n",
       "      <td>0.594912</td>\n",
       "      <td>0.219882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.550204</td>\n",
       "      <td>0.228125</td>\n",
       "      <td>[0.0, 0.0049261083743842365, 0.004926108374384...</td>\n",
       "      <td>[0.0, 0.0, 0.0009784735812133072, 0.0009784735...</td>\n",
       "      <td>[1.9900972, 0.99009717, 0.98125285, 0.9802611,...</td>\n",
       "      <td>0.668167</td>\n",
       "      <td>0.719212</td>\n",
       "      <td>0.516634</td>\n",
       "      <td>0.175563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.601633</td>\n",
       "      <td>0.238532</td>\n",
       "      <td>[0.0, 0.0049261083743842365, 0.004926108374384...</td>\n",
       "      <td>[0.0, 0.0, 0.0019569471624266144, 0.0019569471...</td>\n",
       "      <td>[1.9844738, 0.9844738, 0.9804381, 0.97816294, ...</td>\n",
       "      <td>0.664634</td>\n",
       "      <td>0.640394</td>\n",
       "      <td>0.593933</td>\n",
       "      <td>0.175325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.612245</td>\n",
       "      <td>0.241445</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.009852216748768473, 0.009852...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.001956947162426...</td>\n",
       "      <td>[1.9875392, 0.9875392, 0.9834366, 0.9805925, 0...</td>\n",
       "      <td>0.670100</td>\n",
       "      <td>0.625616</td>\n",
       "      <td>0.609589</td>\n",
       "      <td>0.176680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold   Train_Test  Accuracy  Precision  \\\n",
       "0     0  Independent  0.597551   0.249135   \n",
       "1     1  Independent  0.612245   0.255396   \n",
       "2     2  Independent  0.550204   0.228125   \n",
       "3     3  Independent  0.601633   0.238532   \n",
       "4     4  Independent  0.612245   0.241445   \n",
       "\n",
       "                                                 TPR  \\\n",
       "0  [0.0, 0.0049261083743842365, 0.004926108374384...   \n",
       "1  [0.0, 0.0049261083743842365, 0.009852216748768...   \n",
       "2  [0.0, 0.0049261083743842365, 0.004926108374384...   \n",
       "3  [0.0, 0.0049261083743842365, 0.004926108374384...   \n",
       "4  [0.0, 0.0, 0.0, 0.009852216748768473, 0.009852...   \n",
       "\n",
       "                                                 FPR  \\\n",
       "0  [0.0, 0.0, 0.0019569471624266144, 0.0019569471...   \n",
       "1  [0.0, 0.0, 0.0, 0.0029354207436399216, 0.00293...   \n",
       "2  [0.0, 0.0, 0.0009784735812133072, 0.0009784735...   \n",
       "3  [0.0, 0.0, 0.0019569471624266144, 0.0019569471...   \n",
       "4  [0.0, 0.0009784735812133072, 0.001956947162426...   \n",
       "\n",
       "                                  TPR_FPR_Thresholds       AUC  Sensitivity  \\\n",
       "0  [1.9954306, 0.9954306, 0.9887334, 0.9879417, 0...  0.681172     0.709360   \n",
       "1  [1.9897283, 0.98972833, 0.9892013, 0.98633575,...  0.673633     0.699507   \n",
       "2  [1.9900972, 0.99009717, 0.98125285, 0.9802611,...  0.668167     0.719212   \n",
       "3  [1.9844738, 0.9844738, 0.9804381, 0.97816294, ...  0.664634     0.640394   \n",
       "4  [1.9875392, 0.9875392, 0.9834366, 0.9805925, 0...  0.670100     0.625616   \n",
       "\n",
       "   Specificity       MCC  \n",
       "0     0.575342  0.212055  \n",
       "1     0.594912  0.219882  \n",
       "2     0.516634  0.175563  \n",
       "3     0.593933  0.175325  \n",
       "4     0.609589  0.176680  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean score with k-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.599184</td>\n",
       "      <td>0.243772</td>\n",
       "      <td>0.682762</td>\n",
       "      <td>0.674877</td>\n",
       "      <td>0.584149</td>\n",
       "      <td>0.193282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.599184   0.243772  0.682762     0.674877     0.584149  0.193282"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "total_pred = np.zeros(indpe_labels.shape)\n",
    "all_preds = []\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    total_pred += y_pred\n",
    "    all_preds.append(y_pred)\n",
    "    \n",
    "total_pred = total_pred / n_fold\n",
    "label_pred = pred2label(total_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, total_pred)\n",
    "auc = roc_auc_score(indpe_labels, total_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting score with k-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.598367</td>\n",
       "      <td>0.24515</td>\n",
       "      <td>0.67445</td>\n",
       "      <td>0.684729</td>\n",
       "      <td>0.581213</td>\n",
       "      <td>0.198315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision      AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Independent  0.598367    0.24515  0.67445     0.684729     0.581213  0.198315"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "total_pred = np.zeros(indpe_labels.shape)\n",
    "all_preds = []\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    vote_pred = pred2label(y_pred)\n",
    "    total_pred += vote_pred\n",
    "    all_preds.append(vote_pred)\n",
    "    \n",
    "total_pred = total_pred / n_fold\n",
    "label_pred = pred2label(total_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, total_pred)\n",
    "auc = roc_auc_score(indpe_labels, total_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using New Model\n",
    "\n",
    "Train model on full data from training. Predict and evaluate on Independent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_indexes = np.where(indpe_labels==1)[0]\n",
    "neg_indexes = np.random.permutation(np.where(indpe_labels==0)[0])[0:pos_indexes.shape[0]]\n",
    "indpe_val_indexes = np.concatenate((pos_indexes, neg_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.8569 - accuracy: 0.5562\n",
      "Epoch 1: val_accuracy improved from -inf to 0.59606, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\_fullModel.hdf5\n",
      "298/298 [==============================] - 4s 9ms/step - loss: 0.8561 - accuracy: 0.5571 - val_loss: 0.8078 - val_accuracy: 0.5961\n",
      "Epoch 2/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.7084 - accuracy: 0.6915\n",
      "Epoch 2: val_accuracy did not improve from 0.59606\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.7081 - accuracy: 0.6914 - val_loss: 0.7918 - val_accuracy: 0.5887\n",
      "Epoch 3/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.6491 - accuracy: 0.7197\n",
      "Epoch 3: val_accuracy improved from 0.59606 to 0.60099, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\_fullModel.hdf5\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.6485 - accuracy: 0.7200 - val_loss: 0.7771 - val_accuracy: 0.6010\n",
      "Epoch 4/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.6132 - accuracy: 0.7337\n",
      "Epoch 4: val_accuracy improved from 0.60099 to 0.60837, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\_fullModel.hdf5\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.6126 - accuracy: 0.7349 - val_loss: 0.7438 - val_accuracy: 0.6084\n",
      "Epoch 5/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.5863 - accuracy: 0.7481\n",
      "Epoch 5: val_accuracy did not improve from 0.60837\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.5863 - accuracy: 0.7481 - val_loss: 0.7503 - val_accuracy: 0.6034\n",
      "Epoch 6/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.5760 - accuracy: 0.7426\n",
      "Epoch 6: val_accuracy did not improve from 0.60837\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.5753 - accuracy: 0.7431 - val_loss: 0.7492 - val_accuracy: 0.6034\n",
      "Epoch 7/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.5541 - accuracy: 0.7597\n",
      "Epoch 7: val_accuracy improved from 0.60837 to 0.62315, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\_fullModel.hdf5\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.5541 - accuracy: 0.7597 - val_loss: 0.7249 - val_accuracy: 0.6232\n",
      "Epoch 8/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.5413 - accuracy: 0.7666\n",
      "Epoch 8: val_accuracy did not improve from 0.62315\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.5412 - accuracy: 0.7666 - val_loss: 0.7541 - val_accuracy: 0.6158\n",
      "Epoch 9/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.5329 - accuracy: 0.7677\n",
      "Epoch 9: val_accuracy improved from 0.62315 to 0.62562, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\_fullModel.hdf5\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.5330 - accuracy: 0.7681 - val_loss: 0.7343 - val_accuracy: 0.6256\n",
      "Epoch 10/100\n",
      "291/298 [============================>.] - ETA: 0s - loss: 0.5235 - accuracy: 0.7693\n",
      "Epoch 10: val_accuracy improved from 0.62562 to 0.63054, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\_fullModel.hdf5\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.5218 - accuracy: 0.7706 - val_loss: 0.7540 - val_accuracy: 0.6305\n",
      "Epoch 11/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.5199 - accuracy: 0.7718\n",
      "Epoch 11: val_accuracy did not improve from 0.63054\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.5199 - accuracy: 0.7718 - val_loss: 0.7402 - val_accuracy: 0.6232\n",
      "Epoch 12/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.5162 - accuracy: 0.7804\n",
      "Epoch 12: val_accuracy did not improve from 0.63054\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.5162 - accuracy: 0.7804 - val_loss: 0.7328 - val_accuracy: 0.6232\n",
      "Epoch 13/100\n",
      "291/298 [============================>.] - ETA: 0s - loss: 0.5003 - accuracy: 0.7822\n",
      "Epoch 13: val_accuracy did not improve from 0.63054\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4999 - accuracy: 0.7817 - val_loss: 0.7269 - val_accuracy: 0.6158\n",
      "Epoch 14/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.4974 - accuracy: 0.7864\n",
      "Epoch 14: val_accuracy did not improve from 0.63054\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4967 - accuracy: 0.7872 - val_loss: 0.7424 - val_accuracy: 0.6256\n",
      "Epoch 15/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.4938 - accuracy: 0.7877\n",
      "Epoch 15: val_accuracy did not improve from 0.63054\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4936 - accuracy: 0.7876 - val_loss: 0.7463 - val_accuracy: 0.6010\n",
      "Epoch 16/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.4931 - accuracy: 0.7819\n",
      "Epoch 16: val_accuracy did not improve from 0.63054\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4924 - accuracy: 0.7825 - val_loss: 0.7388 - val_accuracy: 0.6305\n",
      "Epoch 17/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4803 - accuracy: 0.7986\n",
      "Epoch 17: val_accuracy did not improve from 0.63054\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4810 - accuracy: 0.7974 - val_loss: 0.7316 - val_accuracy: 0.6158\n",
      "Epoch 18/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.4700 - accuracy: 0.8071\n",
      "Epoch 18: val_accuracy improved from 0.63054 to 0.63300, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\_fullModel.hdf5\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4700 - accuracy: 0.8071 - val_loss: 0.7802 - val_accuracy: 0.6330\n",
      "Epoch 19/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.4720 - accuracy: 0.8024\n",
      "Epoch 19: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.4714 - accuracy: 0.8029 - val_loss: 0.7849 - val_accuracy: 0.6158\n",
      "Epoch 20/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.4682 - accuracy: 0.8004\n",
      "Epoch 20: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4672 - accuracy: 0.8010 - val_loss: 0.7856 - val_accuracy: 0.6281\n",
      "Epoch 21/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4630 - accuracy: 0.8038\n",
      "Epoch 21: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4641 - accuracy: 0.8031 - val_loss: 0.7526 - val_accuracy: 0.6305\n",
      "Epoch 22/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4647 - accuracy: 0.8080\n",
      "Epoch 22: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.4641 - accuracy: 0.8077 - val_loss: 0.7506 - val_accuracy: 0.6256\n",
      "Epoch 23/100\n",
      "291/298 [============================>.] - ETA: 0s - loss: 0.4579 - accuracy: 0.8174\n",
      "Epoch 23: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 7ms/step - loss: 0.4569 - accuracy: 0.8182 - val_loss: 0.7710 - val_accuracy: 0.6182\n",
      "Epoch 24/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4576 - accuracy: 0.8151\n",
      "Epoch 24: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4572 - accuracy: 0.8147 - val_loss: 0.7798 - val_accuracy: 0.6158\n",
      "Epoch 25/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4476 - accuracy: 0.8236\n",
      "Epoch 25: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4473 - accuracy: 0.8237 - val_loss: 0.7774 - val_accuracy: 0.6182\n",
      "Epoch 26/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.4347 - accuracy: 0.8186\n",
      "Epoch 26: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4352 - accuracy: 0.8184 - val_loss: 0.7907 - val_accuracy: 0.6182\n",
      "Epoch 27/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.4391 - accuracy: 0.8246\n",
      "Epoch 27: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4397 - accuracy: 0.8247 - val_loss: 0.7782 - val_accuracy: 0.6256\n",
      "Epoch 28/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.8203\n",
      "Epoch 28: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4427 - accuracy: 0.8203 - val_loss: 0.7658 - val_accuracy: 0.6256\n",
      "Epoch 29/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.4317 - accuracy: 0.8259\n",
      "Epoch 29: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4310 - accuracy: 0.8260 - val_loss: 0.7656 - val_accuracy: 0.6010\n",
      "Epoch 30/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.4338 - accuracy: 0.8233\n",
      "Epoch 30: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4337 - accuracy: 0.8228 - val_loss: 0.7606 - val_accuracy: 0.6256\n",
      "Epoch 31/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8281\n",
      "Epoch 31: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4355 - accuracy: 0.8283 - val_loss: 0.7744 - val_accuracy: 0.6182\n",
      "Epoch 32/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4236 - accuracy: 0.8268\n",
      "Epoch 32: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4224 - accuracy: 0.8277 - val_loss: 0.7693 - val_accuracy: 0.6133\n",
      "Epoch 33/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.4309 - accuracy: 0.8184\n",
      "Epoch 33: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4299 - accuracy: 0.8195 - val_loss: 0.7948 - val_accuracy: 0.6207\n",
      "Epoch 34/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.4361 - accuracy: 0.8220\n",
      "Epoch 34: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4353 - accuracy: 0.8224 - val_loss: 0.7956 - val_accuracy: 0.5911\n",
      "Epoch 35/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8271\n",
      "Epoch 35: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4277 - accuracy: 0.8268 - val_loss: 0.8093 - val_accuracy: 0.6158\n",
      "Epoch 36/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.4190 - accuracy: 0.8314\n",
      "Epoch 36: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4190 - accuracy: 0.8314 - val_loss: 0.7783 - val_accuracy: 0.6108\n",
      "Epoch 37/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4256 - accuracy: 0.8311\n",
      "Epoch 37: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4261 - accuracy: 0.8306 - val_loss: 0.8012 - val_accuracy: 0.6084\n",
      "Epoch 38/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8313\n",
      "Epoch 38: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4224 - accuracy: 0.8308 - val_loss: 0.8174 - val_accuracy: 0.6034\n",
      "Epoch 39/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4269 - accuracy: 0.8223\n",
      "Epoch 39: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4276 - accuracy: 0.8228 - val_loss: 0.8385 - val_accuracy: 0.6010\n",
      "Epoch 40/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.4166 - accuracy: 0.8346\n",
      "Epoch 40: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4173 - accuracy: 0.8342 - val_loss: 0.8212 - val_accuracy: 0.6108\n",
      "Epoch 41/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4150 - accuracy: 0.8318\n",
      "Epoch 41: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4162 - accuracy: 0.8308 - val_loss: 0.8112 - val_accuracy: 0.5887\n",
      "Epoch 42/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.8293\n",
      "Epoch 42: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4188 - accuracy: 0.8293 - val_loss: 0.8096 - val_accuracy: 0.6158\n",
      "Epoch 43/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.4176 - accuracy: 0.8352\n",
      "Epoch 43: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4165 - accuracy: 0.8363 - val_loss: 0.8884 - val_accuracy: 0.6207\n",
      "Epoch 44/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4172 - accuracy: 0.8364\n",
      "Epoch 44: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4166 - accuracy: 0.8369 - val_loss: 0.8283 - val_accuracy: 0.6034\n",
      "Epoch 45/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8330\n",
      "Epoch 45: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4192 - accuracy: 0.8329 - val_loss: 0.8101 - val_accuracy: 0.6133\n",
      "Epoch 46/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4080 - accuracy: 0.8355\n",
      "Epoch 46: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4093 - accuracy: 0.8348 - val_loss: 0.7785 - val_accuracy: 0.6084\n",
      "Epoch 47/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.4009 - accuracy: 0.8393\n",
      "Epoch 47: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4019 - accuracy: 0.8392 - val_loss: 0.8591 - val_accuracy: 0.6305\n",
      "Epoch 48/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4115 - accuracy: 0.8408\n",
      "Epoch 48: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4102 - accuracy: 0.8417 - val_loss: 0.8071 - val_accuracy: 0.6084\n",
      "Epoch 49/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.4117 - accuracy: 0.8350\n",
      "Epoch 49: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4110 - accuracy: 0.8350 - val_loss: 0.7927 - val_accuracy: 0.6084\n",
      "Epoch 50/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.4117 - accuracy: 0.8379\n",
      "Epoch 50: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4116 - accuracy: 0.8380 - val_loss: 0.8269 - val_accuracy: 0.6133\n",
      "Epoch 51/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4146 - accuracy: 0.8289\n",
      "Epoch 51: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4138 - accuracy: 0.8293 - val_loss: 0.7958 - val_accuracy: 0.6108\n",
      "Epoch 52/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.4137 - accuracy: 0.8382\n",
      "Epoch 52: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4133 - accuracy: 0.8384 - val_loss: 0.8157 - val_accuracy: 0.6084\n",
      "Epoch 53/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4063 - accuracy: 0.8385\n",
      "Epoch 53: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4063 - accuracy: 0.8386 - val_loss: 0.8528 - val_accuracy: 0.6281\n",
      "Epoch 54/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.4047 - accuracy: 0.8402\n",
      "Epoch 54: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4047 - accuracy: 0.8398 - val_loss: 0.8659 - val_accuracy: 0.6330\n",
      "Epoch 55/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.4075 - accuracy: 0.8384\n",
      "Epoch 55: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4080 - accuracy: 0.8380 - val_loss: 0.8299 - val_accuracy: 0.6010\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/298 [============================>.] - ETA: 0s - loss: 0.3994 - accuracy: 0.8451\n",
      "Epoch 56: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3995 - accuracy: 0.8447 - val_loss: 0.8181 - val_accuracy: 0.6158\n",
      "Epoch 57/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.8398\n",
      "Epoch 57: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4019 - accuracy: 0.8398 - val_loss: 0.8543 - val_accuracy: 0.6305\n",
      "Epoch 58/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4079 - accuracy: 0.8382\n",
      "Epoch 58: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4085 - accuracy: 0.8382 - val_loss: 0.8190 - val_accuracy: 0.6158\n",
      "Epoch 59/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8403\n",
      "Epoch 59: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4042 - accuracy: 0.8401 - val_loss: 0.8308 - val_accuracy: 0.6207\n",
      "Epoch 60/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.4049 - accuracy: 0.8392\n",
      "Epoch 60: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4048 - accuracy: 0.8394 - val_loss: 0.8097 - val_accuracy: 0.6034\n",
      "Epoch 61/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.3971 - accuracy: 0.8451\n",
      "Epoch 61: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3971 - accuracy: 0.8451 - val_loss: 0.8558 - val_accuracy: 0.6034\n",
      "Epoch 62/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.4080 - accuracy: 0.8350\n",
      "Epoch 62: val_accuracy did not improve from 0.63300\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4070 - accuracy: 0.8356 - val_loss: 0.8207 - val_accuracy: 0.6059\n",
      "Epoch 63/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8463\n",
      "Epoch 63: val_accuracy improved from 0.63300 to 0.63793, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainsStrat_Reverse\\5fold\\models\\_fullModel.hdf5\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3956 - accuracy: 0.8463 - val_loss: 0.8673 - val_accuracy: 0.6379\n",
      "Epoch 64/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8466\n",
      "Epoch 64: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3910 - accuracy: 0.8455 - val_loss: 0.8771 - val_accuracy: 0.6281\n",
      "Epoch 65/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4068 - accuracy: 0.8412\n",
      "Epoch 65: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4096 - accuracy: 0.8390 - val_loss: 0.8238 - val_accuracy: 0.6232\n",
      "Epoch 66/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4056 - accuracy: 0.8378\n",
      "Epoch 66: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4076 - accuracy: 0.8375 - val_loss: 0.8523 - val_accuracy: 0.6256\n",
      "Epoch 67/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.4007 - accuracy: 0.8386\n",
      "Epoch 67: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4007 - accuracy: 0.8388 - val_loss: 0.8764 - val_accuracy: 0.6207\n",
      "Epoch 68/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.3864 - accuracy: 0.8481\n",
      "Epoch 68: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3861 - accuracy: 0.8482 - val_loss: 0.8141 - val_accuracy: 0.6059\n",
      "Epoch 69/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.3961 - accuracy: 0.8450\n",
      "Epoch 69: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3965 - accuracy: 0.8453 - val_loss: 0.8477 - val_accuracy: 0.6010\n",
      "Epoch 70/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.3909 - accuracy: 0.8445\n",
      "Epoch 70: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3907 - accuracy: 0.8445 - val_loss: 0.8359 - val_accuracy: 0.6108\n",
      "Epoch 71/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.4059 - accuracy: 0.8393\n",
      "Epoch 71: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4052 - accuracy: 0.8398 - val_loss: 0.8549 - val_accuracy: 0.6256\n",
      "Epoch 72/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8459\n",
      "Epoch 72: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3914 - accuracy: 0.8459 - val_loss: 0.9233 - val_accuracy: 0.6379\n",
      "Epoch 73/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.3914 - accuracy: 0.8444\n",
      "Epoch 73: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3897 - accuracy: 0.8455 - val_loss: 0.8433 - val_accuracy: 0.6010\n",
      "Epoch 74/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.4026 - accuracy: 0.8372\n",
      "Epoch 74: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4028 - accuracy: 0.8363 - val_loss: 0.8244 - val_accuracy: 0.6305\n",
      "Epoch 75/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.3947 - accuracy: 0.8417\n",
      "Epoch 75: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3958 - accuracy: 0.8415 - val_loss: 0.8467 - val_accuracy: 0.6059\n",
      "Epoch 76/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.3975 - accuracy: 0.8410\n",
      "Epoch 76: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3983 - accuracy: 0.8401 - val_loss: 0.8537 - val_accuracy: 0.5985\n",
      "Epoch 77/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.3983 - accuracy: 0.8449\n",
      "Epoch 77: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3982 - accuracy: 0.8451 - val_loss: 0.8084 - val_accuracy: 0.6108\n",
      "Epoch 78/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.3969 - accuracy: 0.8403\n",
      "Epoch 78: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3977 - accuracy: 0.8401 - val_loss: 0.8184 - val_accuracy: 0.6158\n",
      "Epoch 79/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.4096 - accuracy: 0.8348\n",
      "Epoch 79: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.4093 - accuracy: 0.8348 - val_loss: 0.8084 - val_accuracy: 0.6379\n",
      "Epoch 80/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.3940 - accuracy: 0.8418\n",
      "Epoch 80: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3937 - accuracy: 0.8419 - val_loss: 0.8471 - val_accuracy: 0.6207\n",
      "Epoch 81/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.3954 - accuracy: 0.8464\n",
      "Epoch 81: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3955 - accuracy: 0.8463 - val_loss: 0.8614 - val_accuracy: 0.6108\n",
      "Epoch 82/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.3848 - accuracy: 0.8510\n",
      "Epoch 82: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3858 - accuracy: 0.8499 - val_loss: 0.8688 - val_accuracy: 0.6305\n",
      "Epoch 83/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.3896 - accuracy: 0.8506\n",
      "Epoch 83: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3892 - accuracy: 0.8503 - val_loss: 0.8748 - val_accuracy: 0.6207\n",
      "Epoch 84/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.3879 - accuracy: 0.8456\n",
      "Epoch 84: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3899 - accuracy: 0.8447 - val_loss: 0.8372 - val_accuracy: 0.6232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.3911 - accuracy: 0.8415\n",
      "Epoch 85: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3913 - accuracy: 0.8409 - val_loss: 0.8918 - val_accuracy: 0.6281\n",
      "Epoch 86/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.3913 - accuracy: 0.8411\n",
      "Epoch 86: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3909 - accuracy: 0.8413 - val_loss: 0.9128 - val_accuracy: 0.6133\n",
      "Epoch 87/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.3950 - accuracy: 0.8461\n",
      "Epoch 87: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3974 - accuracy: 0.8447 - val_loss: 0.8612 - val_accuracy: 0.6182\n",
      "Epoch 88/100\n",
      "297/298 [============================>.] - ETA: 0s - loss: 0.3901 - accuracy: 0.8430\n",
      "Epoch 88: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3899 - accuracy: 0.8434 - val_loss: 0.8943 - val_accuracy: 0.6256\n",
      "Epoch 89/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.3851 - accuracy: 0.8496\n",
      "Epoch 89: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3860 - accuracy: 0.8495 - val_loss: 0.8728 - val_accuracy: 0.5961\n",
      "Epoch 90/100\n",
      "293/298 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8509\n",
      "Epoch 90: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3900 - accuracy: 0.8514 - val_loss: 0.8310 - val_accuracy: 0.6133\n",
      "Epoch 91/100\n",
      "296/298 [============================>.] - ETA: 0s - loss: 0.3908 - accuracy: 0.8433\n",
      "Epoch 91: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3904 - accuracy: 0.8434 - val_loss: 0.8780 - val_accuracy: 0.6084\n",
      "Epoch 92/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.3895 - accuracy: 0.8460\n",
      "Epoch 92: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3891 - accuracy: 0.8463 - val_loss: 0.8484 - val_accuracy: 0.5985\n",
      "Epoch 93/100\n",
      "298/298 [==============================] - ETA: 0s - loss: 0.3933 - accuracy: 0.8442\n",
      "Epoch 93: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3933 - accuracy: 0.8442 - val_loss: 0.8422 - val_accuracy: 0.6010\n",
      "Epoch 94/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.3903 - accuracy: 0.8489\n",
      "Epoch 94: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3908 - accuracy: 0.8480 - val_loss: 0.8688 - val_accuracy: 0.6281\n",
      "Epoch 95/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8515\n",
      "Epoch 95: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3801 - accuracy: 0.8522 - val_loss: 0.8711 - val_accuracy: 0.6207\n",
      "Epoch 96/100\n",
      "294/298 [============================>.] - ETA: 0s - loss: 0.3967 - accuracy: 0.8465\n",
      "Epoch 96: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3957 - accuracy: 0.8470 - val_loss: 0.8630 - val_accuracy: 0.6010\n",
      "Epoch 97/100\n",
      "295/298 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8540\n",
      "Epoch 97: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3715 - accuracy: 0.8535 - val_loss: 0.8675 - val_accuracy: 0.6108\n",
      "Epoch 98/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.8345\n",
      "Epoch 98: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3950 - accuracy: 0.8361 - val_loss: 0.8409 - val_accuracy: 0.5961\n",
      "Epoch 99/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.3786 - accuracy: 0.8527\n",
      "Epoch 99: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3774 - accuracy: 0.8535 - val_loss: 0.9589 - val_accuracy: 0.6207\n",
      "Epoch 100/100\n",
      "292/298 [============================>.] - ETA: 0s - loss: 0.3909 - accuracy: 0.8457\n",
      "Epoch 100: val_accuracy did not improve from 0.63793\n",
      "298/298 [==============================] - 2s 8ms/step - loss: 0.3899 - accuracy: 0.8468 - val_loss: 0.8201 - val_accuracy: 0.6207\n"
     ]
    }
   ],
   "source": [
    "model = DLNN_CORENup(input_seq_shape = input_seq_shape)\n",
    "    \n",
    "## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "current_model_path = os.path.join(modelPath, \"_fullModel.hdf5\")\n",
    "modelCallbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
    "                                       monitor = 'val_accuracy', verbose = 1, save_best_only = True, \n",
    "                                       save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "]\n",
    "\n",
    "X_features = np.concatenate((train_features, \n",
    "                             np.flip(train_features, axis=1)),\n",
    "                            axis = 0\n",
    "                           )\n",
    "Y_labels = np.concatenate((train_labels, \n",
    "                           train_labels), \n",
    "                          axis = 0)\n",
    "\n",
    "# adding random shuffling of the dataset for training purpose\n",
    "index_arr = np.arange(X_features.shape[0])\n",
    "index_arr = np.random.permutation(index_arr)\n",
    "\n",
    "model.fit(x = X_features[index_arr], y = Y_labels[index_arr], batch_size = 16, epochs = epochs, verbose = 1, \n",
    "          callbacks = modelCallbacks, validation_data = (indpe_features[indpe_val_indexes], indpe_labels[indpe_val_indexes]))\n",
    "# model.fit(x = train_features[index_arr], y = train_labels[index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "#           callbacks = modelCallbacks, validation_data = (indpe_features, indpe_labels))\n",
    "# model.fit(x = train_features[index_arr], y = train_labels[index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "#           callbacks = modelCallbacks, validation_split = 0.2)\n",
    "\n",
    "model = tf.keras.models.load_model(current_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.667755</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>0.665671</td>\n",
       "      <td>0.596059</td>\n",
       "      <td>0.681996</td>\n",
       "      <td>0.214866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.667755     0.2713  0.665671     0.596059     0.681996  0.214866"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "y_pred = model.predict(indpe_features)\n",
    "label_pred = pred2label(y_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, y_pred)\n",
    "auc = roc_auc_score(indpe_labels, y_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
