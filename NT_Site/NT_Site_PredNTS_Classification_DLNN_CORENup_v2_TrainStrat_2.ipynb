{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Define all parameters for model tuning\n",
    "##################################################################################\n",
    "\n",
    "n_fold = 5\n",
    "expName = \"NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\"\n",
    "outPath = \"Results\"\n",
    "foldName = \"folds.pickle\"\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "seed = None\n",
    "\n",
    "input_data_folder = \"Data\"\n",
    "training_data_file = \"Training-datasets-PredNTS.txt\"\n",
    "independent_data_file = \"independent dataset-PredNTS.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score, precision_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef, classification_report\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# print(tf.test.is_gpu_available(cuda_only=True))\n",
    "# physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define all CUSTOM functions\n",
    "##################################################################################\n",
    "\n",
    "def one_hot_encode_nt(sequence, char_dict):\n",
    "    \n",
    "    seq_encoded = np.zeros((len(sequence),len(char_dict)))\n",
    "    \n",
    "    i = 0\n",
    "    for single_character in sequence:\n",
    "        if(single_character.upper() in char_dict.keys()):\n",
    "            seq_encoded[i][char_dict[single_character.upper()]] = 1\n",
    "            i = i+1\n",
    "        else:\n",
    "            raise ValueError('Incorrect character in NT sequence: '+sequence)\n",
    "    return seq_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Build k-fold functions\n",
    "##################################################################################\n",
    "\n",
    "## Build the K-fold from dataset\n",
    "def build_kfold(features, labels, k=10, shuffle=False, seed=None):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=shuffle, random_state=seed)\n",
    "    kfoldList = []\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        kfoldList.append({\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\":y_train,\n",
    "            \"y_test\":y_test\n",
    "        })\n",
    "    return kfoldList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### define evaluator functions\n",
    "##################################################################################\n",
    "\n",
    "def pred2label(y_pred):\n",
    "    y_pred = np.round(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "def labels_1d_to_2d(labels_1d):\n",
    "    labels_2d = np.eye(2)[labels_1d]\n",
    "    return labels_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Function to customize the DLNN architecture with parameters\n",
    "##################################################################################\n",
    "\n",
    "def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "                 conv_filters_per_layer_1 = 50, kernel_length_1 = 5, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "                 max_pool_width_1 = 2, max_pool_stride_1 = 2, ## 1st Maxpool layer parameters\n",
    "                 lstm_decode_units = 50, ## LSTM layer parameters\n",
    "                 conv_filters_per_layer_2 = 50,  kernel_length_2 = 10, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "                 max_pool_width_2 = 2, max_pool_stride_2 = 2, ## 2nd Maxpool layer parameters\n",
    "                 dense_decode_units = 370, ## Dense layer parameters\n",
    "                 prob = 0.5, learn_rate = 0.0003, loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "    beta = 0.001\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  SEQUENCE  ##################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "                                strides = conv_strides_1, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = \"same\")(input1)\n",
    "    x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "    x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "    x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "    ## LSTM Path\n",
    "\n",
    "    x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "    ## Conv Path\n",
    "\n",
    "    x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, \n",
    "                                strides = conv_strides_2, \n",
    "                                kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                                padding = 'same')(x1)\n",
    "    x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "    x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "    x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "    x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "    x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ########  Classifier  ################################################################################\n",
    "    ######################################################################################################\n",
    "    \n",
    "    y = tf.keras.layers.Dense(dense_decode_units, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'relu')(x4)\n",
    "    \n",
    "    y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "    y = tf.keras.layers.Dense(1, \n",
    "                              kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "                              activation = 'sigmoid')(y)\n",
    "\n",
    "    ## Generate Model from input and output\n",
    "    model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "    ## Compile model\n",
    "    if(metrics != None):\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), \n",
    "                      loss = loss, metrics = metrics)\n",
    "    else:\n",
    "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), \n",
    "                      loss = loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################################################################################\n",
    "# ##### Function to customize the DLNN architecture with parameters\n",
    "# ##################################################################################\n",
    "\n",
    "# def DLNN_CORENup(input_seq_shape = (41, 21),\n",
    "#                  conv_filters_per_layer_1 = 10, kernel_length_1 = 10, conv_strides_1 = 1, ## 1st Convolutional layer parameters\n",
    "#                  max_pool_width_1 = 3, max_pool_stride_1 = 3, ## 1st Maxpool layer parameters\n",
    "#                  lstm_decode_units = 10, ## LSTM layer parameters\n",
    "#                  conv_filters_per_layer_2 = 10,  kernel_length_2 = 5, conv_strides_2 = 1, ## 2nd Convolutional layer parameters\n",
    "#                  max_pool_width_2 = 3, max_pool_stride_2 = 3, ## 2nd Maxpool layer parameters\n",
    "#                  dense_decode_units = 32, ## Dense layer parameters\n",
    "#                  prob = 0.5, learn_rate = 0.0005, \n",
    "#                  loss = 'binary_crossentropy', metrics = None):\n",
    "    \n",
    "#     beta = 0.001\n",
    "    \n",
    "#     ######################################################################################################\n",
    "#     ########  SEQUENCE  ##################################################################################\n",
    "#     ######################################################################################################\n",
    "    \n",
    "#     input1 = tf.keras.layers.Input(shape=input_seq_shape)\n",
    "\n",
    "#     x1 = tf.keras.layers.Conv1D(conv_filters_per_layer_1, kernel_length_1,\n",
    "#                                 strides = conv_strides_1, kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                                 padding = \"same\")(input1)\n",
    "#     x1 = tf.keras.layers.Activation('relu')(x1)\n",
    "#     x1 = tf.keras.layers.MaxPool1D(pool_size = max_pool_width_1, strides = max_pool_stride_1)(x1)\n",
    "#     x1 = tf.keras.layers.Dropout(prob)(x1)\n",
    "\n",
    "#     ## LSTM Path\n",
    "\n",
    "#     x2 = tf.keras.layers.LSTM(lstm_decode_units, return_sequences = True, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta))(x1)\n",
    "    \n",
    "#     x2 = tf.keras.layers.Dropout(prob)(x2)\n",
    "    \n",
    "#     x2 = tf.keras.layers.Flatten()(x2)\n",
    "\n",
    "#     ## Conv Path\n",
    "\n",
    "#     x3 = tf.keras.layers.Conv1D(conv_filters_per_layer_2, kernel_length_2, strides = conv_strides_2, \n",
    "#                                 kernel_regularizer = tf.keras.regularizers.l2(beta), padding = 'same')(x1)\n",
    "#     x3 = tf.keras.layers.Activation('relu')(x3)\n",
    "#     x3 = tf.keras.layers.MaxPooling1D(pool_size = max_pool_width_2, strides = max_pool_stride_2)(x3)\n",
    "#     x3 = tf.keras.layers.Dropout(prob)(x3)\n",
    "    \n",
    "#     x3 = tf.keras.layers.Flatten()(x3)\n",
    "    \n",
    "#     x4 = tf.keras.layers.Concatenate(1)([x2,x3])\n",
    "    \n",
    "#     ######################################################################################################\n",
    "#     ########  Classifier  ################################################################################\n",
    "#     ######################################################################################################\n",
    "    \n",
    "#     y = tf.keras.layers.Dense(dense_decode_units, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta), \n",
    "#                               activation = 'relu')(x4)\n",
    "    \n",
    "#     y = tf.keras.layers.Dropout(prob)(y)\n",
    "    \n",
    "#     y = tf.keras.layers.Dense(1, \n",
    "#                               kernel_regularizer = tf.keras.regularizers.l2(beta),\n",
    "#                               activation = 'sigmoid')(y)\n",
    "\n",
    "#     ## Generate Model from input and output\n",
    "#     model = tf.keras.models.Model(inputs=input1, outputs=y)\n",
    "    \n",
    "#     ## Compile model\n",
    "#     if(metrics != None):\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss, metrics = metrics)\n",
    "#     else:\n",
    "#         model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate), loss = loss)\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 41, 21)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 41, 50)       5300        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 41, 50)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 20, 50)       0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 20, 50)       0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 20, 50)       25050       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 20, 50)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 20, 50)       20200       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 10, 50)      0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 20, 50)       0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 10, 50)       0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1000)         0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 500)          0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 1500)         0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 370)          555370      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 370)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            371         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 606,291\n",
      "Trainable params: 606,291\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DLNN_CORENup().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### read training file\n",
    "##################################################################################\n",
    "train_file_path = os.path.join(input_data_folder, training_data_file)\n",
    "train_data = pd.read_csv(train_file_path, sep='\\t', header=None)\n",
    "train_data.columns = ['Sequence', 'name', 'id', 'flag', 'label_original', 'type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### read independent data file\n",
    "##################################################################################\n",
    "indpe_file_path = os.path.join(input_data_folder, independent_data_file)\n",
    "indpe_data = pd.read_csv(indpe_file_path, sep='\\t', header=None)\n",
    "indpe_data.columns = ['Sequence', 'name', 'id', 'flag', 'label_original', 'type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_get_stats(source_data, target_data):\n",
    "    \n",
    "    # empty char count per sequence\n",
    "    source_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in source_data['Sequence']]\n",
    "    # incomplete sequence flag\n",
    "    source_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in source_data['Sequence']]\n",
    "    \n",
    "    target_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in target_data['Sequence']]\n",
    "    target_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in target_data['Sequence']]\n",
    "\n",
    "    # 0:1\n",
    "    train_label_nonempty_ratio = source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    train_label_ratio = (source_data.shape[0]-sum(source_data[\"label_original\"] == 1)) / sum(source_data[\"label_original\"] == 1)\n",
    "\n",
    "    # 0:1\n",
    "    indpe_label_nonempty_ratio = target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    indpe_label_ratio = (target_data.shape[0]-sum(target_data[\"label_original\"] == 1)) / sum(target_data[\"label_original\"] == 1)\n",
    "\n",
    "    print('Current train_label_nonempty_ratio:', train_label_nonempty_ratio, 'train_label_ratio:', train_label_ratio)\n",
    "    print('Target indpe_label_nonempty_ratio:', indpe_label_nonempty_ratio, 'indpe_label_ratio:', indpe_label_ratio)\n",
    "\n",
    "    increase_0_data_factor = int(round(indpe_label_ratio/train_label_ratio)) - 1\n",
    "    increase_empty_data_factor = int(round(indpe_label_nonempty_ratio/train_label_nonempty_ratio)) - 1\n",
    "    \n",
    "    return increase_0_data_factor, increase_empty_data_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty char count per sequence\n",
    "train_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in train_data['Sequence']]\n",
    "# incomplete sequence flag\n",
    "train_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in train_data['Sequence']]\n",
    "\n",
    "indpe_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in indpe_data['Sequence']]\n",
    "indpe_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in indpe_data['Sequence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 183\n",
      "Ratio empty/total: 0.07682619647355164\n"
     ]
    }
   ],
   "source": [
    "plt.hist(train_data['Empty_count'][train_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(train_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(train_data['has_empty'])/train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(train_data['Empty_count'][(train_data['Empty_count'] != 0) & (train_data['label_original'] == -1)])\n",
    "axs[1].hist(train_data['Empty_count'][(train_data['Empty_count'] != 0) & (train_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_original</th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_original  has_empty\n",
       "0              -1         39\n",
       "1               1        144"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 68\n",
      "Ratio empty/total: 0.05551020408163265\n"
     ]
    }
   ],
   "source": [
    "plt.hist(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(indpe_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(indpe_data['has_empty'])/indpe_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(indpe_data['Empty_count'][(indpe_data['Empty_count'] != 0) & (indpe_data['label_original'] == -1)])\n",
    "axs[1].hist(indpe_data['Empty_count'][(indpe_data['Empty_count'] != 0) & (indpe_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_original</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                has_empty\n",
       "label_original           \n",
       "-1                     52\n",
       " 1                     16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indpe_data.groupby([\"label_original\"]).sum().filter(['has_empty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current train_label_nonempty_ratio: 0.2708333333333333 train_label_ratio: 1.0\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n"
     ]
    }
   ],
   "source": [
    "_,_ = print_and_get_stats(train_data, indpe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation on Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sequence_truncate(seq, random_length):\n",
    "    rep_str = \"\".join(['-']*random_length)\n",
    "    if np.random.choice((True, False)):\n",
    "        return_seq = rep_str + seq[random_length:]\n",
    "    else:\n",
    "        return_seq = seq[:-random_length] + rep_str\n",
    "    return return_seq\n",
    "\n",
    "def random_sequence_truncate(seq):\n",
    "    random_length = np.random.randint(1, 20)\n",
    "    return_seq = sequence_truncate(seq, random_length)\n",
    "    return return_seq\n",
    "\n",
    "def repeat_truncate_sequence_steps(seq, factor):\n",
    "    random_length = random.sample(range(1, int(len(seq)/2)), \n",
    "                                  factor)\n",
    "    return_seqs = []\n",
    "    for i in range(factor):\n",
    "        ret_seq = sequence_truncate(seq, random_length[i])\n",
    "        return_seqs.append(ret_seq)\n",
    "    return return_seqs\n",
    "\n",
    "def truncate_sequence_by_len(seq, ran_len):\n",
    "    return_seqs = []\n",
    "    ret_seq = sequence_truncate(seq, ran_len)\n",
    "    return ret_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_get_stats(source_data, target_data):\n",
    "    \n",
    "    # empty char count per sequence\n",
    "    source_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in source_data['Sequence']]\n",
    "    # incomplete sequence flag\n",
    "    source_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in source_data['Sequence']]\n",
    "    \n",
    "    target_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in target_data['Sequence']]\n",
    "    target_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in target_data['Sequence']]\n",
    "\n",
    "    # 0:1\n",
    "    train_label_nonempty_ratio = source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / source_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    train_label_ratio = (source_data.shape[0]-sum(source_data[\"label_original\"] == 1)) / sum(source_data[\"label_original\"] == 1)\n",
    "\n",
    "    # 0:1\n",
    "    indpe_label_nonempty_ratio = target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][0] / target_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()['has_empty'][1]\n",
    "    indpe_label_ratio = (target_data.shape[0]-sum(target_data[\"label_original\"] == 1)) / sum(target_data[\"label_original\"] == 1)\n",
    "\n",
    "    print('Current train_label_nonempty_ratio:', train_label_nonempty_ratio, 'train_label_ratio:', train_label_ratio)\n",
    "    print('Target indpe_label_nonempty_ratio:', indpe_label_nonempty_ratio, 'indpe_label_ratio:', indpe_label_ratio)\n",
    "\n",
    "    increase_0_data_factor = int(round(indpe_label_ratio/train_label_ratio)) - 1\n",
    "    increase_empty_data_factor = int(round(indpe_label_nonempty_ratio/train_label_nonempty_ratio)) - 1\n",
    "    \n",
    "    return increase_0_data_factor, increase_empty_data_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=10)\n",
    "\n",
    "bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "cdf = np.cumsum(hist)\n",
    "cdf = cdf / cdf[-1]\n",
    "values = np.random.rand(100)\n",
    "value_bins = np.searchsorted(cdf, values)\n",
    "random_from_cdf = bin_midpoints[value_bins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(121)\n",
    "plt.hist(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], 10)\n",
    "plt.subplot(122)\n",
    "plt.hist(np.round(random_from_cdf).astype(int), 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6963, 8)\n",
      "Current train_label_nonempty_ratio: 0.9213483146067416 train_label_ratio: 1.0306211723534557\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n",
      "(4581, 8)\n",
      "\n",
      "##### After removing duplicates #####\n",
      "\n",
      "Current train_label_nonempty_ratio: 1.0 train_label_ratio: 1.0469168900804289\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n"
     ]
    }
   ],
   "source": [
    "# empty char count per sequence\n",
    "train_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in train_data['Sequence']]\n",
    "# incomplete sequence flag\n",
    "train_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in train_data['Sequence']]\n",
    "\n",
    "# before balancing\n",
    "\n",
    "# train_data = train_data_backup\n",
    "\n",
    "########\n",
    "\n",
    "factor = 1\n",
    "\n",
    "data = train_data\n",
    "neg_data = data[data['label_original'] == -1].reset_index(drop=True)\n",
    "pos_data = data[data['label_original'] == 1].reset_index(drop=True)\n",
    "# neg_data = data.reset_index(drop=True)\n",
    "\n",
    "not_empty_neg_idxs = np.where(neg_data['has_empty'] != True)[0]\n",
    "not_empty_neg_idxs = np.random.permutation(not_empty_neg_idxs)\n",
    "not_empty_pos_idxs = np.where(pos_data['has_empty'] != True)[0]\n",
    "not_empty_pos_idxs = np.random.permutation(not_empty_pos_idxs)[0:int(not_empty_neg_idxs.shape[0]/factor)]\n",
    "\n",
    "##### Getting missing distribution\n",
    "hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=20)\n",
    "bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "cdf = np.cumsum(hist)\n",
    "cdf = cdf / cdf[-1]\n",
    "\n",
    "##### Negative records augmentation\n",
    "neg_values = np.random.rand(not_empty_neg_idxs.shape[0])\n",
    "neg_value_bins = np.searchsorted(cdf, neg_values)\n",
    "neg_random_from_cdf = bin_midpoints[neg_value_bins]\n",
    "neg_random_from_cdf = np.round(neg_random_from_cdf).astype(int)\n",
    "\n",
    "for idx, ran_len in zip(list(not_empty_neg_idxs), list(list(neg_random_from_cdf))):\n",
    "    record = neg_data.iloc[idx].to_dict()\n",
    "    seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "    record['Sequence'] = seq\n",
    "    neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "##### Positive records augmentation\n",
    "pos_values = np.random.rand(not_empty_pos_idxs.shape[0])\n",
    "pos_value_bins = np.searchsorted(cdf, pos_values)\n",
    "pos_random_from_cdf = bin_midpoints[pos_value_bins]\n",
    "pos_random_from_cdf = np.round(pos_random_from_cdf).astype(int)\n",
    "    \n",
    "for idx, ran_len in zip(list(not_empty_pos_idxs), list(list(pos_random_from_cdf))):\n",
    "    record = pos_data.iloc[idx].to_dict()\n",
    "    seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "    record['Sequence'] = seq\n",
    "    pos_data = pos_data.append(record, ignore_index=True)\n",
    "    \n",
    "#     seqs = repeat_truncate_sequence_steps(record['Sequence'], factor)\n",
    "#     for seq in seqs:\n",
    "#         record['Sequence'] = seq\n",
    "#         neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "final_data = pd.concat((neg_data, pos_data, data))\n",
    "print(final_data.shape)\n",
    "\n",
    "##########################\n",
    "\n",
    "_ , _ = print_and_get_stats(final_data, indpe_data)\n",
    "\n",
    "##########################\n",
    "\n",
    "final_data = final_data.drop_duplicates().reset_index(drop=True)\n",
    "print(final_data.shape)\n",
    "\n",
    "print('\\n##### After removing duplicates #####\\n')\n",
    "##########\n",
    "\n",
    "_ , _ = print_and_get_stats(final_data, indpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11361, 8)\n",
      "Current train_label_nonempty_ratio: 1.0306211723534557 train_label_ratio: 1.0570342205323193\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n",
      "(6706, 8)\n",
      "\n",
      "##### After removing duplicates #####\n",
      "\n",
      "Current train_label_nonempty_ratio: 1.049567985447931 train_label_ratio: 1.0659272951324708\n",
      "Target indpe_label_nonempty_ratio: 3.25 indpe_label_ratio: 5.0344827586206895\n"
     ]
    }
   ],
   "source": [
    "train_data = final_data\n",
    "\n",
    "# empty char count per sequence\n",
    "train_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in train_data['Sequence']]\n",
    "# incomplete sequence flag\n",
    "train_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in train_data['Sequence']]\n",
    "\n",
    "# before balancing\n",
    "\n",
    "# train_data = train_data_backup\n",
    "\n",
    "########\n",
    "\n",
    "factor = 1\n",
    "\n",
    "data = train_data\n",
    "neg_data = data[data['label_original'] == -1].reset_index(drop=True)\n",
    "pos_data = data[data['label_original'] == 1].reset_index(drop=True)\n",
    "# neg_data = data.reset_index(drop=True)\n",
    "\n",
    "not_empty_neg_idxs = np.where(neg_data['has_empty'] != True)[0]\n",
    "not_empty_neg_idxs = np.random.permutation(not_empty_neg_idxs)\n",
    "not_empty_pos_idxs = np.where(pos_data['has_empty'] != True)[0]\n",
    "not_empty_pos_idxs = np.random.permutation(not_empty_pos_idxs)[0:int(not_empty_neg_idxs.shape[0]/factor)]\n",
    "\n",
    "##### Getting missing distribution\n",
    "hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=20)\n",
    "bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "cdf = np.cumsum(hist)\n",
    "cdf = cdf / cdf[-1]\n",
    "\n",
    "##### Negative records augmentation\n",
    "neg_values = np.random.rand(not_empty_neg_idxs.shape[0])\n",
    "neg_value_bins = np.searchsorted(cdf, neg_values)\n",
    "neg_random_from_cdf = bin_midpoints[neg_value_bins]\n",
    "neg_random_from_cdf = np.round(neg_random_from_cdf).astype(int)\n",
    "\n",
    "for idx, ran_len in zip(list(not_empty_neg_idxs), list(list(neg_random_from_cdf))):\n",
    "    record = neg_data.iloc[idx].to_dict()\n",
    "    seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "    record['Sequence'] = seq\n",
    "    neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "##### Positive records augmentation\n",
    "pos_values = np.random.rand(not_empty_pos_idxs.shape[0])\n",
    "pos_value_bins = np.searchsorted(cdf, pos_values)\n",
    "pos_random_from_cdf = bin_midpoints[pos_value_bins]\n",
    "pos_random_from_cdf = np.round(pos_random_from_cdf).astype(int)\n",
    "    \n",
    "for idx, ran_len in zip(list(not_empty_pos_idxs), list(list(pos_random_from_cdf))):\n",
    "    record = pos_data.iloc[idx].to_dict()\n",
    "    seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "    record['Sequence'] = seq\n",
    "    pos_data = pos_data.append(record, ignore_index=True)\n",
    "    \n",
    "#     seqs = repeat_truncate_sequence_steps(record['Sequence'], factor)\n",
    "#     for seq in seqs:\n",
    "#         record['Sequence'] = seq\n",
    "#         neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "final_data = pd.concat((neg_data, pos_data, data))\n",
    "print(final_data.shape)\n",
    "\n",
    "##########################\n",
    "\n",
    "_ , _ = print_and_get_stats(final_data, indpe_data)\n",
    "\n",
    "##########################\n",
    "\n",
    "final_data = final_data.drop_duplicates().reset_index(drop=True)\n",
    "print(final_data.shape)\n",
    "\n",
    "print('\\n##### After removing duplicates #####\\n')\n",
    "##########\n",
    "\n",
    "_ , _ = print_and_get_stats(final_data, indpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after balancing\n",
    "\n",
    "# # train_data = train_data_backup\n",
    "\n",
    "# ########\n",
    "\n",
    "# factor = 3\n",
    "\n",
    "# data = train_data\n",
    "# neg_data = data[data['label_original'] == -1].reset_index(drop=True)\n",
    "# pos_data = data[data['label_original'] == 1].reset_index(drop=True)\n",
    "# # neg_data = data.reset_index(drop=True)\n",
    "\n",
    "# not_empty_neg_idxs = np.where(neg_data['has_empty'] != True)[0]\n",
    "# not_empty_pos_idxs = np.where(pos_data['has_empty'] != True)[0]\n",
    "# not_empty_pos_idxs = np.random.permutation(not_empty_pos_idxs)[0:int(not_empty_neg_idxs.shape[0]/factor)]\n",
    "# not_empty_neg_idxs = np.random.permutation(not_empty_neg_idxs)[0:int(not_empty_pos_idxs.shape[0]/factor)]\n",
    "\n",
    "\n",
    "# ##### Getting missing distribution\n",
    "# hist, bins = np.histogram(indpe_data['Empty_count'][indpe_data['Empty_count'] != 0], bins=20)\n",
    "# bin_midpoints = bins[:-1] + np.diff(bins)/2\n",
    "# cdf = np.cumsum(hist)\n",
    "# cdf = cdf / cdf[-1]\n",
    "\n",
    "# ##### Negative records augmentation\n",
    "# neg_values = np.random.rand(not_empty_neg_idxs.shape[0])\n",
    "# neg_value_bins = np.searchsorted(cdf, neg_values)\n",
    "# neg_random_from_cdf = bin_midpoints[neg_value_bins]\n",
    "# neg_random_from_cdf = np.round(neg_random_from_cdf).astype(int)\n",
    "\n",
    "# for idx, ran_len in zip(list(not_empty_neg_idxs), list(list(neg_random_from_cdf))):\n",
    "#     record = neg_data.iloc[idx].to_dict()\n",
    "#     seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "#     record['Sequence'] = seq\n",
    "#     neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "# ##### Positive records augmentation\n",
    "# pos_values = np.random.rand(not_empty_pos_idxs.shape[0])\n",
    "# pos_value_bins = np.searchsorted(cdf, pos_values)\n",
    "# pos_random_from_cdf = bin_midpoints[pos_value_bins]\n",
    "# pos_random_from_cdf = np.round(pos_random_from_cdf).astype(int)\n",
    "    \n",
    "# for idx, ran_len in zip(list(not_empty_pos_idxs), list(list(pos_random_from_cdf))):\n",
    "#     record = pos_data.iloc[idx].to_dict()\n",
    "#     seq = truncate_sequence_by_len(record['Sequence'], ran_len)\n",
    "#     record['Sequence'] = seq\n",
    "#     pos_data = pos_data.append(record, ignore_index=True)\n",
    "    \n",
    "# #     seqs = repeat_truncate_sequence_steps(record['Sequence'], factor)\n",
    "# #     for seq in seqs:\n",
    "# #         record['Sequence'] = seq\n",
    "# #         neg_data = neg_data.append(record, ignore_index=True)\n",
    "\n",
    "# final_data = pd.concat((neg_data, pos_data, data))\n",
    "# print(final_data.shape)\n",
    "\n",
    "# ##########################\n",
    "\n",
    "# _ , _ = print_and_get_stats(final_data, indpe_data)\n",
    "\n",
    "# ##########################\n",
    "\n",
    "# final_data = final_data.drop_duplicates().reset_index(drop=True)\n",
    "# print(final_data.shape)\n",
    "\n",
    "# print('\\n##### After removing duplicates #####\\n')\n",
    "# ##########\n",
    "\n",
    "# _ , _ = print_and_get_stats(final_data, indpe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty char count per sequence\n",
    "final_data['Empty_count'] = [np.sum(np.array(list(val)) == '-') for val in final_data['Sequence']]\n",
    "# incomplete sequence flag\n",
    "final_data['has_empty'] = [True if np.sum(np.array(list(val)) == '-') > 0 else False for val in final_data['Sequence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total empty records: 4507\n",
      "Ratio empty/total: 0.6720847002684164\n"
     ]
    }
   ],
   "source": [
    "plt.hist(final_data['Empty_count'][final_data['Empty_count'] != 0])\n",
    "plt.show()\n",
    "print('Total empty records:', sum(final_data['has_empty']))\n",
    "print('Ratio empty/total:', sum(final_data['has_empty'])/final_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle('Train Data\\nlabel=0                                                                      label=1')\n",
    "axs[0].hist(final_data['Empty_count'][(final_data['Empty_count'] != 0) & (final_data['label_original'] == -1)])\n",
    "axs[1].hist(final_data['Empty_count'][(final_data['Empty_count'] != 0) & (final_data['label_original'] == 1)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_original</th>\n",
       "      <th>has_empty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>2308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label_original  has_empty\n",
       "0              -1       2308\n",
       "1               1       2199"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.groupby([\"label_original\"]).sum().filter(['has_empty']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Final preparations for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = final_data\n",
    "\n",
    "##################################################################################\n",
    "##### Create dictionary of all characters in the NT sequence \n",
    "##################################################################################\n",
    "all_char_set = set({})\n",
    "for val in [set(val) for val in train_data['Sequence']]:\n",
    "    all_char_set = all_char_set.union(val)\n",
    "all_char_list = list(all_char_set)\n",
    "all_char_list.sort()\n",
    "all_char_dict = {}\n",
    "for i in range(len(all_char_list)):\n",
    "    all_char_dict[all_char_list[i]] = i\n",
    "    \n",
    "##################################################################################\n",
    "##### Create OHE of sequence\n",
    "##################################################################################\n",
    "train_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                        for val in train_data[\"Sequence\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Fix the labels\n",
    "##################################################################################\n",
    "train_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                 for val in train_data[\"label_original\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Extract features and labels, create folds\n",
    "##################################################################################\n",
    "\n",
    "train_features = np.array(list(train_data['OHE_Sequence']))\n",
    "train_labels = np.array(list(train_data['label']))\n",
    "train_labels = train_labels.reshape((train_labels.shape[0], 1))\n",
    "\n",
    "input_seq_shape = train_features[0].shape\n",
    "\n",
    "folds = build_kfold(train_features, train_labels, k=n_fold, shuffle=shuffle, seed=seed)\n",
    "\n",
    "## Write the k-fold dataset to file\n",
    "foldPath = os.path.join(outPath, expName, \"{}fold\".format(n_fold))\n",
    "if(not os.path.isdir(foldPath)):\n",
    "    os.makedirs(foldPath)\n",
    "pickle.dump(folds, open(os.path.join(foldPath, foldName), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final prep for Independent dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##### Create OHE of sequence\n",
    "##################################################################################\n",
    "indpe_data['OHE_Sequence'] = pd.Series([one_hot_encode_nt(val, all_char_dict) \n",
    "                                        for val in indpe_data[\"Sequence\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Fix the labels\n",
    "##################################################################################\n",
    "indpe_data['label'] = pd.Series([1 if val == 1 else 0 \n",
    "                                 for val in indpe_data[\"label_original\"]])\n",
    "\n",
    "##################################################################################\n",
    "##### Extract features and labels, create folds\n",
    "##################################################################################\n",
    "\n",
    "indpe_features = np.array(list(indpe_data['OHE_Sequence']))\n",
    "indpe_labels = np.array(list(indpe_data['label']))\n",
    "indpe_labels = indpe_labels.reshape((indpe_labels.shape[0], 1))\n",
    "\n",
    "input_seq_shape = indpe_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 21)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train/Test model on Fold #0.\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3380\n",
      "Epoch 1: val_loss improved from inf to 1.22081, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 6s 15ms/step - loss: 1.3380 - val_loss: 1.2208\n",
      "Epoch 2/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 1.1354\n",
      "Epoch 2: val_loss improved from 1.22081 to 0.99049, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 1.1300 - val_loss: 0.9905\n",
      "Epoch 3/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.9759\n",
      "Epoch 3: val_loss improved from 0.99049 to 0.88487, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.9741 - val_loss: 0.8849\n",
      "Epoch 4/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.8838\n",
      "Epoch 4: val_loss improved from 0.88487 to 0.81511, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.8837 - val_loss: 0.8151\n",
      "Epoch 5/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.8334\n",
      "Epoch 5: val_loss improved from 0.81511 to 0.76126, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8338 - val_loss: 0.7613\n",
      "Epoch 6/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.7792\n",
      "Epoch 6: val_loss improved from 0.76126 to 0.72891, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7745 - val_loss: 0.7289\n",
      "Epoch 7/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.7415\n",
      "Epoch 7: val_loss improved from 0.72891 to 0.69572, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7389 - val_loss: 0.6957\n",
      "Epoch 8/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7069\n",
      "Epoch 8: val_loss improved from 0.69572 to 0.66413, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7069 - val_loss: 0.6641\n",
      "Epoch 9/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.6849\n",
      "Epoch 9: val_loss improved from 0.66413 to 0.64503, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6836 - val_loss: 0.6450\n",
      "Epoch 10/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6710\n",
      "Epoch 10: val_loss improved from 0.64503 to 0.62823, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6681 - val_loss: 0.6282\n",
      "Epoch 11/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6291\n",
      "Epoch 11: val_loss improved from 0.62823 to 0.60444, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6318 - val_loss: 0.6044\n",
      "Epoch 12/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6234\n",
      "Epoch 12: val_loss improved from 0.60444 to 0.59332, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6259 - val_loss: 0.5933\n",
      "Epoch 13/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.6021\n",
      "Epoch 13: val_loss improved from 0.59332 to 0.57517, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6022 - val_loss: 0.5752\n",
      "Epoch 14/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5999\n",
      "Epoch 14: val_loss did not improve from 0.57517\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5969 - val_loss: 0.5846\n",
      "Epoch 15/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5798\n",
      "Epoch 15: val_loss improved from 0.57517 to 0.54945, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5784 - val_loss: 0.5494\n",
      "Epoch 16/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5660\n",
      "Epoch 16: val_loss improved from 0.54945 to 0.53900, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5647 - val_loss: 0.5390\n",
      "Epoch 17/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5490\n",
      "Epoch 17: val_loss improved from 0.53900 to 0.52717, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5461 - val_loss: 0.5272\n",
      "Epoch 18/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5352\n",
      "Epoch 18: val_loss improved from 0.52717 to 0.52219, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5378 - val_loss: 0.5222\n",
      "Epoch 19/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5272\n",
      "Epoch 19: val_loss did not improve from 0.52219\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5253 - val_loss: 0.5252\n",
      "Epoch 20/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5167\n",
      "Epoch 20: val_loss improved from 0.52219 to 0.50062, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5160 - val_loss: 0.5006\n",
      "Epoch 21/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5115\n",
      "Epoch 21: val_loss improved from 0.50062 to 0.49095, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5130 - val_loss: 0.4910\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5039\n",
      "Epoch 22: val_loss improved from 0.49095 to 0.48384, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5039 - val_loss: 0.4838\n",
      "Epoch 23/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4923\n",
      "Epoch 23: val_loss improved from 0.48384 to 0.47008, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4926 - val_loss: 0.4701\n",
      "Epoch 24/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4853\n",
      "Epoch 24: val_loss improved from 0.47008 to 0.46618, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4852 - val_loss: 0.4662\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4885\n",
      "Epoch 25: val_loss did not improve from 0.46618\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4885 - val_loss: 0.4663\n",
      "Epoch 26/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4651\n",
      "Epoch 26: val_loss improved from 0.46618 to 0.45173, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4653 - val_loss: 0.4517\n",
      "Epoch 27/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4542\n",
      "Epoch 27: val_loss improved from 0.45173 to 0.44660, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4551 - val_loss: 0.4466\n",
      "Epoch 28/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4496\n",
      "Epoch 28: val_loss improved from 0.44660 to 0.44229, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4509 - val_loss: 0.4423\n",
      "Epoch 29/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4485\n",
      "Epoch 29: val_loss did not improve from 0.44229\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4503 - val_loss: 0.4468\n",
      "Epoch 30/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4345\n",
      "Epoch 30: val_loss improved from 0.44229 to 0.42826, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4335 - val_loss: 0.4283\n",
      "Epoch 31/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4462\n",
      "Epoch 31: val_loss improved from 0.42826 to 0.42529, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4459 - val_loss: 0.4253\n",
      "Epoch 32/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4360\n",
      "Epoch 32: val_loss improved from 0.42529 to 0.42029, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4349 - val_loss: 0.4203\n",
      "Epoch 33/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4269\n",
      "Epoch 33: val_loss improved from 0.42029 to 0.41501, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4267 - val_loss: 0.4150\n",
      "Epoch 34/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4202\n",
      "Epoch 34: val_loss improved from 0.41501 to 0.40687, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4172 - val_loss: 0.4069\n",
      "Epoch 35/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4023\n",
      "Epoch 35: val_loss improved from 0.40687 to 0.40304, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4038 - val_loss: 0.4030\n",
      "Epoch 36/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4022\n",
      "Epoch 36: val_loss improved from 0.40304 to 0.38945, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4009 - val_loss: 0.3894\n",
      "Epoch 37/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4082\n",
      "Epoch 37: val_loss did not improve from 0.38945\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4067 - val_loss: 0.3972\n",
      "Epoch 38/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4124\n",
      "Epoch 38: val_loss improved from 0.38945 to 0.38816, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4097 - val_loss: 0.3882\n",
      "Epoch 39/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3941\n",
      "Epoch 39: val_loss improved from 0.38816 to 0.38150, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3921 - val_loss: 0.3815\n",
      "Epoch 40/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3893\n",
      "Epoch 40: val_loss improved from 0.38150 to 0.38050, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3876 - val_loss: 0.3805\n",
      "Epoch 41/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3912\n",
      "Epoch 41: val_loss improved from 0.38050 to 0.37619, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3913 - val_loss: 0.3762\n",
      "Epoch 42/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3744\n",
      "Epoch 42: val_loss improved from 0.37619 to 0.36966, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3773 - val_loss: 0.3697\n",
      "Epoch 43/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3799\n",
      "Epoch 43: val_loss did not improve from 0.36966\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3799 - val_loss: 0.3753\n",
      "Epoch 44/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3874\n",
      "Epoch 44: val_loss improved from 0.36966 to 0.36811, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3861 - val_loss: 0.3681\n",
      "Epoch 45/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3718\n",
      "Epoch 45: val_loss improved from 0.36811 to 0.36266, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3690 - val_loss: 0.3627\n",
      "Epoch 46/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3656\n",
      "Epoch 46: val_loss did not improve from 0.36266\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3673 - val_loss: 0.3627\n",
      "Epoch 47/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3732\n",
      "Epoch 47: val_loss did not improve from 0.36266\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3741 - val_loss: 0.3773\n",
      "Epoch 48/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3682\n",
      "Epoch 48: val_loss improved from 0.36266 to 0.36054, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3686 - val_loss: 0.3605\n",
      "Epoch 49/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3569\n",
      "Epoch 49: val_loss improved from 0.36054 to 0.35185, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3582 - val_loss: 0.3518\n",
      "Epoch 50/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3551\n",
      "Epoch 50: val_loss did not improve from 0.35185\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3556 - val_loss: 0.3542\n",
      "Epoch 51/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3556\n",
      "Epoch 51: val_loss did not improve from 0.35185\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3544 - val_loss: 0.3789\n",
      "Epoch 52/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3569\n",
      "Epoch 52: val_loss did not improve from 0.35185\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3596 - val_loss: 0.3734\n",
      "Epoch 53/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3417\n",
      "Epoch 53: val_loss improved from 0.35185 to 0.34073, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3415 - val_loss: 0.3407\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3490\n",
      "Epoch 54: val_loss did not improve from 0.34073\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3496 - val_loss: 0.3469\n",
      "Epoch 55/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3532\n",
      "Epoch 55: val_loss improved from 0.34073 to 0.33872, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3532 - val_loss: 0.3387\n",
      "Epoch 56/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3436\n",
      "Epoch 56: val_loss improved from 0.33872 to 0.33852, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3438 - val_loss: 0.3385\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3250\n",
      "Epoch 57: val_loss did not improve from 0.33852\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3250 - val_loss: 0.3427\n",
      "Epoch 58/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3410\n",
      "Epoch 58: val_loss improved from 0.33852 to 0.33337, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3407 - val_loss: 0.3334\n",
      "Epoch 59/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3365\n",
      "Epoch 59: val_loss improved from 0.33337 to 0.32718, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3360 - val_loss: 0.3272\n",
      "Epoch 60/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3304\n",
      "Epoch 60: val_loss improved from 0.32718 to 0.32511, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3317 - val_loss: 0.3251\n",
      "Epoch 61/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3396\n",
      "Epoch 61: val_loss did not improve from 0.32511\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3396 - val_loss: 0.3354\n",
      "Epoch 62/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3382\n",
      "Epoch 62: val_loss did not improve from 0.32511\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3381 - val_loss: 0.3259\n",
      "Epoch 63/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3239\n",
      "Epoch 63: val_loss improved from 0.32511 to 0.32319, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3257 - val_loss: 0.3232\n",
      "Epoch 64/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3257\n",
      "Epoch 64: val_loss did not improve from 0.32319\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3259 - val_loss: 0.3585\n",
      "Epoch 65/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3407\n",
      "Epoch 65: val_loss improved from 0.32319 to 0.32091, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3414 - val_loss: 0.3209\n",
      "Epoch 66/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3214\n",
      "Epoch 66: val_loss did not improve from 0.32091\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3180 - val_loss: 0.3252\n",
      "Epoch 67/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3110\n",
      "Epoch 67: val_loss did not improve from 0.32091\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3125 - val_loss: 0.3228\n",
      "Epoch 68/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3198\n",
      "Epoch 68: val_loss improved from 0.32091 to 0.31406, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3188 - val_loss: 0.3141\n",
      "Epoch 69/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3151\n",
      "Epoch 69: val_loss did not improve from 0.31406\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3160 - val_loss: 0.3166\n",
      "Epoch 70/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3318\n",
      "Epoch 70: val_loss did not improve from 0.31406\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3298 - val_loss: 0.3187\n",
      "Epoch 71/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3109\n",
      "Epoch 71: val_loss improved from 0.31406 to 0.31027, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3109 - val_loss: 0.3103\n",
      "Epoch 72/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3135\n",
      "Epoch 72: val_loss did not improve from 0.31027\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3135 - val_loss: 0.3144\n",
      "Epoch 73/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3140\n",
      "Epoch 73: val_loss improved from 0.31027 to 0.30946, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3140 - val_loss: 0.3095\n",
      "Epoch 74/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3130\n",
      "Epoch 74: val_loss did not improve from 0.30946\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3118 - val_loss: 0.3147\n",
      "Epoch 75/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3122\n",
      "Epoch 75: val_loss did not improve from 0.30946\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3147 - val_loss: 0.3126\n",
      "Epoch 76/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3148\n",
      "Epoch 76: val_loss improved from 0.30946 to 0.30704, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3149 - val_loss: 0.3070\n",
      "Epoch 77/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3059\n",
      "Epoch 77: val_loss improved from 0.30704 to 0.30385, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3053 - val_loss: 0.3038\n",
      "Epoch 78/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2999\n",
      "Epoch 78: val_loss did not improve from 0.30385\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2996 - val_loss: 0.3058\n",
      "Epoch 79/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3033\n",
      "Epoch 79: val_loss improved from 0.30385 to 0.30362, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3036 - val_loss: 0.3036\n",
      "Epoch 80/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2976\n",
      "Epoch 80: val_loss did not improve from 0.30362\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2981 - val_loss: 0.3091\n",
      "Epoch 81/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3170\n",
      "Epoch 81: val_loss improved from 0.30362 to 0.30070, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3164 - val_loss: 0.3007\n",
      "Epoch 82/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2908\n",
      "Epoch 82: val_loss improved from 0.30070 to 0.30016, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2923 - val_loss: 0.3002\n",
      "Epoch 83/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3075\n",
      "Epoch 83: val_loss did not improve from 0.30016\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3078 - val_loss: 0.3132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3080\n",
      "Epoch 84: val_loss improved from 0.30016 to 0.29775, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3071 - val_loss: 0.2978\n",
      "Epoch 85/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3070\n",
      "Epoch 85: val_loss did not improve from 0.29775\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3056 - val_loss: 0.3049\n",
      "Epoch 86/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3079\n",
      "Epoch 86: val_loss improved from 0.29775 to 0.29579, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3083 - val_loss: 0.2958\n",
      "Epoch 87/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3047\n",
      "Epoch 87: val_loss did not improve from 0.29579\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3065 - val_loss: 0.2969\n",
      "Epoch 88/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2948\n",
      "Epoch 88: val_loss did not improve from 0.29579\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2926 - val_loss: 0.3031\n",
      "Epoch 89/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3012\n",
      "Epoch 89: val_loss did not improve from 0.29579\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3019 - val_loss: 0.3011\n",
      "Epoch 90/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2845\n",
      "Epoch 90: val_loss did not improve from 0.29579\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2842 - val_loss: 0.2983\n",
      "Epoch 91/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2916\n",
      "Epoch 91: val_loss improved from 0.29579 to 0.29167, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2935 - val_loss: 0.2917\n",
      "Epoch 92/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2998\n",
      "Epoch 92: val_loss did not improve from 0.29167\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2987 - val_loss: 0.2964\n",
      "Epoch 93/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2901\n",
      "Epoch 93: val_loss did not improve from 0.29167\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2885 - val_loss: 0.2952\n",
      "Epoch 94/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2914\n",
      "Epoch 94: val_loss did not improve from 0.29167\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2895 - val_loss: 0.3026\n",
      "Epoch 95/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3005\n",
      "Epoch 95: val_loss improved from 0.29167 to 0.29085, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3033 - val_loss: 0.2909\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2839\n",
      "Epoch 96: val_loss improved from 0.29085 to 0.28921, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2839 - val_loss: 0.2892\n",
      "Epoch 97/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2736\n",
      "Epoch 97: val_loss did not improve from 0.28921\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2773 - val_loss: 0.2917\n",
      "Epoch 98/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2875\n",
      "Epoch 98: val_loss improved from 0.28921 to 0.28625, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold0.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2865 - val_loss: 0.2862\n",
      "Epoch 99/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2888\n",
      "Epoch 99: val_loss did not improve from 0.28625\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2874 - val_loss: 0.3025\n",
      "Epoch 100/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2765\n",
      "Epoch 100: val_loss did not improve from 0.28625\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2772 - val_loss: 0.2911\n",
      "\n",
      "Train/Test model on Fold #1.\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3264\n",
      "Epoch 1: val_loss improved from inf to 1.20009, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 3s 15ms/step - loss: 1.3264 - val_loss: 1.2001\n",
      "Epoch 2/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 1.0998\n",
      "Epoch 2: val_loss improved from 1.20009 to 0.99504, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 1.0967 - val_loss: 0.9950\n",
      "Epoch 3/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.9606\n",
      "Epoch 3: val_loss improved from 0.99504 to 0.89716, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.9593 - val_loss: 0.8972\n",
      "Epoch 4/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.8786\n",
      "Epoch 4: val_loss improved from 0.89716 to 0.84277, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8774 - val_loss: 0.8428\n",
      "Epoch 5/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.8142\n",
      "Epoch 5: val_loss improved from 0.84277 to 0.79065, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.8157 - val_loss: 0.7907\n",
      "Epoch 6/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7711\n",
      "Epoch 6: val_loss improved from 0.79065 to 0.75345, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7696 - val_loss: 0.7534\n",
      "Epoch 7/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.7320\n",
      "Epoch 7: val_loss improved from 0.75345 to 0.72558, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7315 - val_loss: 0.7256\n",
      "Epoch 8/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7034\n",
      "Epoch 8: val_loss improved from 0.72558 to 0.69971, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.7039 - val_loss: 0.6997\n",
      "Epoch 9/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.6761\n",
      "Epoch 9: val_loss improved from 0.69971 to 0.67160, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6756 - val_loss: 0.6716\n",
      "Epoch 10/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6576\n",
      "Epoch 10: val_loss improved from 0.67160 to 0.65918, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6607 - val_loss: 0.6592\n",
      "Epoch 11/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.6384\n",
      "Epoch 11: val_loss improved from 0.65918 to 0.64523, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6431 - val_loss: 0.6452\n",
      "Epoch 12/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.6197\n",
      "Epoch 12: val_loss improved from 0.64523 to 0.62500, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6222 - val_loss: 0.6250\n",
      "Epoch 13/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.6053\n",
      "Epoch 13: val_loss improved from 0.62500 to 0.60655, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6057 - val_loss: 0.6066\n",
      "Epoch 14/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5901\n",
      "Epoch 14: val_loss improved from 0.60655 to 0.60239, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5901 - val_loss: 0.6024\n",
      "Epoch 15/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5805\n",
      "Epoch 15: val_loss improved from 0.60239 to 0.58870, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5784 - val_loss: 0.5887\n",
      "Epoch 16/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5706\n",
      "Epoch 16: val_loss improved from 0.58870 to 0.57530, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5711 - val_loss: 0.5753\n",
      "Epoch 17/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5483\n",
      "Epoch 17: val_loss improved from 0.57530 to 0.56556, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5511 - val_loss: 0.5656\n",
      "Epoch 18/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.5414\n",
      "Epoch 18: val_loss improved from 0.56556 to 0.55367, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5449 - val_loss: 0.5537\n",
      "Epoch 19/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5324\n",
      "Epoch 19: val_loss improved from 0.55367 to 0.55038, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5315 - val_loss: 0.5504\n",
      "Epoch 20/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.5259\n",
      "Epoch 20: val_loss improved from 0.55038 to 0.53357, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5277 - val_loss: 0.5336\n",
      "Epoch 21/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5090\n",
      "Epoch 21: val_loss improved from 0.53357 to 0.52315, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5100 - val_loss: 0.5232\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4988\n",
      "Epoch 22: val_loss did not improve from 0.52315\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4988 - val_loss: 0.5316\n",
      "Epoch 23/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4883\n",
      "Epoch 23: val_loss improved from 0.52315 to 0.50771, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4858 - val_loss: 0.5077\n",
      "Epoch 24/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4890\n",
      "Epoch 24: val_loss improved from 0.50771 to 0.50583, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4916 - val_loss: 0.5058\n",
      "Epoch 25/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4797\n",
      "Epoch 25: val_loss improved from 0.50583 to 0.49296, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4787 - val_loss: 0.4930\n",
      "Epoch 26/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4693\n",
      "Epoch 26: val_loss improved from 0.49296 to 0.48824, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4669 - val_loss: 0.4882\n",
      "Epoch 27/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4538\n",
      "Epoch 27: val_loss improved from 0.48824 to 0.47978, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4524 - val_loss: 0.4798\n",
      "Epoch 28/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4462\n",
      "Epoch 28: val_loss improved from 0.47978 to 0.46876, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4500 - val_loss: 0.4688\n",
      "Epoch 29/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4375\n",
      "Epoch 29: val_loss improved from 0.46876 to 0.46004, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4380 - val_loss: 0.4600\n",
      "Epoch 30/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4348\n",
      "Epoch 30: val_loss improved from 0.46004 to 0.45144, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4348 - val_loss: 0.4514\n",
      "Epoch 31/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4328\n",
      "Epoch 31: val_loss improved from 0.45144 to 0.44480, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4330 - val_loss: 0.4448\n",
      "Epoch 32/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4280\n",
      "Epoch 32: val_loss improved from 0.44480 to 0.44396, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4268 - val_loss: 0.4440\n",
      "Epoch 33/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4191\n",
      "Epoch 33: val_loss did not improve from 0.44396\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4210 - val_loss: 0.4491\n",
      "Epoch 34/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4201\n",
      "Epoch 34: val_loss improved from 0.44396 to 0.43553, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4207 - val_loss: 0.4355\n",
      "Epoch 35/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4144\n",
      "Epoch 35: val_loss did not improve from 0.43553\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4149 - val_loss: 0.4374\n",
      "Epoch 36/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4020\n",
      "Epoch 36: val_loss did not improve from 0.43553\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4005 - val_loss: 0.4386\n",
      "Epoch 37/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4056\n",
      "Epoch 37: val_loss improved from 0.43553 to 0.41206, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4044 - val_loss: 0.4121\n",
      "Epoch 38/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3893\n",
      "Epoch 38: val_loss improved from 0.41206 to 0.40622, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3901 - val_loss: 0.4062\n",
      "Epoch 39/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3861\n",
      "Epoch 39: val_loss did not improve from 0.40622\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3863 - val_loss: 0.4216\n",
      "Epoch 40/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3872\n",
      "Epoch 40: val_loss improved from 0.40622 to 0.40183, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3875 - val_loss: 0.4018\n",
      "Epoch 41/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3743\n",
      "Epoch 41: val_loss improved from 0.40183 to 0.39926, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3758 - val_loss: 0.3993\n",
      "Epoch 42/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3767\n",
      "Epoch 42: val_loss improved from 0.39926 to 0.39325, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3763 - val_loss: 0.3933\n",
      "Epoch 43/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3711\n",
      "Epoch 43: val_loss improved from 0.39325 to 0.38630, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3700 - val_loss: 0.3863\n",
      "Epoch 44/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3731\n",
      "Epoch 44: val_loss did not improve from 0.38630\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3732 - val_loss: 0.3876\n",
      "Epoch 45/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3699\n",
      "Epoch 45: val_loss improved from 0.38630 to 0.38049, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3665 - val_loss: 0.3805\n",
      "Epoch 46/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3544\n",
      "Epoch 46: val_loss improved from 0.38049 to 0.37743, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3544 - val_loss: 0.3774\n",
      "Epoch 47/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3599\n",
      "Epoch 47: val_loss improved from 0.37743 to 0.37728, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3599 - val_loss: 0.3773\n",
      "Epoch 48/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3665\n",
      "Epoch 48: val_loss improved from 0.37728 to 0.37407, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3677 - val_loss: 0.3741\n",
      "Epoch 49/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3521\n",
      "Epoch 49: val_loss improved from 0.37407 to 0.36870, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3526 - val_loss: 0.3687\n",
      "Epoch 50/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3562\n",
      "Epoch 50: val_loss improved from 0.36870 to 0.36337, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3559 - val_loss: 0.3634\n",
      "Epoch 51/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3594\n",
      "Epoch 51: val_loss did not improve from 0.36337\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3606 - val_loss: 0.3797\n",
      "Epoch 52/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3578\n",
      "Epoch 52: val_loss did not improve from 0.36337\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3558 - val_loss: 0.3749\n",
      "Epoch 53/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3457\n",
      "Epoch 53: val_loss improved from 0.36337 to 0.36321, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3478 - val_loss: 0.3632\n",
      "Epoch 54/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3340\n",
      "Epoch 54: val_loss improved from 0.36321 to 0.35144, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3351 - val_loss: 0.3514\n",
      "Epoch 55/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3393\n",
      "Epoch 55: val_loss did not improve from 0.35144\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3415 - val_loss: 0.3528\n",
      "Epoch 56/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3260\n",
      "Epoch 56: val_loss improved from 0.35144 to 0.34603, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3280 - val_loss: 0.3460\n",
      "Epoch 57/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3313\n",
      "Epoch 57: val_loss did not improve from 0.34603\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3313 - val_loss: 0.3619\n",
      "Epoch 58/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3424\n",
      "Epoch 58: val_loss improved from 0.34603 to 0.34546, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3412 - val_loss: 0.3455\n",
      "Epoch 59/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3376\n",
      "Epoch 59: val_loss improved from 0.34546 to 0.34526, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3357 - val_loss: 0.3453\n",
      "Epoch 60/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3242\n",
      "Epoch 60: val_loss improved from 0.34526 to 0.34494, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3269 - val_loss: 0.3449\n",
      "Epoch 61/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3305\n",
      "Epoch 61: val_loss did not improve from 0.34494\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3288 - val_loss: 0.3503\n",
      "Epoch 62/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3174\n",
      "Epoch 62: val_loss improved from 0.34494 to 0.34437, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3170 - val_loss: 0.3444\n",
      "Epoch 63/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3293\n",
      "Epoch 63: val_loss improved from 0.34437 to 0.34133, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3297 - val_loss: 0.3413\n",
      "Epoch 64/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3236\n",
      "Epoch 64: val_loss improved from 0.34133 to 0.33662, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3236 - val_loss: 0.3366\n",
      "Epoch 65/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3353\n",
      "Epoch 65: val_loss did not improve from 0.33662\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3357 - val_loss: 0.3410\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3259\n",
      "Epoch 66: val_loss did not improve from 0.33662\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3250 - val_loss: 0.3387\n",
      "Epoch 67/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3213\n",
      "Epoch 67: val_loss did not improve from 0.33662\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3244 - val_loss: 0.3395\n",
      "Epoch 68/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3131\n",
      "Epoch 68: val_loss did not improve from 0.33662\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3133 - val_loss: 0.3377\n",
      "Epoch 69/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3046\n",
      "Epoch 69: val_loss improved from 0.33662 to 0.32728, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3067 - val_loss: 0.3273\n",
      "Epoch 70/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3109\n",
      "Epoch 70: val_loss did not improve from 0.32728\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3121 - val_loss: 0.3334\n",
      "Epoch 71/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3149\n",
      "Epoch 71: val_loss did not improve from 0.32728\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3108 - val_loss: 0.3330\n",
      "Epoch 72/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3130\n",
      "Epoch 72: val_loss improved from 0.32728 to 0.32630, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3129 - val_loss: 0.3263\n",
      "Epoch 73/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3125\n",
      "Epoch 73: val_loss improved from 0.32630 to 0.32191, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3135 - val_loss: 0.3219\n",
      "Epoch 74/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3007\n",
      "Epoch 74: val_loss did not improve from 0.32191\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3009 - val_loss: 0.3219\n",
      "Epoch 75/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3130\n",
      "Epoch 75: val_loss improved from 0.32191 to 0.32174, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3148 - val_loss: 0.3217\n",
      "Epoch 76/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3016\n",
      "Epoch 76: val_loss did not improve from 0.32174\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3032 - val_loss: 0.3230\n",
      "Epoch 77/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3053\n",
      "Epoch 77: val_loss improved from 0.32174 to 0.31769, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3043 - val_loss: 0.3177\n",
      "Epoch 78/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2958\n",
      "Epoch 78: val_loss improved from 0.31769 to 0.31315, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2955 - val_loss: 0.3132\n",
      "Epoch 79/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2949\n",
      "Epoch 79: val_loss did not improve from 0.31315\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.2945 - val_loss: 0.3356\n",
      "Epoch 80/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3018\n",
      "Epoch 80: val_loss improved from 0.31315 to 0.31191, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3018 - val_loss: 0.3119\n",
      "Epoch 81/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2994\n",
      "Epoch 81: val_loss did not improve from 0.31191\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3005 - val_loss: 0.3150\n",
      "Epoch 82/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2980\n",
      "Epoch 82: val_loss did not improve from 0.31191\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2980 - val_loss: 0.3150\n",
      "Epoch 83/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2921\n",
      "Epoch 83: val_loss improved from 0.31191 to 0.31071, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2919 - val_loss: 0.3107\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2941\n",
      "Epoch 84: val_loss improved from 0.31071 to 0.30976, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2941 - val_loss: 0.3098\n",
      "Epoch 85/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2812\n",
      "Epoch 85: val_loss did not improve from 0.30976\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2831 - val_loss: 0.3101\n",
      "Epoch 86/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2903\n",
      "Epoch 86: val_loss improved from 0.30976 to 0.30863, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2911 - val_loss: 0.3086\n",
      "Epoch 87/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2849\n",
      "Epoch 87: val_loss improved from 0.30863 to 0.30485, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2861 - val_loss: 0.3048\n",
      "Epoch 88/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2814\n",
      "Epoch 88: val_loss did not improve from 0.30485\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2825 - val_loss: 0.3070\n",
      "Epoch 89/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2841\n",
      "Epoch 89: val_loss did not improve from 0.30485\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2846 - val_loss: 0.3081\n",
      "Epoch 90/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2839\n",
      "Epoch 90: val_loss improved from 0.30485 to 0.30235, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2859 - val_loss: 0.3024\n",
      "Epoch 91/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2819\n",
      "Epoch 91: val_loss improved from 0.30235 to 0.30194, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2816 - val_loss: 0.3019\n",
      "Epoch 92/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2944\n",
      "Epoch 92: val_loss improved from 0.30194 to 0.29648, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2937 - val_loss: 0.2965\n",
      "Epoch 93/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2815\n",
      "Epoch 93: val_loss did not improve from 0.29648\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2802 - val_loss: 0.3005\n",
      "Epoch 94/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2794\n",
      "Epoch 94: val_loss did not improve from 0.29648\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2808 - val_loss: 0.2987\n",
      "Epoch 95/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2816\n",
      "Epoch 95: val_loss improved from 0.29648 to 0.29331, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2826 - val_loss: 0.2933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2960\n",
      "Epoch 96: val_loss did not improve from 0.29331\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2932 - val_loss: 0.2961\n",
      "Epoch 97/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2851\n",
      "Epoch 97: val_loss did not improve from 0.29331\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2883 - val_loss: 0.2952\n",
      "Epoch 98/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2841\n",
      "Epoch 98: val_loss improved from 0.29331 to 0.29177, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2827 - val_loss: 0.2918\n",
      "Epoch 99/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2767\n",
      "Epoch 99: val_loss improved from 0.29177 to 0.29087, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold1.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2752 - val_loss: 0.2909\n",
      "Epoch 100/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2862\n",
      "Epoch 100: val_loss did not improve from 0.29087\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2863 - val_loss: 0.2931\n",
      "\n",
      "Train/Test model on Fold #2.\n",
      "Epoch 1/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 1.3407\n",
      "Epoch 1: val_loss improved from inf to 1.21646, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 3s 15ms/step - loss: 1.3353 - val_loss: 1.2165\n",
      "Epoch 2/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 1.1145\n",
      "Epoch 2: val_loss improved from 1.21646 to 1.00630, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 1.1118 - val_loss: 1.0063\n",
      "Epoch 3/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.9692\n",
      "Epoch 3: val_loss improved from 1.00630 to 0.91580, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.9674 - val_loss: 0.9158\n",
      "Epoch 4/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.8858\n",
      "Epoch 4: val_loss improved from 0.91580 to 0.85312, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.8861 - val_loss: 0.8531\n",
      "Epoch 5/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.8248\n",
      "Epoch 5: val_loss improved from 0.85312 to 0.80241, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.8248 - val_loss: 0.8024\n",
      "Epoch 6/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7798\n",
      "Epoch 6: val_loss improved from 0.80241 to 0.76569, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7782 - val_loss: 0.7657\n",
      "Epoch 7/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.7423\n",
      "Epoch 7: val_loss improved from 0.76569 to 0.73388, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7437 - val_loss: 0.7339\n",
      "Epoch 8/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.7098\n",
      "Epoch 8: val_loss improved from 0.73388 to 0.70941, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7103 - val_loss: 0.7094\n",
      "Epoch 9/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.6849\n",
      "Epoch 9: val_loss improved from 0.70941 to 0.68544, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6847 - val_loss: 0.6854\n",
      "Epoch 10/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6602\n",
      "Epoch 10: val_loss did not improve from 0.68544\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6602 - val_loss: 0.6860\n",
      "Epoch 11/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6471\n",
      "Epoch 11: val_loss improved from 0.68544 to 0.65497, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6472 - val_loss: 0.6550\n",
      "Epoch 12/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.6316\n",
      "Epoch 12: val_loss improved from 0.65497 to 0.64560, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6300 - val_loss: 0.6456\n",
      "Epoch 13/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.6084\n",
      "Epoch 13: val_loss improved from 0.64560 to 0.62904, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6081 - val_loss: 0.6290\n",
      "Epoch 14/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.5933\n",
      "Epoch 14: val_loss improved from 0.62904 to 0.61697, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5925 - val_loss: 0.6170\n",
      "Epoch 15/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.5818\n",
      "Epoch 15: val_loss improved from 0.61697 to 0.60885, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5828 - val_loss: 0.6088\n",
      "Epoch 16/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.5618\n",
      "Epoch 16: val_loss improved from 0.60885 to 0.59504, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5638 - val_loss: 0.5950\n",
      "Epoch 17/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.5649\n",
      "Epoch 17: val_loss improved from 0.59504 to 0.58382, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5632 - val_loss: 0.5838\n",
      "Epoch 18/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.5421\n",
      "Epoch 18: val_loss improved from 0.58382 to 0.57233, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5412 - val_loss: 0.5723\n",
      "Epoch 19/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5337\n",
      "Epoch 19: val_loss improved from 0.57233 to 0.56638, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5337 - val_loss: 0.5664\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5318\n",
      "Epoch 20: val_loss improved from 0.56638 to 0.55630, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5318 - val_loss: 0.5563\n",
      "Epoch 21/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.5200\n",
      "Epoch 21: val_loss improved from 0.55630 to 0.54497, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5172 - val_loss: 0.5450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.5077\n",
      "Epoch 22: val_loss improved from 0.54497 to 0.53758, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5099 - val_loss: 0.5376\n",
      "Epoch 23/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4937\n",
      "Epoch 23: val_loss did not improve from 0.53758\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.4927 - val_loss: 0.5379\n",
      "Epoch 24/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.4925\n",
      "Epoch 24: val_loss improved from 0.53758 to 0.51990, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4928 - val_loss: 0.5199\n",
      "Epoch 25/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4785\n",
      "Epoch 25: val_loss improved from 0.51990 to 0.51925, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4785 - val_loss: 0.5193\n",
      "Epoch 26/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.4698\n",
      "Epoch 26: val_loss improved from 0.51925 to 0.50818, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4702 - val_loss: 0.5082\n",
      "Epoch 27/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4635\n",
      "Epoch 27: val_loss improved from 0.50818 to 0.50557, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4658 - val_loss: 0.5056\n",
      "Epoch 28/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.4637\n",
      "Epoch 28: val_loss improved from 0.50557 to 0.49898, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4636 - val_loss: 0.4990\n",
      "Epoch 29/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4572\n",
      "Epoch 29: val_loss improved from 0.49898 to 0.49376, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4572 - val_loss: 0.4938\n",
      "Epoch 30/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.4552\n",
      "Epoch 30: val_loss improved from 0.49376 to 0.48402, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4554 - val_loss: 0.4840\n",
      "Epoch 31/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4389\n",
      "Epoch 31: val_loss improved from 0.48402 to 0.47966, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4397 - val_loss: 0.4797\n",
      "Epoch 32/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4350\n",
      "Epoch 32: val_loss improved from 0.47966 to 0.47009, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4349 - val_loss: 0.4701\n",
      "Epoch 33/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4210\n",
      "Epoch 33: val_loss did not improve from 0.47009\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4217 - val_loss: 0.4724\n",
      "Epoch 34/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4237\n",
      "Epoch 34: val_loss improved from 0.47009 to 0.46524, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4239 - val_loss: 0.4652\n",
      "Epoch 35/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4265\n",
      "Epoch 35: val_loss improved from 0.46524 to 0.46311, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4265 - val_loss: 0.4631\n",
      "Epoch 36/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4180\n",
      "Epoch 36: val_loss improved from 0.46311 to 0.46045, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4191 - val_loss: 0.4605\n",
      "Epoch 37/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4133\n",
      "Epoch 37: val_loss improved from 0.46045 to 0.45572, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4117 - val_loss: 0.4557\n",
      "Epoch 38/100\n",
      "77/84 [==========================>...] - ETA: 0s - loss: 0.4023\n",
      "Epoch 38: val_loss improved from 0.45572 to 0.44923, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4083 - val_loss: 0.4492\n",
      "Epoch 39/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4120\n",
      "Epoch 39: val_loss did not improve from 0.44923\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4113 - val_loss: 0.4523\n",
      "Epoch 40/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3971\n",
      "Epoch 40: val_loss improved from 0.44923 to 0.44086, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3940 - val_loss: 0.4409\n",
      "Epoch 41/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3924\n",
      "Epoch 41: val_loss improved from 0.44086 to 0.43708, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3905 - val_loss: 0.4371\n",
      "Epoch 42/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3911\n",
      "Epoch 42: val_loss improved from 0.43708 to 0.43075, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3899 - val_loss: 0.4307\n",
      "Epoch 43/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3862\n",
      "Epoch 43: val_loss improved from 0.43075 to 0.42865, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3878 - val_loss: 0.4287\n",
      "Epoch 44/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3856\n",
      "Epoch 44: val_loss did not improve from 0.42865\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3841 - val_loss: 0.4294\n",
      "Epoch 45/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3709\n",
      "Epoch 45: val_loss improved from 0.42865 to 0.42793, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3735 - val_loss: 0.4279\n",
      "Epoch 46/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3844\n",
      "Epoch 46: val_loss did not improve from 0.42793\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3836 - val_loss: 0.4286\n",
      "Epoch 47/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3706\n",
      "Epoch 47: val_loss improved from 0.42793 to 0.41935, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3727 - val_loss: 0.4193\n",
      "Epoch 48/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3714\n",
      "Epoch 48: val_loss did not improve from 0.41935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3700 - val_loss: 0.4249\n",
      "Epoch 49/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3680\n",
      "Epoch 49: val_loss improved from 0.41935 to 0.41625, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3700 - val_loss: 0.4163\n",
      "Epoch 50/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3575\n",
      "Epoch 50: val_loss improved from 0.41625 to 0.40555, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3588 - val_loss: 0.4056\n",
      "Epoch 51/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3492\n",
      "Epoch 51: val_loss did not improve from 0.40555\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3522 - val_loss: 0.4333\n",
      "Epoch 52/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3701\n",
      "Epoch 52: val_loss did not improve from 0.40555\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3701 - val_loss: 0.4056\n",
      "Epoch 53/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3534\n",
      "Epoch 53: val_loss improved from 0.40555 to 0.40384, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3537 - val_loss: 0.4038\n",
      "Epoch 54/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3534\n",
      "Epoch 54: val_loss improved from 0.40384 to 0.39600, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3521 - val_loss: 0.3960\n",
      "Epoch 55/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3421\n",
      "Epoch 55: val_loss improved from 0.39600 to 0.39187, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3432 - val_loss: 0.3919\n",
      "Epoch 56/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3485\n",
      "Epoch 56: val_loss did not improve from 0.39187\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3475 - val_loss: 0.3989\n",
      "Epoch 57/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3446\n",
      "Epoch 57: val_loss improved from 0.39187 to 0.39072, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3438 - val_loss: 0.3907\n",
      "Epoch 58/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3430\n",
      "Epoch 58: val_loss did not improve from 0.39072\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3415 - val_loss: 0.3928\n",
      "Epoch 59/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3416\n",
      "Epoch 59: val_loss improved from 0.39072 to 0.38470, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3417 - val_loss: 0.3847\n",
      "Epoch 60/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3415\n",
      "Epoch 60: val_loss improved from 0.38470 to 0.38165, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3412 - val_loss: 0.3816\n",
      "Epoch 61/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3398\n",
      "Epoch 61: val_loss improved from 0.38165 to 0.37893, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3407 - val_loss: 0.3789\n",
      "Epoch 62/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3310\n",
      "Epoch 62: val_loss did not improve from 0.37893\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3310 - val_loss: 0.3861\n",
      "Epoch 63/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3288\n",
      "Epoch 63: val_loss improved from 0.37893 to 0.37349, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3289 - val_loss: 0.3735\n",
      "Epoch 64/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3203\n",
      "Epoch 64: val_loss did not improve from 0.37349\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3204 - val_loss: 0.3789\n",
      "Epoch 65/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3294\n",
      "Epoch 65: val_loss improved from 0.37349 to 0.37135, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3283 - val_loss: 0.3713\n",
      "Epoch 66/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3305\n",
      "Epoch 66: val_loss improved from 0.37135 to 0.36935, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3312 - val_loss: 0.3694\n",
      "Epoch 67/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3261\n",
      "Epoch 67: val_loss improved from 0.36935 to 0.36490, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3246 - val_loss: 0.3649\n",
      "Epoch 68/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3120\n",
      "Epoch 68: val_loss did not improve from 0.36490\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3131 - val_loss: 0.3840\n",
      "Epoch 69/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3079\n",
      "Epoch 69: val_loss improved from 0.36490 to 0.36210, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3088 - val_loss: 0.3621\n",
      "Epoch 70/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3250\n",
      "Epoch 70: val_loss did not improve from 0.36210\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3254 - val_loss: 0.3636\n",
      "Epoch 71/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3207\n",
      "Epoch 71: val_loss did not improve from 0.36210\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3211 - val_loss: 0.3806\n",
      "Epoch 72/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3345\n",
      "Epoch 72: val_loss did not improve from 0.36210\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3307 - val_loss: 0.3746\n",
      "Epoch 73/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3165\n",
      "Epoch 73: val_loss improved from 0.36210 to 0.35983, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3151 - val_loss: 0.3598\n",
      "Epoch 74/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3157\n",
      "Epoch 74: val_loss did not improve from 0.35983\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3140 - val_loss: 0.3688\n",
      "Epoch 75/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3124\n",
      "Epoch 75: val_loss improved from 0.35983 to 0.35518, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3096 - val_loss: 0.3552\n",
      "Epoch 76/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3141\n",
      "Epoch 76: val_loss improved from 0.35518 to 0.35241, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3119 - val_loss: 0.3524\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/84 [============================>.] - ETA: 0s - loss: 0.3120\n",
      "Epoch 77: val_loss improved from 0.35241 to 0.35114, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3102 - val_loss: 0.3511\n",
      "Epoch 78/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3204\n",
      "Epoch 78: val_loss did not improve from 0.35114\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3202 - val_loss: 0.3535\n",
      "Epoch 79/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3145\n",
      "Epoch 79: val_loss improved from 0.35114 to 0.34668, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3142 - val_loss: 0.3467\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3073\n",
      "Epoch 80: val_loss did not improve from 0.34668\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3073 - val_loss: 0.3518\n",
      "Epoch 81/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3108\n",
      "Epoch 81: val_loss did not improve from 0.34668\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3108 - val_loss: 0.3488\n",
      "Epoch 82/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3017\n",
      "Epoch 82: val_loss improved from 0.34668 to 0.34223, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3006 - val_loss: 0.3422\n",
      "Epoch 83/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3010\n",
      "Epoch 83: val_loss improved from 0.34223 to 0.33988, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2989 - val_loss: 0.3399\n",
      "Epoch 84/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2953\n",
      "Epoch 84: val_loss improved from 0.33988 to 0.33754, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2953 - val_loss: 0.3375\n",
      "Epoch 85/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3093\n",
      "Epoch 85: val_loss did not improve from 0.33754\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3092 - val_loss: 0.3775\n",
      "Epoch 86/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3024\n",
      "Epoch 86: val_loss did not improve from 0.33754\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3024 - val_loss: 0.3380\n",
      "Epoch 87/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2956\n",
      "Epoch 87: val_loss did not improve from 0.33754\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2952 - val_loss: 0.3400\n",
      "Epoch 88/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2900\n",
      "Epoch 88: val_loss improved from 0.33754 to 0.33632, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2911 - val_loss: 0.3363\n",
      "Epoch 89/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3002\n",
      "Epoch 89: val_loss did not improve from 0.33632\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3011 - val_loss: 0.3394\n",
      "Epoch 90/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2843\n",
      "Epoch 90: val_loss improved from 0.33632 to 0.33383, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2842 - val_loss: 0.3338\n",
      "Epoch 91/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2936\n",
      "Epoch 91: val_loss did not improve from 0.33383\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2945 - val_loss: 0.3353\n",
      "Epoch 92/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2936\n",
      "Epoch 92: val_loss did not improve from 0.33383\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2939 - val_loss: 0.3389\n",
      "Epoch 93/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2884\n",
      "Epoch 93: val_loss improved from 0.33383 to 0.33232, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2929 - val_loss: 0.3323\n",
      "Epoch 94/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2825\n",
      "Epoch 94: val_loss did not improve from 0.33232\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2820 - val_loss: 0.3327\n",
      "Epoch 95/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2941\n",
      "Epoch 95: val_loss improved from 0.33232 to 0.32975, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2941 - val_loss: 0.3298\n",
      "Epoch 96/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2909\n",
      "Epoch 96: val_loss improved from 0.32975 to 0.32714, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2909 - val_loss: 0.3271\n",
      "Epoch 97/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2974\n",
      "Epoch 97: val_loss did not improve from 0.32714\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2960 - val_loss: 0.3315\n",
      "Epoch 98/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2880\n",
      "Epoch 98: val_loss improved from 0.32714 to 0.32227, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold2.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2892 - val_loss: 0.3223\n",
      "Epoch 99/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2922\n",
      "Epoch 99: val_loss did not improve from 0.32227\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2923 - val_loss: 0.3252\n",
      "Epoch 100/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.2847\n",
      "Epoch 100: val_loss did not improve from 0.32227\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2869 - val_loss: 0.3240\n",
      "\n",
      "Train/Test model on Fold #3.\n",
      "Epoch 1/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 1.3461\n",
      "Epoch 1: val_loss improved from inf to 1.22317, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 3s 15ms/step - loss: 1.3417 - val_loss: 1.2232\n",
      "Epoch 2/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.1229\n",
      "Epoch 2: val_loss improved from 1.22317 to 1.01785, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 1.1229 - val_loss: 1.0179\n",
      "Epoch 3/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.9747\n",
      "Epoch 3: val_loss improved from 1.01785 to 0.91671, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.9745 - val_loss: 0.9167\n",
      "Epoch 4/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.8926\n",
      "Epoch 4: val_loss improved from 0.91671 to 0.84909, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.8906 - val_loss: 0.8491\n",
      "Epoch 5/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.8299\n",
      "Epoch 5: val_loss improved from 0.84909 to 0.80041, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.8292 - val_loss: 0.8004\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - ETA: 0s - loss: 0.7805\n",
      "Epoch 6: val_loss improved from 0.80041 to 0.76008, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7805 - val_loss: 0.7601\n",
      "Epoch 7/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.7474\n",
      "Epoch 7: val_loss improved from 0.76008 to 0.73911, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7473 - val_loss: 0.7391\n",
      "Epoch 8/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7220\n",
      "Epoch 8: val_loss improved from 0.73911 to 0.70152, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7216 - val_loss: 0.7015\n",
      "Epoch 9/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.6905\n",
      "Epoch 9: val_loss improved from 0.70152 to 0.67783, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6888 - val_loss: 0.6778\n",
      "Epoch 10/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.6661\n",
      "Epoch 10: val_loss improved from 0.67783 to 0.65786, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6660 - val_loss: 0.6579\n",
      "Epoch 11/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.6461\n",
      "Epoch 11: val_loss improved from 0.65786 to 0.64592, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6443 - val_loss: 0.6459\n",
      "Epoch 12/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.6293\n",
      "Epoch 12: val_loss improved from 0.64592 to 0.62842, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6276 - val_loss: 0.6284\n",
      "Epoch 13/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.6149\n",
      "Epoch 13: val_loss improved from 0.62842 to 0.61463, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6152 - val_loss: 0.6146\n",
      "Epoch 14/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.6026\n",
      "Epoch 14: val_loss improved from 0.61463 to 0.60503, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6026 - val_loss: 0.6050\n",
      "Epoch 15/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.5865\n",
      "Epoch 15: val_loss improved from 0.60503 to 0.58464, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5870 - val_loss: 0.5846\n",
      "Epoch 16/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.5632\n",
      "Epoch 16: val_loss improved from 0.58464 to 0.56370, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5633 - val_loss: 0.5637\n",
      "Epoch 17/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.5598\n",
      "Epoch 17: val_loss improved from 0.56370 to 0.55367, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5586 - val_loss: 0.5537\n",
      "Epoch 18/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5430\n",
      "Epoch 18: val_loss improved from 0.55367 to 0.55293, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5445 - val_loss: 0.5529\n",
      "Epoch 19/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.5344\n",
      "Epoch 19: val_loss improved from 0.55293 to 0.53409, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5338 - val_loss: 0.5341\n",
      "Epoch 20/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5228\n",
      "Epoch 20: val_loss improved from 0.53409 to 0.52120, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5228 - val_loss: 0.5212\n",
      "Epoch 21/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.5176\n",
      "Epoch 21: val_loss improved from 0.52120 to 0.51336, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5156 - val_loss: 0.5134\n",
      "Epoch 22/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.5034\n",
      "Epoch 22: val_loss improved from 0.51336 to 0.50507, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5034 - val_loss: 0.5051\n",
      "Epoch 23/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.4862\n",
      "Epoch 23: val_loss improved from 0.50507 to 0.49479, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4882 - val_loss: 0.4948\n",
      "Epoch 24/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4848\n",
      "Epoch 24: val_loss improved from 0.49479 to 0.48433, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4844 - val_loss: 0.4843\n",
      "Epoch 25/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4809\n",
      "Epoch 25: val_loss improved from 0.48433 to 0.48143, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4810 - val_loss: 0.4814\n",
      "Epoch 26/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.4735\n",
      "Epoch 26: val_loss improved from 0.48143 to 0.46914, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4734 - val_loss: 0.4691\n",
      "Epoch 27/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.4624\n",
      "Epoch 27: val_loss did not improve from 0.46914\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4634 - val_loss: 0.4703\n",
      "Epoch 28/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4564\n",
      "Epoch 28: val_loss did not improve from 0.46914\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4544 - val_loss: 0.4812\n",
      "Epoch 29/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4494\n",
      "Epoch 29: val_loss improved from 0.46914 to 0.45405, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4505 - val_loss: 0.4541\n",
      "Epoch 30/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4455\n",
      "Epoch 30: val_loss improved from 0.45405 to 0.45142, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4432 - val_loss: 0.4514\n",
      "Epoch 31/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4180\n",
      "Epoch 31: val_loss improved from 0.45142 to 0.43505, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4223 - val_loss: 0.4350\n",
      "Epoch 32/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4288\n",
      "Epoch 32: val_loss improved from 0.43505 to 0.43417, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4288 - val_loss: 0.4342\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4253\n",
      "Epoch 33: val_loss improved from 0.43417 to 0.42379, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4253 - val_loss: 0.4238\n",
      "Epoch 34/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4097\n",
      "Epoch 34: val_loss improved from 0.42379 to 0.41839, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4155 - val_loss: 0.4184\n",
      "Epoch 35/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4043\n",
      "Epoch 35: val_loss improved from 0.41839 to 0.41164, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4040 - val_loss: 0.4116\n",
      "Epoch 36/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4003\n",
      "Epoch 36: val_loss improved from 0.41164 to 0.41148, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3975 - val_loss: 0.4115\n",
      "Epoch 37/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4139\n",
      "Epoch 37: val_loss improved from 0.41148 to 0.40817, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4112 - val_loss: 0.4082\n",
      "Epoch 38/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3815\n",
      "Epoch 38: val_loss improved from 0.40817 to 0.39631, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3805 - val_loss: 0.3963\n",
      "Epoch 39/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3892\n",
      "Epoch 39: val_loss improved from 0.39631 to 0.39601, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3892 - val_loss: 0.3960\n",
      "Epoch 40/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3929\n",
      "Epoch 40: val_loss improved from 0.39601 to 0.39369, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3948 - val_loss: 0.3937\n",
      "Epoch 41/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3822\n",
      "Epoch 41: val_loss did not improve from 0.39369\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3850 - val_loss: 0.3950\n",
      "Epoch 42/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3859\n",
      "Epoch 42: val_loss did not improve from 0.39369\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3853 - val_loss: 0.3952\n",
      "Epoch 43/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3763\n",
      "Epoch 43: val_loss did not improve from 0.39369\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3762 - val_loss: 0.3953\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3748\n",
      "Epoch 44: val_loss improved from 0.39369 to 0.38469, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3748 - val_loss: 0.3847\n",
      "Epoch 45/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3695\n",
      "Epoch 45: val_loss improved from 0.38469 to 0.37807, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3693 - val_loss: 0.3781\n",
      "Epoch 46/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3539\n",
      "Epoch 46: val_loss improved from 0.37807 to 0.36818, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3526 - val_loss: 0.3682\n",
      "Epoch 47/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3630\n",
      "Epoch 47: val_loss improved from 0.36818 to 0.36805, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3624 - val_loss: 0.3680\n",
      "Epoch 48/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3575\n",
      "Epoch 48: val_loss improved from 0.36805 to 0.36789, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3588 - val_loss: 0.3679\n",
      "Epoch 49/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3574\n",
      "Epoch 49: val_loss did not improve from 0.36789\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3581 - val_loss: 0.3690\n",
      "Epoch 50/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3490\n",
      "Epoch 50: val_loss improved from 0.36789 to 0.36179, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3484 - val_loss: 0.3618\n",
      "Epoch 51/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3548\n",
      "Epoch 51: val_loss did not improve from 0.36179\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3549 - val_loss: 0.3620\n",
      "Epoch 52/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3491\n",
      "Epoch 52: val_loss improved from 0.36179 to 0.35404, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3481 - val_loss: 0.3540\n",
      "Epoch 53/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3494\n",
      "Epoch 53: val_loss did not improve from 0.35404\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3480 - val_loss: 0.3569\n",
      "Epoch 54/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3379\n",
      "Epoch 54: val_loss improved from 0.35404 to 0.35388, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3380 - val_loss: 0.3539\n",
      "Epoch 55/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3355\n",
      "Epoch 55: val_loss did not improve from 0.35388\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3379 - val_loss: 0.3770\n",
      "Epoch 56/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3465\n",
      "Epoch 56: val_loss improved from 0.35388 to 0.34245, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3463 - val_loss: 0.3425\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3253\n",
      "Epoch 57: val_loss did not improve from 0.34245\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3253 - val_loss: 0.3530\n",
      "Epoch 58/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3312\n",
      "Epoch 58: val_loss did not improve from 0.34245\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3314 - val_loss: 0.3447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3388\n",
      "Epoch 59: val_loss did not improve from 0.34245\n",
      "84/84 [==============================] - 1s 8ms/step - loss: 0.3394 - val_loss: 0.3436\n",
      "Epoch 60/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3366\n",
      "Epoch 60: val_loss did not improve from 0.34245\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3367 - val_loss: 0.3476\n",
      "Epoch 61/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3250\n",
      "Epoch 61: val_loss did not improve from 0.34245\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3276 - val_loss: 0.3434\n",
      "Epoch 62/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3322\n",
      "Epoch 62: val_loss improved from 0.34245 to 0.33679, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3312 - val_loss: 0.3368\n",
      "Epoch 63/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3244\n",
      "Epoch 63: val_loss did not improve from 0.33679\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3248 - val_loss: 0.3473\n",
      "Epoch 64/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3273\n",
      "Epoch 64: val_loss improved from 0.33679 to 0.33463, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3279 - val_loss: 0.3346\n",
      "Epoch 65/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3314\n",
      "Epoch 65: val_loss improved from 0.33463 to 0.33402, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3295 - val_loss: 0.3340\n",
      "Epoch 66/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3254\n",
      "Epoch 66: val_loss improved from 0.33402 to 0.33094, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3254 - val_loss: 0.3309\n",
      "Epoch 67/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3274\n",
      "Epoch 67: val_loss did not improve from 0.33094\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3286 - val_loss: 0.3447\n",
      "Epoch 68/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3184\n",
      "Epoch 68: val_loss improved from 0.33094 to 0.33053, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3181 - val_loss: 0.3305\n",
      "Epoch 69/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3189\n",
      "Epoch 69: val_loss did not improve from 0.33053\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3196 - val_loss: 0.3408\n",
      "Epoch 70/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3244\n",
      "Epoch 70: val_loss did not improve from 0.33053\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3236 - val_loss: 0.3353\n",
      "Epoch 71/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3214\n",
      "Epoch 71: val_loss did not improve from 0.33053\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3210 - val_loss: 0.3558\n",
      "Epoch 72/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3174\n",
      "Epoch 72: val_loss improved from 0.33053 to 0.32815, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3176 - val_loss: 0.3281\n",
      "Epoch 73/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3132\n",
      "Epoch 73: val_loss did not improve from 0.32815\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3120 - val_loss: 0.3488\n",
      "Epoch 74/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3152\n",
      "Epoch 74: val_loss improved from 0.32815 to 0.32699, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3152 - val_loss: 0.3270\n",
      "Epoch 75/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3232\n",
      "Epoch 75: val_loss did not improve from 0.32699\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3220 - val_loss: 0.3383\n",
      "Epoch 76/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3089\n",
      "Epoch 76: val_loss improved from 0.32699 to 0.32005, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3074 - val_loss: 0.3200\n",
      "Epoch 77/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3022\n",
      "Epoch 77: val_loss improved from 0.32005 to 0.31351, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3025 - val_loss: 0.3135\n",
      "Epoch 78/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3029\n",
      "Epoch 78: val_loss did not improve from 0.31351\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3029 - val_loss: 0.3220\n",
      "Epoch 79/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3044\n",
      "Epoch 79: val_loss did not improve from 0.31351\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3064 - val_loss: 0.3138\n",
      "Epoch 80/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2929\n",
      "Epoch 80: val_loss did not improve from 0.31351\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2929 - val_loss: 0.3201\n",
      "Epoch 81/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3007\n",
      "Epoch 81: val_loss did not improve from 0.31351\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3002 - val_loss: 0.3142\n",
      "Epoch 82/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3038\n",
      "Epoch 82: val_loss improved from 0.31351 to 0.31011, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3039 - val_loss: 0.3101\n",
      "Epoch 83/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2934\n",
      "Epoch 83: val_loss did not improve from 0.31011\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2927 - val_loss: 0.3147\n",
      "Epoch 84/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2984\n",
      "Epoch 84: val_loss did not improve from 0.31011\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2998 - val_loss: 0.3129\n",
      "Epoch 85/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.2920\n",
      "Epoch 85: val_loss improved from 0.31011 to 0.30974, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2920 - val_loss: 0.3097\n",
      "Epoch 86/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2983\n",
      "Epoch 86: val_loss improved from 0.30974 to 0.30705, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2994 - val_loss: 0.3071\n",
      "Epoch 87/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3004\n",
      "Epoch 87: val_loss did not improve from 0.30705\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2997 - val_loss: 0.3078\n",
      "Epoch 88/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2975\n",
      "Epoch 88: val_loss did not improve from 0.30705\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2976 - val_loss: 0.3077\n",
      "Epoch 89/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2851\n",
      "Epoch 89: val_loss did not improve from 0.30705\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2869 - val_loss: 0.3104\n",
      "Epoch 90/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3026\n",
      "Epoch 90: val_loss improved from 0.30705 to 0.30564, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3016 - val_loss: 0.3056\n",
      "Epoch 91/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2813\n",
      "Epoch 91: val_loss improved from 0.30564 to 0.30413, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2809 - val_loss: 0.3041\n",
      "Epoch 92/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2981\n",
      "Epoch 92: val_loss improved from 0.30413 to 0.30240, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2992 - val_loss: 0.3024\n",
      "Epoch 93/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2891\n",
      "Epoch 93: val_loss did not improve from 0.30240\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2886 - val_loss: 0.3141\n",
      "Epoch 94/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2932\n",
      "Epoch 94: val_loss did not improve from 0.30240\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2931 - val_loss: 0.3024\n",
      "Epoch 95/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2860\n",
      "Epoch 95: val_loss improved from 0.30240 to 0.30047, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2871 - val_loss: 0.3005\n",
      "Epoch 96/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2776\n",
      "Epoch 96: val_loss improved from 0.30047 to 0.29837, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold3.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2765 - val_loss: 0.2984\n",
      "Epoch 97/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2947\n",
      "Epoch 97: val_loss did not improve from 0.29837\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2941 - val_loss: 0.3058\n",
      "Epoch 98/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2879\n",
      "Epoch 98: val_loss did not improve from 0.29837\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2875 - val_loss: 0.3135\n",
      "Epoch 99/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2825\n",
      "Epoch 99: val_loss did not improve from 0.29837\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2828 - val_loss: 0.2988\n",
      "Epoch 100/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2790\n",
      "Epoch 100: val_loss did not improve from 0.29837\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2798 - val_loss: 0.3020\n",
      "\n",
      "Train/Test model on Fold #4.\n",
      "Epoch 1/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 1.3378\n",
      "Epoch 1: val_loss improved from inf to 1.21829, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 3s 16ms/step - loss: 1.3378 - val_loss: 1.2183\n",
      "Epoch 2/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 1.1311\n",
      "Epoch 2: val_loss improved from 1.21829 to 1.00801, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 1.1297 - val_loss: 1.0080\n",
      "Epoch 3/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.9746\n",
      "Epoch 3: val_loss improved from 1.00801 to 0.89095, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.9735 - val_loss: 0.8910\n",
      "Epoch 4/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.8833\n",
      "Epoch 4: val_loss improved from 0.89095 to 0.82518, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.8823 - val_loss: 0.8252\n",
      "Epoch 5/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.8243\n",
      "Epoch 5: val_loss improved from 0.82518 to 0.77255, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.8251 - val_loss: 0.7725\n",
      "Epoch 6/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.7736\n",
      "Epoch 6: val_loss improved from 0.77255 to 0.73601, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7736 - val_loss: 0.7360\n",
      "Epoch 7/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.7376\n",
      "Epoch 7: val_loss improved from 0.73601 to 0.69779, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7378 - val_loss: 0.6978\n",
      "Epoch 8/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.7044\n",
      "Epoch 8: val_loss improved from 0.69779 to 0.67790, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.7058 - val_loss: 0.6779\n",
      "Epoch 9/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.6835\n",
      "Epoch 9: val_loss improved from 0.67790 to 0.65728, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6827 - val_loss: 0.6573\n",
      "Epoch 10/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.6525\n",
      "Epoch 10: val_loss improved from 0.65728 to 0.62952, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6526 - val_loss: 0.6295\n",
      "Epoch 11/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6357\n",
      "Epoch 11: val_loss improved from 0.62952 to 0.61620, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.6357 - val_loss: 0.6162\n",
      "Epoch 12/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.6165\n",
      "Epoch 12: val_loss improved from 0.61620 to 0.59892, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.6165 - val_loss: 0.5989\n",
      "Epoch 13/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5974\n",
      "Epoch 13: val_loss improved from 0.59892 to 0.58652, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.5985 - val_loss: 0.5865\n",
      "Epoch 14/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.5887\n",
      "Epoch 14: val_loss improved from 0.58652 to 0.57121, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5906 - val_loss: 0.5712\n",
      "Epoch 15/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.5769\n",
      "Epoch 15: val_loss improved from 0.57121 to 0.55748, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5775 - val_loss: 0.5575\n",
      "Epoch 16/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.5568\n",
      "Epoch 16: val_loss improved from 0.55748 to 0.55105, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 11ms/step - loss: 0.5562 - val_loss: 0.5510\n",
      "Epoch 17/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.5569\n",
      "Epoch 17: val_loss improved from 0.55105 to 0.53737, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5561 - val_loss: 0.5374\n",
      "Epoch 18/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.5428\n",
      "Epoch 18: val_loss improved from 0.53737 to 0.53570, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5429 - val_loss: 0.5357\n",
      "Epoch 19/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.5340\n",
      "Epoch 19: val_loss improved from 0.53570 to 0.52877, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5340 - val_loss: 0.5288\n",
      "Epoch 20/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.5221\n",
      "Epoch 20: val_loss improved from 0.52877 to 0.51119, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5228 - val_loss: 0.5112\n",
      "Epoch 21/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.5073\n",
      "Epoch 21: val_loss improved from 0.51119 to 0.50757, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.5059 - val_loss: 0.5076\n",
      "Epoch 22/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4977\n",
      "Epoch 22: val_loss improved from 0.50757 to 0.49777, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4957 - val_loss: 0.4978\n",
      "Epoch 23/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4872\n",
      "Epoch 23: val_loss improved from 0.49777 to 0.48952, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4874 - val_loss: 0.4895\n",
      "Epoch 24/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4788\n",
      "Epoch 24: val_loss improved from 0.48952 to 0.47968, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4787 - val_loss: 0.4797\n",
      "Epoch 25/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4675\n",
      "Epoch 25: val_loss improved from 0.47968 to 0.47199, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4680 - val_loss: 0.4720\n",
      "Epoch 26/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.4649\n",
      "Epoch 26: val_loss did not improve from 0.47199\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.4640 - val_loss: 0.4856\n",
      "Epoch 27/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4657\n",
      "Epoch 27: val_loss improved from 0.47199 to 0.45958, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4635 - val_loss: 0.4596\n",
      "Epoch 28/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4468\n",
      "Epoch 28: val_loss improved from 0.45958 to 0.45202, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4463 - val_loss: 0.4520\n",
      "Epoch 29/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4497\n",
      "Epoch 29: val_loss improved from 0.45202 to 0.44924, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4510 - val_loss: 0.4492\n",
      "Epoch 30/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4385\n",
      "Epoch 30: val_loss improved from 0.44924 to 0.44272, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4376 - val_loss: 0.4427\n",
      "Epoch 31/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.4413\n",
      "Epoch 31: val_loss improved from 0.44272 to 0.43457, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4402 - val_loss: 0.4346\n",
      "Epoch 32/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.4181\n",
      "Epoch 32: val_loss improved from 0.43457 to 0.43044, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4208 - val_loss: 0.4304\n",
      "Epoch 33/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4191\n",
      "Epoch 33: val_loss improved from 0.43044 to 0.42450, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4191 - val_loss: 0.4245\n",
      "Epoch 34/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4167\n",
      "Epoch 34: val_loss improved from 0.42450 to 0.41855, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4187 - val_loss: 0.4185\n",
      "Epoch 35/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.4181\n",
      "Epoch 35: val_loss improved from 0.41855 to 0.41678, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4179 - val_loss: 0.4168\n",
      "Epoch 36/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.4061\n",
      "Epoch 36: val_loss improved from 0.41678 to 0.41246, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4061 - val_loss: 0.4125\n",
      "Epoch 37/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.4139\n",
      "Epoch 37: val_loss improved from 0.41246 to 0.40617, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.4137 - val_loss: 0.4062\n",
      "Epoch 38/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3903\n",
      "Epoch 38: val_loss improved from 0.40617 to 0.40007, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3903 - val_loss: 0.4001\n",
      "Epoch 39/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3947\n",
      "Epoch 39: val_loss did not improve from 0.40007\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3961 - val_loss: 0.4089\n",
      "Epoch 40/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3907\n",
      "Epoch 40: val_loss improved from 0.40007 to 0.39620, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3907 - val_loss: 0.3962\n",
      "Epoch 41/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3864\n",
      "Epoch 41: val_loss improved from 0.39620 to 0.39242, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3864 - val_loss: 0.3924\n",
      "Epoch 42/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3805\n",
      "Epoch 42: val_loss improved from 0.39242 to 0.38385, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3788 - val_loss: 0.3838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3772\n",
      "Epoch 43: val_loss improved from 0.38385 to 0.38035, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3772 - val_loss: 0.3803\n",
      "Epoch 44/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3785\n",
      "Epoch 44: val_loss did not improve from 0.38035\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3785 - val_loss: 0.3812\n",
      "Epoch 45/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3814\n",
      "Epoch 45: val_loss improved from 0.38035 to 0.37972, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3813 - val_loss: 0.3797\n",
      "Epoch 46/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3768\n",
      "Epoch 46: val_loss improved from 0.37972 to 0.37959, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3766 - val_loss: 0.3796\n",
      "Epoch 47/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3668\n",
      "Epoch 47: val_loss improved from 0.37959 to 0.37102, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3680 - val_loss: 0.3710\n",
      "Epoch 48/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3680\n",
      "Epoch 48: val_loss improved from 0.37102 to 0.36820, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3670 - val_loss: 0.3682\n",
      "Epoch 49/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3612\n",
      "Epoch 49: val_loss did not improve from 0.36820\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3612 - val_loss: 0.3710\n",
      "Epoch 50/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3682\n",
      "Epoch 50: val_loss improved from 0.36820 to 0.36659, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3680 - val_loss: 0.3666\n",
      "Epoch 51/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3524\n",
      "Epoch 51: val_loss improved from 0.36659 to 0.36048, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3521 - val_loss: 0.3605\n",
      "Epoch 52/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3453\n",
      "Epoch 52: val_loss improved from 0.36048 to 0.35965, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3435 - val_loss: 0.3597\n",
      "Epoch 53/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3551\n",
      "Epoch 53: val_loss did not improve from 0.35965\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3551 - val_loss: 0.3637\n",
      "Epoch 54/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3431\n",
      "Epoch 54: val_loss improved from 0.35965 to 0.35417, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3440 - val_loss: 0.3542\n",
      "Epoch 55/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3567\n",
      "Epoch 55: val_loss did not improve from 0.35417\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3555 - val_loss: 0.3551\n",
      "Epoch 56/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3484\n",
      "Epoch 56: val_loss improved from 0.35417 to 0.34918, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3484 - val_loss: 0.3492\n",
      "Epoch 57/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3459\n",
      "Epoch 57: val_loss did not improve from 0.34918\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3459 - val_loss: 0.3537\n",
      "Epoch 58/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3482\n",
      "Epoch 58: val_loss did not improve from 0.34918\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3469 - val_loss: 0.3498\n",
      "Epoch 59/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3407\n",
      "Epoch 59: val_loss improved from 0.34918 to 0.34802, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3403 - val_loss: 0.3480\n",
      "Epoch 60/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3359\n",
      "Epoch 60: val_loss improved from 0.34802 to 0.34636, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3383 - val_loss: 0.3464\n",
      "Epoch 61/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3362\n",
      "Epoch 61: val_loss improved from 0.34636 to 0.34446, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3386 - val_loss: 0.3445\n",
      "Epoch 62/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3325\n",
      "Epoch 62: val_loss improved from 0.34446 to 0.33810, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3316 - val_loss: 0.3381\n",
      "Epoch 63/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3343\n",
      "Epoch 63: val_loss did not improve from 0.33810\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3343 - val_loss: 0.3498\n",
      "Epoch 64/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3324\n",
      "Epoch 64: val_loss improved from 0.33810 to 0.33691, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3326 - val_loss: 0.3369\n",
      "Epoch 65/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3226\n",
      "Epoch 65: val_loss improved from 0.33691 to 0.33386, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3219 - val_loss: 0.3339\n",
      "Epoch 66/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3304\n",
      "Epoch 66: val_loss improved from 0.33386 to 0.33235, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3299 - val_loss: 0.3324\n",
      "Epoch 67/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3304\n",
      "Epoch 67: val_loss did not improve from 0.33235\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3311 - val_loss: 0.3554\n",
      "Epoch 68/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.3127\n",
      "Epoch 68: val_loss improved from 0.33235 to 0.32681, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3120 - val_loss: 0.3268\n",
      "Epoch 69/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3147\n",
      "Epoch 69: val_loss did not improve from 0.32681\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3141 - val_loss: 0.3300\n",
      "Epoch 70/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3249\n",
      "Epoch 70: val_loss did not improve from 0.32681\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3248 - val_loss: 0.3280\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3274\n",
      "Epoch 71: val_loss did not improve from 0.32681\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3306 - val_loss: 0.3283\n",
      "Epoch 72/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3108\n",
      "Epoch 72: val_loss improved from 0.32681 to 0.32327, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3108 - val_loss: 0.3233\n",
      "Epoch 73/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3158\n",
      "Epoch 73: val_loss did not improve from 0.32327\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3167 - val_loss: 0.3266\n",
      "Epoch 74/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3113\n",
      "Epoch 74: val_loss improved from 0.32327 to 0.32146, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3109 - val_loss: 0.3215\n",
      "Epoch 75/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3205\n",
      "Epoch 75: val_loss did not improve from 0.32146\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3206 - val_loss: 0.3329\n",
      "Epoch 76/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3219\n",
      "Epoch 76: val_loss did not improve from 0.32146\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3223 - val_loss: 0.3228\n",
      "Epoch 77/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3161\n",
      "Epoch 77: val_loss improved from 0.32146 to 0.32117, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3169 - val_loss: 0.3212\n",
      "Epoch 78/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.3087\n",
      "Epoch 78: val_loss did not improve from 0.32117\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3096 - val_loss: 0.3230\n",
      "Epoch 79/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3177\n",
      "Epoch 79: val_loss improved from 0.32117 to 0.31580, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3155 - val_loss: 0.3158\n",
      "Epoch 80/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3088\n",
      "Epoch 80: val_loss improved from 0.31580 to 0.31533, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3072 - val_loss: 0.3153\n",
      "Epoch 81/100\n",
      "78/84 [==========================>...] - ETA: 0s - loss: 0.3120\n",
      "Epoch 81: val_loss did not improve from 0.31533\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3126 - val_loss: 0.3283\n",
      "Epoch 82/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3050\n",
      "Epoch 82: val_loss did not improve from 0.31533\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3044 - val_loss: 0.3236\n",
      "Epoch 83/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3013\n",
      "Epoch 83: val_loss did not improve from 0.31533\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3006 - val_loss: 0.3204\n",
      "Epoch 84/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3044\n",
      "Epoch 84: val_loss did not improve from 0.31533\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3052 - val_loss: 0.3164\n",
      "Epoch 85/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2978\n",
      "Epoch 85: val_loss improved from 0.31533 to 0.31343, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2976 - val_loss: 0.3134\n",
      "Epoch 86/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.3022\n",
      "Epoch 86: val_loss did not improve from 0.31343\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3015 - val_loss: 0.3162\n",
      "Epoch 87/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.3081\n",
      "Epoch 87: val_loss did not improve from 0.31343\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3082 - val_loss: 0.3180\n",
      "Epoch 88/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2967\n",
      "Epoch 88: val_loss did not improve from 0.31343\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2972 - val_loss: 0.3138\n",
      "Epoch 89/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2914\n",
      "Epoch 89: val_loss improved from 0.31343 to 0.30848, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2936 - val_loss: 0.3085\n",
      "Epoch 90/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.3009\n",
      "Epoch 90: val_loss improved from 0.30848 to 0.30444, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3011 - val_loss: 0.3044\n",
      "Epoch 91/100\n",
      "84/84 [==============================] - ETA: 0s - loss: 0.3002\n",
      "Epoch 91: val_loss did not improve from 0.30444\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.3002 - val_loss: 0.3121\n",
      "Epoch 92/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2899\n",
      "Epoch 92: val_loss improved from 0.30444 to 0.30136, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2904 - val_loss: 0.3014\n",
      "Epoch 93/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2955\n",
      "Epoch 93: val_loss did not improve from 0.30136\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2959 - val_loss: 0.3092\n",
      "Epoch 94/100\n",
      "80/84 [===========================>..] - ETA: 0s - loss: 0.2908\n",
      "Epoch 94: val_loss improved from 0.30136 to 0.30019, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2921 - val_loss: 0.3002\n",
      "Epoch 95/100\n",
      "79/84 [===========================>..] - ETA: 0s - loss: 0.2983\n",
      "Epoch 95: val_loss did not improve from 0.30019\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.3015 - val_loss: 0.3059\n",
      "Epoch 96/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2852\n",
      "Epoch 96: val_loss did not improve from 0.30019\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2841 - val_loss: 0.3036\n",
      "Epoch 97/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2993\n",
      "Epoch 97: val_loss did not improve from 0.30019\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2987 - val_loss: 0.3036\n",
      "Epoch 98/100\n",
      "83/84 [============================>.] - ETA: 0s - loss: 0.2777\n",
      "Epoch 98: val_loss improved from 0.30019 to 0.29579, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2776 - val_loss: 0.2958\n",
      "Epoch 99/100\n",
      "82/84 [============================>.] - ETA: 0s - loss: 0.2798\n",
      "Epoch 99: val_loss did not improve from 0.29579\n",
      "84/84 [==============================] - 1s 9ms/step - loss: 0.2807 - val_loss: 0.3000\n",
      "Epoch 100/100\n",
      "81/84 [===========================>..] - ETA: 0s - loss: 0.2906\n",
      "Epoch 100: val_loss improved from 0.29579 to 0.29375, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\bestModel-fold4.hdf5\n",
      "84/84 [==============================] - 1s 10ms/step - loss: 0.2892 - val_loss: 0.2938\n"
     ]
    }
   ],
   "source": [
    "##################################################################################\n",
    "##### For each input file, train model and generate different outputs in a structured folder\n",
    "##################################################################################\n",
    "\n",
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Train/Test model on all folds, generate evaluations\n",
    "##################################################################################\n",
    "\n",
    "## Create and set directory to save model\n",
    "modelPath = os.path.join(outPath, expName, \"{}fold\".format(n_fold), \"models\")\n",
    "if(not os.path.isdir(modelPath)):\n",
    "    os.makedirs(modelPath)\n",
    "\n",
    "i = -1\n",
    "for fold in folds:\n",
    "    i += 1\n",
    "    \n",
    "    print(\"\\nTrain/Test model on Fold #\"+str(i)+\".\")\n",
    "    \n",
    "    model = DLNN_CORENup(input_seq_shape = input_seq_shape)\n",
    "    \n",
    "    ## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    modelCallbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
    "                                           monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                           save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "    ]\n",
    "    \n",
    "    # adding random shuffling of the dataset for training purpose\n",
    "    index_arr = np.arange(fold[\"X_train\"].shape[0])\n",
    "    index_arr = np.random.permutation(index_arr)\n",
    "    \n",
    "    model.fit(x = fold[\"X_train\"][index_arr], y = fold[\"y_train\"][index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "              callbacks = modelCallbacks, validation_data = (fold[\"X_test\"], fold[\"y_test\"]))\n",
    "    \n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Prediction and metrics for TRAIN dataset\n",
    "    ##################################################################################\n",
    "\n",
    "    y_pred = model.predict(fold[\"X_train\"])\n",
    "    label_pred = pred2label(y_pred)\n",
    "    \n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(fold[\"y_train\"], label_pred)\n",
    "    prec = precision_score(fold[\"y_train\"],label_pred)\n",
    "    mcc = matthews_corrcoef(fold[\"y_train\"], label_pred)\n",
    "\n",
    "    conf = confusion_matrix(fold[\"y_train\"], label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(fold[\"y_train\"], y_pred)\n",
    "    auc = roc_auc_score(fold[\"y_train\"], y_pred)\n",
    "    \n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Train\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "    \n",
    "    ##################################################################################\n",
    "    ##### Prediction and metrics for TEST dataset\n",
    "    ##################################################################################\n",
    "\n",
    "    y_pred = model.predict(fold[\"X_test\"])\n",
    "    label_pred = pred2label(y_pred)\n",
    "    \n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(fold[\"y_test\"], label_pred)\n",
    "    prec = precision_score(fold[\"y_test\"],label_pred)\n",
    "    mcc = matthews_corrcoef(fold[\"y_test\"], label_pred)\n",
    "\n",
    "    conf = confusion_matrix(fold[\"y_test\"], label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(fold[\"y_test\"], y_pred)\n",
    "    auc = roc_auc_score(fold[\"y_test\"], y_pred)\n",
    "    \n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Test\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Training evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.926930</td>\n",
       "      <td>0.924226</td>\n",
       "      <td>0.976035</td>\n",
       "      <td>0.924830</td>\n",
       "      <td>0.928902</td>\n",
       "      <td>0.853762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train</th>\n",
       "      <td>0.991314</td>\n",
       "      <td>0.991903</td>\n",
       "      <td>0.999392</td>\n",
       "      <td>0.990142</td>\n",
       "      <td>0.992413</td>\n",
       "      <td>0.982615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Test        0.926930   0.924226  0.976035     0.924830     0.928902  0.853762\n",
       "Train       0.991314   0.991903  0.999392     0.990142     0.992413  0.982615"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.933681</td>\n",
       "      <td>0.934884</td>\n",
       "      <td>[0.0, 0.0015384615384615385, 0.26, 0.26, 0.34,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.001445086705202312, 0.001445...</td>\n",
       "      <td>[1.999573, 0.99957305, 0.98379797, 0.98372287,...</td>\n",
       "      <td>0.979201</td>\n",
       "      <td>0.927692</td>\n",
       "      <td>0.939306</td>\n",
       "      <td>0.867225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.934377</td>\n",
       "      <td>0.928244</td>\n",
       "      <td>[0.0, 0.0015408320493066256, 0.146379044684129...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.001445086705202312...</td>\n",
       "      <td>[1.999999, 0.99999905, 0.99426866, 0.9941268, ...</td>\n",
       "      <td>0.977853</td>\n",
       "      <td>0.936826</td>\n",
       "      <td>0.932081</td>\n",
       "      <td>0.868692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.906040</td>\n",
       "      <td>0.909233</td>\n",
       "      <td>[0.0, 0.0015408320493066256, 0.505392912172573...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.001445086705202312, 0.001445...</td>\n",
       "      <td>[1.9999429, 0.9999429, 0.9266747, 0.92638284, ...</td>\n",
       "      <td>0.971991</td>\n",
       "      <td>0.895223</td>\n",
       "      <td>0.916185</td>\n",
       "      <td>0.811888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.928412</td>\n",
       "      <td>0.926040</td>\n",
       "      <td>[0.0, 0.0015408320493066256, 0.351309707241910...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.001445086705202312, 0.001445...</td>\n",
       "      <td>[1.99997, 0.99996996, 0.9733287, 0.9729856, 0....</td>\n",
       "      <td>0.975264</td>\n",
       "      <td>0.926040</td>\n",
       "      <td>0.930636</td>\n",
       "      <td>0.856676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>Test</td>\n",
       "      <td>0.932140</td>\n",
       "      <td>0.922727</td>\n",
       "      <td>[0.0, 0.0015408320493066256, 0.100154083204930...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.001445086705202312, 0.001445...</td>\n",
       "      <td>[1.9998537, 0.99985373, 0.9920839, 0.9919121, ...</td>\n",
       "      <td>0.975868</td>\n",
       "      <td>0.938367</td>\n",
       "      <td>0.926301</td>\n",
       "      <td>0.864329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold Train_Test  Accuracy  Precision  \\\n",
       "1     0       Test  0.933681   0.934884   \n",
       "3     1       Test  0.934377   0.928244   \n",
       "5     2       Test  0.906040   0.909233   \n",
       "7     3       Test  0.928412   0.926040   \n",
       "9     4       Test  0.932140   0.922727   \n",
       "\n",
       "                                                 TPR  \\\n",
       "1  [0.0, 0.0015384615384615385, 0.26, 0.26, 0.34,...   \n",
       "3  [0.0, 0.0015408320493066256, 0.146379044684129...   \n",
       "5  [0.0, 0.0015408320493066256, 0.505392912172573...   \n",
       "7  [0.0, 0.0015408320493066256, 0.351309707241910...   \n",
       "9  [0.0, 0.0015408320493066256, 0.100154083204930...   \n",
       "\n",
       "                                                 FPR  \\\n",
       "1  [0.0, 0.0, 0.0, 0.001445086705202312, 0.001445...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.001445086705202312...   \n",
       "5  [0.0, 0.0, 0.0, 0.001445086705202312, 0.001445...   \n",
       "7  [0.0, 0.0, 0.0, 0.001445086705202312, 0.001445...   \n",
       "9  [0.0, 0.0, 0.0, 0.001445086705202312, 0.001445...   \n",
       "\n",
       "                                  TPR_FPR_Thresholds       AUC  Sensitivity  \\\n",
       "1  [1.999573, 0.99957305, 0.98379797, 0.98372287,...  0.979201     0.927692   \n",
       "3  [1.999999, 0.99999905, 0.99426866, 0.9941268, ...  0.977853     0.936826   \n",
       "5  [1.9999429, 0.9999429, 0.9266747, 0.92638284, ...  0.971991     0.895223   \n",
       "7  [1.99997, 0.99996996, 0.9733287, 0.9729856, 0....  0.975264     0.926040   \n",
       "9  [1.9998537, 0.99985373, 0.9920839, 0.9919121, ...  0.975868     0.938367   \n",
       "\n",
       "   Specificity       MCC  \n",
       "1     0.939306  0.867225  \n",
       "3     0.932081  0.868692  \n",
       "5     0.916185  0.811888  \n",
       "7     0.930636  0.856676  \n",
       "9     0.926301  0.864329  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df[evaluations_df[\"Train_Test\"] == \"Test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent data testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k-fold Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of each k-fold model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.650612</td>\n",
       "      <td>0.257916</td>\n",
       "      <td>0.666421</td>\n",
       "      <td>0.589163</td>\n",
       "      <td>0.662818</td>\n",
       "      <td>0.193297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.650612   0.257916  0.666421     0.589163     0.662818  0.193297"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Fold\" : [],\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    label_pred = pred2label(y_pred)\n",
    "\n",
    "    # Compute precision, recall, sensitivity, specifity, mcc\n",
    "    acc = accuracy_score(indpe_labels, label_pred)\n",
    "    prec = precision_score(indpe_labels,label_pred)\n",
    "    mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "    conf = confusion_matrix(indpe_labels, label_pred)\n",
    "    tn, fp, fn, tp = conf.ravel()\n",
    "    sens = tp/(tp+fn)\n",
    "    spec = tn/(tn+fp)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(indpe_labels, y_pred)\n",
    "    auc = roc_auc_score(indpe_labels, y_pred)\n",
    "\n",
    "    evaluations[\"Fold\"].append(i)\n",
    "    evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "    evaluations[\"Accuracy\"].append(acc)\n",
    "    evaluations[\"Precision\"].append(prec)\n",
    "    evaluations[\"TPR\"].append(tpr)\n",
    "    evaluations[\"FPR\"].append(fpr)\n",
    "    evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "    evaluations[\"AUC\"].append(auc)\n",
    "    evaluations[\"Sensitivity\"].append(sens)\n",
    "    evaluations[\"Specificity\"].append(spec)\n",
    "    evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>Train_Test</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>TPR_FPR_Thresholds</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.259740</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0049261083743842365, 0.00492...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.003913894324853...</td>\n",
       "      <td>[1.9995211, 0.99952114, 0.99893075, 0.9989041,...</td>\n",
       "      <td>0.665969</td>\n",
       "      <td>0.591133</td>\n",
       "      <td>0.665362</td>\n",
       "      <td>0.196775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.630204</td>\n",
       "      <td>0.243852</td>\n",
       "      <td>[0.0, 0.0049261083743842365, 0.004926108374384...</td>\n",
       "      <td>[0.0, 0.0, 0.0019569471624266144, 0.0019569471...</td>\n",
       "      <td>[1.99989, 0.99988997, 0.9997781, 0.9995054, 0....</td>\n",
       "      <td>0.667748</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.638943</td>\n",
       "      <td>0.171002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.671837</td>\n",
       "      <td>0.269142</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0049261083743842365, 0.00492...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.001956947162426...</td>\n",
       "      <td>[1.9999336, 0.9999336, 0.99989986, 0.9997888, ...</td>\n",
       "      <td>0.659525</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.691781</td>\n",
       "      <td>0.204940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.648163</td>\n",
       "      <td>0.253247</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.014778325123152709, 0.014778...</td>\n",
       "      <td>[0.0, 0.0009784735812133072, 0.001956947162426...</td>\n",
       "      <td>[1.9999205, 0.9999205, 0.9998555, 0.9993937, 0...</td>\n",
       "      <td>0.663381</td>\n",
       "      <td>0.576355</td>\n",
       "      <td>0.662427</td>\n",
       "      <td>0.183185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0.649796</td>\n",
       "      <td>0.263598</td>\n",
       "      <td>[0.0, 0.0049261083743842365, 0.004926108374384...</td>\n",
       "      <td>[0.0, 0.0, 0.0009784735812133072, 0.0009784735...</td>\n",
       "      <td>[1.9997094, 0.9997093, 0.9996772, 0.9991978, 0...</td>\n",
       "      <td>0.675484</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.655577</td>\n",
       "      <td>0.210585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Fold   Train_Test  Accuracy  Precision  \\\n",
       "0     0  Independent  0.653061   0.259740   \n",
       "1     1  Independent  0.630204   0.243852   \n",
       "2     2  Independent  0.671837   0.269142   \n",
       "3     3  Independent  0.648163   0.253247   \n",
       "4     4  Independent  0.649796   0.263598   \n",
       "\n",
       "                                                 TPR  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0049261083743842365, 0.00492...   \n",
       "1  [0.0, 0.0049261083743842365, 0.004926108374384...   \n",
       "2  [0.0, 0.0, 0.0, 0.0049261083743842365, 0.00492...   \n",
       "3  [0.0, 0.0, 0.0, 0.014778325123152709, 0.014778...   \n",
       "4  [0.0, 0.0049261083743842365, 0.004926108374384...   \n",
       "\n",
       "                                                 FPR  \\\n",
       "0  [0.0, 0.0009784735812133072, 0.003913894324853...   \n",
       "1  [0.0, 0.0, 0.0019569471624266144, 0.0019569471...   \n",
       "2  [0.0, 0.0009784735812133072, 0.001956947162426...   \n",
       "3  [0.0, 0.0009784735812133072, 0.001956947162426...   \n",
       "4  [0.0, 0.0, 0.0009784735812133072, 0.0009784735...   \n",
       "\n",
       "                                  TPR_FPR_Thresholds       AUC  Sensitivity  \\\n",
       "0  [1.9995211, 0.99952114, 0.99893075, 0.9989041,...  0.665969     0.591133   \n",
       "1  [1.99989, 0.99988997, 0.9997781, 0.9995054, 0....  0.667748     0.586207   \n",
       "2  [1.9999336, 0.9999336, 0.99989986, 0.9997888, ...  0.659525     0.571429   \n",
       "3  [1.9999205, 0.9999205, 0.9998555, 0.9993937, 0...  0.663381     0.576355   \n",
       "4  [1.9997094, 0.9997093, 0.9996772, 0.9991978, 0...  0.675484     0.620690   \n",
       "\n",
       "   Specificity       MCC  \n",
       "0     0.665362  0.196775  \n",
       "1     0.638943  0.171002  \n",
       "2     0.691781  0.204940  \n",
       "3     0.662427  0.183185  \n",
       "4     0.655577  0.210585  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean score with k-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.662857</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.67371</td>\n",
       "      <td>0.591133</td>\n",
       "      <td>0.677104</td>\n",
       "      <td>0.206888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision      AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Independent  0.662857   0.266667  0.67371     0.591133     0.677104  0.206888"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "total_pred = np.zeros(indpe_labels.shape)\n",
    "all_preds = []\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    total_pred += y_pred\n",
    "    all_preds.append(y_pred)\n",
    "    \n",
    "total_pred = total_pred / n_fold\n",
    "label_pred = pred2label(total_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, total_pred)\n",
    "auc = roc_auc_score(indpe_labels, total_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting score with k-fold models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.658776</td>\n",
       "      <td>0.26477</td>\n",
       "      <td>0.654324</td>\n",
       "      <td>0.596059</td>\n",
       "      <td>0.671233</td>\n",
       "      <td>0.205504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision       AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                    \n",
       "Independent  0.658776    0.26477  0.654324     0.596059     0.671233  0.205504"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "total_pred = np.zeros(indpe_labels.shape)\n",
    "all_preds = []\n",
    "\n",
    "for i in range(n_fold):\n",
    "    \n",
    "    current_model_path = os.path.join(modelPath, \"bestModel-fold{}.hdf5\".format(i))\n",
    "    model = tf.keras.models.load_model(current_model_path)\n",
    "\n",
    "    y_pred = model.predict(indpe_features)\n",
    "    vote_pred = pred2label(y_pred)\n",
    "    total_pred += vote_pred\n",
    "    all_preds.append(vote_pred)\n",
    "    \n",
    "total_pred = total_pred / n_fold\n",
    "label_pred = pred2label(total_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, total_pred)\n",
    "auc = roc_auc_score(indpe_labels, total_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using New Model\n",
    "\n",
    "Train model on full data from training. Predict and evaluate on Independent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 1.3191 - accuracy: 0.5314\n",
      "Epoch 1: val_loss improved from inf to 1.13135, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 3s 15ms/step - loss: 1.3154 - accuracy: 0.5334 - val_loss: 1.1314 - val_accuracy: 0.7559\n",
      "Epoch 2/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 1.0706 - accuracy: 0.6483\n",
      "Epoch 2: val_loss did not improve from 1.13135\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 1.0687 - accuracy: 0.6490 - val_loss: 1.1386 - val_accuracy: 0.5673\n",
      "Epoch 3/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.9273 - accuracy: 0.6929\n",
      "Epoch 3: val_loss improved from 1.13135 to 0.99962, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.9248 - accuracy: 0.6945 - val_loss: 0.9996 - val_accuracy: 0.6343\n",
      "Epoch 4/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.8458 - accuracy: 0.7098\n",
      "Epoch 4: val_loss improved from 0.99962 to 0.98920, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.8434 - accuracy: 0.7110 - val_loss: 0.9892 - val_accuracy: 0.5935\n",
      "Epoch 5/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.7837 - accuracy: 0.7227\n",
      "Epoch 5: val_loss improved from 0.98920 to 0.96821, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.7835 - accuracy: 0.7223 - val_loss: 0.9682 - val_accuracy: 0.5878\n",
      "Epoch 6/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.7433 - accuracy: 0.7262\n",
      "Epoch 6: val_loss improved from 0.96821 to 0.93611, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.7422 - accuracy: 0.7264 - val_loss: 0.9361 - val_accuracy: 0.5894\n",
      "Epoch 7/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.7050 - accuracy: 0.7285\n",
      "Epoch 7: val_loss improved from 0.93611 to 0.81767, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.7041 - accuracy: 0.7293 - val_loss: 0.8177 - val_accuracy: 0.6449\n",
      "Epoch 8/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.6755 - accuracy: 0.7363\n",
      "Epoch 8: val_loss did not improve from 0.81767\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.6752 - accuracy: 0.7364 - val_loss: 0.9179 - val_accuracy: 0.5592\n",
      "Epoch 9/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.6490 - accuracy: 0.7461\n",
      "Epoch 9: val_loss did not improve from 0.81767\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.6507 - accuracy: 0.7446 - val_loss: 0.8838 - val_accuracy: 0.5698\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.6302 - accuracy: 0.7584\n",
      "Epoch 10: val_loss did not improve from 0.81767\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.6302 - accuracy: 0.7584 - val_loss: 0.8285 - val_accuracy: 0.6139\n",
      "Epoch 11/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.6102 - accuracy: 0.7589\n",
      "Epoch 11: val_loss improved from 0.81767 to 0.75713, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.6087 - accuracy: 0.7593 - val_loss: 0.7571 - val_accuracy: 0.6571\n",
      "Epoch 12/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.6008 - accuracy: 0.7564\n",
      "Epoch 12: val_loss did not improve from 0.75713\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.6003 - accuracy: 0.7568 - val_loss: 0.8491 - val_accuracy: 0.6024\n",
      "Epoch 13/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.5768 - accuracy: 0.7760\n",
      "Epoch 13: val_loss did not improve from 0.75713\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.5766 - accuracy: 0.7765 - val_loss: 0.8352 - val_accuracy: 0.6237\n",
      "Epoch 14/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.5701 - accuracy: 0.7763\n",
      "Epoch 14: val_loss did not improve from 0.75713\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.5709 - accuracy: 0.7754 - val_loss: 0.8058 - val_accuracy: 0.6278\n",
      "Epoch 15/100\n",
      " 99/105 [===========================>..] - ETA: 0s - loss: 0.5544 - accuracy: 0.7836\n",
      "Epoch 15: val_loss did not improve from 0.75713\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.5543 - accuracy: 0.7848 - val_loss: 0.7947 - val_accuracy: 0.6416\n",
      "Epoch 16/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.5384 - accuracy: 0.7916\n",
      "Epoch 16: val_loss did not improve from 0.75713\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.5389 - accuracy: 0.7911 - val_loss: 0.7649 - val_accuracy: 0.6531\n",
      "Epoch 17/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.5193 - accuracy: 0.7994\n",
      "Epoch 17: val_loss did not improve from 0.75713\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.5199 - accuracy: 0.7993 - val_loss: 0.7937 - val_accuracy: 0.6400\n",
      "Epoch 18/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.5118 - accuracy: 0.8063\n",
      "Epoch 18: val_loss did not improve from 0.75713\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.5123 - accuracy: 0.8057 - val_loss: 0.8309 - val_accuracy: 0.6180\n",
      "Epoch 19/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.5072 - accuracy: 0.8083\n",
      "Epoch 19: val_loss did not improve from 0.75713\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.5071 - accuracy: 0.8081 - val_loss: 0.8031 - val_accuracy: 0.6278\n",
      "Epoch 20/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.4881 - accuracy: 0.8228\n",
      "Epoch 20: val_loss improved from 0.75713 to 0.75177, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 11ms/step - loss: 0.4884 - accuracy: 0.8224 - val_loss: 0.7518 - val_accuracy: 0.6686\n",
      "Epoch 21/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.4876 - accuracy: 0.8192\n",
      "Epoch 21: val_loss did not improve from 0.75177\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.4884 - accuracy: 0.8181 - val_loss: 0.8757 - val_accuracy: 0.5951\n",
      "Epoch 22/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.4761 - accuracy: 0.8242\n",
      "Epoch 22: val_loss did not improve from 0.75177\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.4757 - accuracy: 0.8246 - val_loss: 0.8113 - val_accuracy: 0.6531\n",
      "Epoch 23/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.4552 - accuracy: 0.8413\n",
      "Epoch 23: val_loss did not improve from 0.75177\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.4561 - accuracy: 0.8400 - val_loss: 0.9957 - val_accuracy: 0.5820\n",
      "Epoch 24/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.4648 - accuracy: 0.8325\n",
      "Epoch 24: val_loss did not improve from 0.75177\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.4643 - accuracy: 0.8321 - val_loss: 0.8526 - val_accuracy: 0.6261\n",
      "Epoch 25/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.4553 - accuracy: 0.8356\n",
      "Epoch 25: val_loss did not improve from 0.75177\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.4540 - accuracy: 0.8367 - val_loss: 0.8966 - val_accuracy: 0.6433\n",
      "Epoch 26/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.4440 - accuracy: 0.8464\n",
      "Epoch 26: val_loss improved from 0.75177 to 0.73239, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.4451 - accuracy: 0.8452 - val_loss: 0.7324 - val_accuracy: 0.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.4428 - accuracy: 0.8415\n",
      "Epoch 27: val_loss improved from 0.73239 to 0.72573, saving model to Results\\NT_Site_PredNTS_Classification_DLNN_CORENup_v2_TrainStrat_2\\5fold\\models\\_fullModel.hdf5\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.4426 - accuracy: 0.8416 - val_loss: 0.7257 - val_accuracy: 0.6759\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.4326 - accuracy: 0.8510\n",
      "Epoch 28: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.4326 - accuracy: 0.8510 - val_loss: 0.9145 - val_accuracy: 0.6016\n",
      "Epoch 29/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.4134 - accuracy: 0.8615\n",
      "Epoch 29: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.4146 - accuracy: 0.8612 - val_loss: 1.0087 - val_accuracy: 0.5878\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.4209 - accuracy: 0.8513\n",
      "Epoch 30: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.4209 - accuracy: 0.8513 - val_loss: 0.8597 - val_accuracy: 0.6294\n",
      "Epoch 31/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.8565\n",
      "Epoch 31: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.4212 - accuracy: 0.8568 - val_loss: 0.8135 - val_accuracy: 0.6604\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.4084 - accuracy: 0.8621\n",
      "Epoch 32: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.4084 - accuracy: 0.8621 - val_loss: 0.8868 - val_accuracy: 0.6522\n",
      "Epoch 33/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.4029 - accuracy: 0.8636\n",
      "Epoch 33: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.4046 - accuracy: 0.8624 - val_loss: 0.9851 - val_accuracy: 0.6057\n",
      "Epoch 34/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.3971 - accuracy: 0.8680\n",
      "Epoch 34: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3965 - accuracy: 0.8688 - val_loss: 0.9245 - val_accuracy: 0.6327\n",
      "Epoch 35/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.3917 - accuracy: 0.8718\n",
      "Epoch 35: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3916 - accuracy: 0.8724 - val_loss: 0.8345 - val_accuracy: 0.6710\n",
      "Epoch 36/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.3916 - accuracy: 0.8724\n",
      "Epoch 36: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3916 - accuracy: 0.8724 - val_loss: 0.9564 - val_accuracy: 0.6212\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.8752\n",
      "Epoch 37: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3862 - accuracy: 0.8752 - val_loss: 0.8591 - val_accuracy: 0.6678\n",
      "Epoch 38/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3766 - accuracy: 0.8770\n",
      "Epoch 38: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3755 - accuracy: 0.8771 - val_loss: 0.9592 - val_accuracy: 0.6367\n",
      "Epoch 39/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3743 - accuracy: 0.8808\n",
      "Epoch 39: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3735 - accuracy: 0.8817 - val_loss: 0.9361 - val_accuracy: 0.6269\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3801 - accuracy: 0.8792\n",
      "Epoch 40: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3801 - accuracy: 0.8792 - val_loss: 0.8262 - val_accuracy: 0.6865\n",
      "Epoch 41/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3716 - accuracy: 0.8836\n",
      "Epoch 41: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3706 - accuracy: 0.8843 - val_loss: 0.9470 - val_accuracy: 0.6318\n",
      "Epoch 42/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3652 - accuracy: 0.8790\n",
      "Epoch 42: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3656 - accuracy: 0.8791 - val_loss: 0.8652 - val_accuracy: 0.6702\n",
      "Epoch 43/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.3737 - accuracy: 0.8773\n",
      "Epoch 43: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3736 - accuracy: 0.8773 - val_loss: 0.8571 - val_accuracy: 0.6710\n",
      "Epoch 44/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.3572 - accuracy: 0.8891\n",
      "Epoch 44: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3576 - accuracy: 0.8888 - val_loss: 0.9877 - val_accuracy: 0.6269\n",
      "Epoch 45/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.3656 - accuracy: 0.8846\n",
      "Epoch 45: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3654 - accuracy: 0.8847 - val_loss: 0.8777 - val_accuracy: 0.6727\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3436 - accuracy: 0.8965\n",
      "Epoch 46: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3436 - accuracy: 0.8965 - val_loss: 1.0051 - val_accuracy: 0.6139\n",
      "Epoch 47/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3531 - accuracy: 0.8864\n",
      "Epoch 47: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3540 - accuracy: 0.8862 - val_loss: 0.8646 - val_accuracy: 0.6890\n",
      "Epoch 48/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.3500 - accuracy: 0.8950\n",
      "Epoch 48: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3508 - accuracy: 0.8946 - val_loss: 0.9253 - val_accuracy: 0.6555\n",
      "Epoch 49/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3576 - accuracy: 0.8881\n",
      "Epoch 49: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3585 - accuracy: 0.8879 - val_loss: 0.9700 - val_accuracy: 0.6433\n",
      "Epoch 50/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3569 - accuracy: 0.8853\n",
      "Epoch 50: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3555 - accuracy: 0.8861 - val_loss: 0.9830 - val_accuracy: 0.6318\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.8962\n",
      "Epoch 51: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3445 - accuracy: 0.8962 - val_loss: 1.1115 - val_accuracy: 0.5796\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8962\n",
      "Epoch 52: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3400 - accuracy: 0.8962 - val_loss: 0.9575 - val_accuracy: 0.6498\n",
      "Epoch 53/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.3388 - accuracy: 0.8977\n",
      "Epoch 53: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3386 - accuracy: 0.8974 - val_loss: 1.0488 - val_accuracy: 0.6204\n",
      "Epoch 54/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.9024\n",
      "Epoch 54: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3363 - accuracy: 0.9023 - val_loss: 0.9655 - val_accuracy: 0.6596\n",
      "Epoch 55/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.3325 - accuracy: 0.9004\n",
      "Epoch 55: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3329 - accuracy: 0.8999 - val_loss: 0.9046 - val_accuracy: 0.6629\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.9032\n",
      "Epoch 56: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3324 - accuracy: 0.9032 - val_loss: 0.9396 - val_accuracy: 0.6604\n",
      "Epoch 57/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.3366 - accuracy: 0.8991\n",
      "Epoch 57: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3366 - accuracy: 0.8996 - val_loss: 0.8802 - val_accuracy: 0.6906\n",
      "Epoch 58/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.3246 - accuracy: 0.9067\n",
      "Epoch 58: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3260 - accuracy: 0.9067 - val_loss: 0.9366 - val_accuracy: 0.6580\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3315 - accuracy: 0.9008\n",
      "Epoch 59: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3315 - accuracy: 0.9008 - val_loss: 0.8891 - val_accuracy: 0.6808\n",
      "Epoch 60/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.9043\n",
      "Epoch 60: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3276 - accuracy: 0.9044 - val_loss: 0.9400 - val_accuracy: 0.6743\n",
      "Epoch 61/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.3238 - accuracy: 0.9029\n",
      "Epoch 61: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3243 - accuracy: 0.9028 - val_loss: 1.0365 - val_accuracy: 0.6392\n",
      "Epoch 62/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.3215 - accuracy: 0.9062\n",
      "Epoch 62: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3212 - accuracy: 0.9067 - val_loss: 1.0238 - val_accuracy: 0.6441\n",
      "Epoch 63/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.3146 - accuracy: 0.9111\n",
      "Epoch 63: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3154 - accuracy: 0.9104 - val_loss: 0.9885 - val_accuracy: 0.6522\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.9105\n",
      "Epoch 64: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3183 - accuracy: 0.9105 - val_loss: 1.0764 - val_accuracy: 0.6253\n",
      "Epoch 65/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3182 - accuracy: 0.9083\n",
      "Epoch 65: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3184 - accuracy: 0.9081 - val_loss: 0.9995 - val_accuracy: 0.6596\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3271 - accuracy: 0.9044\n",
      "Epoch 66: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3271 - accuracy: 0.9044 - val_loss: 1.2016 - val_accuracy: 0.5820\n",
      "Epoch 67/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3128 - accuracy: 0.9099\n",
      "Epoch 67: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3147 - accuracy: 0.9099 - val_loss: 0.9691 - val_accuracy: 0.6751\n",
      "Epoch 68/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3150 - accuracy: 0.9102\n",
      "Epoch 68: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3143 - accuracy: 0.9101 - val_loss: 1.0808 - val_accuracy: 0.6343\n",
      "Epoch 69/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.9153\n",
      "Epoch 69: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3119 - accuracy: 0.9150 - val_loss: 1.0260 - val_accuracy: 0.6620\n",
      "Epoch 70/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3213 - accuracy: 0.9097\n",
      "Epoch 70: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3201 - accuracy: 0.9108 - val_loss: 0.9848 - val_accuracy: 0.6522\n",
      "Epoch 71/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3122 - accuracy: 0.9114\n",
      "Epoch 71: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3112 - accuracy: 0.9117 - val_loss: 0.9916 - val_accuracy: 0.6588\n",
      "Epoch 72/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.3074 - accuracy: 0.9163\n",
      "Epoch 72: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3086 - accuracy: 0.9154 - val_loss: 0.9178 - val_accuracy: 0.6898\n",
      "Epoch 73/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3031 - accuracy: 0.9182\n",
      "Epoch 73: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3034 - accuracy: 0.9184 - val_loss: 1.1290 - val_accuracy: 0.6269\n",
      "Epoch 74/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3109 - accuracy: 0.9128\n",
      "Epoch 74: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3113 - accuracy: 0.9129 - val_loss: 0.9593 - val_accuracy: 0.6824\n",
      "Epoch 75/100\n",
      " 99/105 [===========================>..] - ETA: 0s - loss: 0.3114 - accuracy: 0.9134\n",
      "Epoch 75: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3103 - accuracy: 0.9144 - val_loss: 0.9876 - val_accuracy: 0.6849\n",
      "Epoch 76/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.9165\n",
      "Epoch 76: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3066 - accuracy: 0.9163 - val_loss: 1.0526 - val_accuracy: 0.6563\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.9149\n",
      "Epoch 77: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3077 - accuracy: 0.9149 - val_loss: 1.0376 - val_accuracy: 0.6514\n",
      "Epoch 78/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.2976 - accuracy: 0.9230\n",
      "Epoch 78: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2995 - accuracy: 0.9228 - val_loss: 1.0224 - val_accuracy: 0.6490\n",
      "Epoch 79/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3011 - accuracy: 0.9180\n",
      "Epoch 79: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3028 - accuracy: 0.9175 - val_loss: 0.9384 - val_accuracy: 0.6784\n",
      "Epoch 80/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.2960 - accuracy: 0.9214\n",
      "Epoch 80: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2968 - accuracy: 0.9213 - val_loss: 1.1139 - val_accuracy: 0.6408\n",
      "Epoch 81/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.3021 - accuracy: 0.9194\n",
      "Epoch 81: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3008 - accuracy: 0.9201 - val_loss: 1.0821 - val_accuracy: 0.6498\n",
      "Epoch 82/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.2956 - accuracy: 0.9201\n",
      "Epoch 82: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2970 - accuracy: 0.9187 - val_loss: 0.9562 - val_accuracy: 0.6914\n",
      "Epoch 83/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.3040 - accuracy: 0.9158\n",
      "Epoch 83: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.3048 - accuracy: 0.9156 - val_loss: 1.1418 - val_accuracy: 0.6253\n",
      "Epoch 84/100\n",
      " 99/105 [===========================>..] - ETA: 0s - loss: 0.3044 - accuracy: 0.9151\n",
      "Epoch 84: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.3059 - accuracy: 0.9146 - val_loss: 0.9637 - val_accuracy: 0.6841\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.2907 - accuracy: 0.9214\n",
      "Epoch 85: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2907 - accuracy: 0.9214 - val_loss: 1.0579 - val_accuracy: 0.6694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.2940 - accuracy: 0.9184\n",
      "Epoch 86: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2923 - accuracy: 0.9195 - val_loss: 1.1483 - val_accuracy: 0.6269\n",
      "Epoch 87/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.2902 - accuracy: 0.9239\n",
      "Epoch 87: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2908 - accuracy: 0.9241 - val_loss: 1.0841 - val_accuracy: 0.6522\n",
      "Epoch 88/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.2942 - accuracy: 0.9228\n",
      "Epoch 88: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2948 - accuracy: 0.9220 - val_loss: 1.1988 - val_accuracy: 0.6376\n",
      "Epoch 89/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9259\n",
      "Epoch 89: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2921 - accuracy: 0.9262 - val_loss: 1.0133 - val_accuracy: 0.6743\n",
      "Epoch 90/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.2959 - accuracy: 0.9171\n",
      "Epoch 90: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2958 - accuracy: 0.9174 - val_loss: 1.0920 - val_accuracy: 0.6661\n",
      "Epoch 91/100\n",
      " 99/105 [===========================>..] - ETA: 0s - loss: 0.2972 - accuracy: 0.9211\n",
      "Epoch 91: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2953 - accuracy: 0.9217 - val_loss: 1.1618 - val_accuracy: 0.6449\n",
      "Epoch 92/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.2766 - accuracy: 0.9312\n",
      "Epoch 92: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2788 - accuracy: 0.9299 - val_loss: 1.0895 - val_accuracy: 0.6514\n",
      "Epoch 93/100\n",
      "100/105 [===========================>..] - ETA: 0s - loss: 0.2959 - accuracy: 0.9220\n",
      "Epoch 93: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2940 - accuracy: 0.9223 - val_loss: 1.2638 - val_accuracy: 0.6343\n",
      "Epoch 94/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.2972 - accuracy: 0.9203\n",
      "Epoch 94: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 10ms/step - loss: 0.2966 - accuracy: 0.9211 - val_loss: 0.9698 - val_accuracy: 0.6906\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.2933 - accuracy: 0.9239\n",
      "Epoch 95: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2933 - accuracy: 0.9239 - val_loss: 1.0601 - val_accuracy: 0.6596\n",
      "Epoch 96/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.2780 - accuracy: 0.9325\n",
      "Epoch 96: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2784 - accuracy: 0.9332 - val_loss: 0.9716 - val_accuracy: 0.6988\n",
      "Epoch 97/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9269\n",
      "Epoch 97: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2874 - accuracy: 0.9263 - val_loss: 1.0524 - val_accuracy: 0.6539\n",
      "Epoch 98/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.2946 - accuracy: 0.9219\n",
      "Epoch 98: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2932 - accuracy: 0.9228 - val_loss: 1.0836 - val_accuracy: 0.6563\n",
      "Epoch 99/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.2826 - accuracy: 0.9268\n",
      "Epoch 99: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2817 - accuracy: 0.9275 - val_loss: 0.9781 - val_accuracy: 0.6980\n",
      "Epoch 100/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9261\n",
      "Epoch 100: val_loss did not improve from 0.72573\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.2866 - accuracy: 0.9259 - val_loss: 1.0628 - val_accuracy: 0.6522\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "model = DLNN_CORENup(input_seq_shape = input_seq_shape,\n",
    "                     metrics='accuracy')\n",
    "    \n",
    "## Define the model callbacks for early stopping and saving the model. Then train model\n",
    "current_model_path = os.path.join(modelPath, \"_fullModel.hdf5\")\n",
    "modelCallbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(current_model_path,\n",
    "                                       monitor = 'val_loss', verbose = 1, save_best_only = True, \n",
    "                                       save_weights_only = False, mode = 'auto', save_freq = 'epoch'),\n",
    "]\n",
    "\n",
    "# adding random shuffling of the dataset for training purpose\n",
    "index_arr = np.arange(train_features.shape[0])\n",
    "index_arr = np.random.permutation(index_arr)\n",
    "\n",
    "model.fit(x = train_features[index_arr], y = train_labels[index_arr], \n",
    "          batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "          callbacks = modelCallbacks, validation_data = (indpe_features, indpe_labels))\n",
    "# model.fit(x = train_features[index_arr], y = train_labels[index_arr], batch_size = batch_size, epochs = epochs, verbose = 1, \n",
    "#           callbacks = modelCallbacks, validation_split = 0.2)\n",
    "\n",
    "model = tf.keras.models.load_model(current_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train_Test</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Independent</th>\n",
       "      <td>0.675918</td>\n",
       "      <td>0.263415</td>\n",
       "      <td>0.66929</td>\n",
       "      <td>0.53202</td>\n",
       "      <td>0.704501</td>\n",
       "      <td>0.186368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision      AUC  Sensitivity  Specificity       MCC\n",
       "Train_Test                                                                   \n",
       "Independent  0.675918   0.263415  0.66929      0.53202     0.704501  0.186368"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the evaluation data structure for all iterations\n",
    "evaluations = {\n",
    "    \"Train_Test\" : [],\n",
    "    \"Accuracy\" : [],\n",
    "    \"Precision\": [],\n",
    "    \"TPR\": [],\n",
    "    \"FPR\": [],\n",
    "    \"TPR_FPR_Thresholds\": [],\n",
    "    \"AUC\": [],\n",
    "    \"Sensitivity\": [],\n",
    "    \"Specificity\": [],\n",
    "    \"MCC\":[]\n",
    "}\n",
    "\n",
    "##################################################################################\n",
    "##### Prediction and metrics for Independent dataset\n",
    "##################################################################################\n",
    "\n",
    "y_pred = model.predict(indpe_features)\n",
    "label_pred = pred2label(y_pred)\n",
    "\n",
    "# Compute precision, recall, sensitivity, specifity, mcc\n",
    "acc = accuracy_score(indpe_labels, label_pred)\n",
    "prec = precision_score(indpe_labels,label_pred)\n",
    "mcc = matthews_corrcoef(indpe_labels, label_pred)\n",
    "\n",
    "conf = confusion_matrix(indpe_labels, label_pred)\n",
    "tn, fp, fn, tp = conf.ravel()\n",
    "sens = tp/(tp+fn)\n",
    "spec = tn/(tn+fp)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(indpe_labels, y_pred)\n",
    "auc = roc_auc_score(indpe_labels, y_pred)\n",
    "\n",
    "evaluations[\"Train_Test\"].append(\"Independent\")\n",
    "evaluations[\"Accuracy\"].append(acc)\n",
    "evaluations[\"Precision\"].append(prec)\n",
    "evaluations[\"TPR\"].append(tpr)\n",
    "evaluations[\"FPR\"].append(fpr)\n",
    "evaluations[\"TPR_FPR_Thresholds\"].append(thresholds)\n",
    "evaluations[\"AUC\"].append(auc)\n",
    "evaluations[\"Sensitivity\"].append(sens)\n",
    "evaluations[\"Specificity\"].append(spec)\n",
    "evaluations[\"MCC\"].append(mcc)\n",
    "\n",
    "##################################################################################\n",
    "\n",
    "evaluations_df = pd.DataFrame.from_dict(evaluations)\n",
    "\n",
    "evaluations_df_grouped = evaluations_df.groupby([\"Train_Test\"]).mean().filter(['Accuracy', \n",
    "                                                                               'Precision', \n",
    "                                                                               'AUC', \n",
    "                                                                               'Sensitivity', \n",
    "                                                                               'Specificity', \n",
    "                                                                               'MCC'])\n",
    "\n",
    "evaluations_df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # after balancing the augmentation - large n/w - factor 3\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.68898\t0.265789\t0.680907\t0.497537\t0.727006\t0.18049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before balancing - large n/w - factor 3\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.733878\t0.273063\t0.666128\t0.364532\t0.807241\t0.153875\n",
    "\n",
    "# \tAccuracy\tPrecision\tAUC\tSensitivity\tSpecificity\tMCC\n",
    "# Train_Test\t\t\t\t\t\t\n",
    "# Independent\t0.715918\t0.280967\t0.673619\t0.458128\t0.767123\t0.188607"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.70      0.78      1022\n",
      "           1       0.26      0.53      0.35       203\n",
      "\n",
      "    accuracy                           0.68      1225\n",
      "   macro avg       0.57      0.62      0.57      1225\n",
      "weighted avg       0.78      0.68      0.71      1225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(indpe_labels, np.round(y_pred).astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after balancing the augmentation - large n/w - factor 3\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.73      0.80      1022\n",
    "#            1       0.27      0.50      0.35       203\n",
    "\n",
    "#     accuracy                           0.69      1225\n",
    "#    macro avg       0.57      0.61      0.57      1225\n",
    "# weighted avg       0.78      0.69      0.72      1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before balancing - large n/w - factor 3\n",
    "\n",
    "# precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.88      0.75      0.81      1022\n",
    "#            1       0.29      0.50      0.37       203\n",
    "\n",
    "#     accuracy                           0.71      1225\n",
    "#    macro avg       0.59      0.63      0.59      1225\n",
    "# weighted avg       0.79      0.71      0.74      1225\n",
    "\n",
    "# precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.86      0.81      0.84      1022\n",
    "#            1       0.27      0.36      0.31       203\n",
    "\n",
    "#     accuracy                           0.73      1225\n",
    "#    macro avg       0.57      0.59      0.57      1225\n",
    "# weighted avg       0.77      0.73      0.75      1225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 410)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(indpe_labels), np.sum(np.round(y_pred).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (203, 331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
